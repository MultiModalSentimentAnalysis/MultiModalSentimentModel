{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjAnNBpam-Jg"
   },
   "outputs": [],
   "source": [
    "!pip install scipy scikit-image torch tqdm transformers mediapipe opencv-python torchvision numpy pandas timm evaluate facenet-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mount google drive if working on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5KkoGi4r12v",
    "outputId": "173e2955-b7d1-4be3-de04-837a7d8a1a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to data dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B75co_pTBvdO",
    "outputId": "5c7d9ef6-02ad-4357-bed7-646769c86e58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/NLP/MultiModalEmotionRecognition/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2e9lCQoCIwo",
    "outputId": "2e1742eb-03ba-4bf1-8e77-b780b23dd769"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backup\t images  README.md\t test.zip  train_ende.zip\n",
      "dev.zip  labels  saved_features  texts\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unzip data archives, that are downloaded from specified links in the data README file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r_Wnev6CBnm"
   },
   "outputs": [],
   "source": [
    "!unzip dev.zip\n",
    "!unzip test.zip\n",
    "!unzip train_ende.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move extracted data to it's folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-rhhJ0rETml"
   },
   "outputs": [],
   "source": [
    "%mv dev/ images/val\n",
    "%mv test/ images/test\n",
    "%mv train_ende/ images/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIlQopjzdVRT",
    "outputId": "e46e2d5c-67b1-4e6e-ca03-224bc616da19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mval\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure every thing is fine and data is extracted in it's folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ_jlOxeD5bk",
    "outputId": "5f981aff-a550-48f3-de61-fb127ab36115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240\n",
      "5067\n",
      "5063\n"
     ]
    }
   ],
   "source": [
    "%ls images/train -1 | wc -l\n",
    "%ls images/test -1 | wc -l\n",
    "%ls images/val -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ2DTtLSXNso",
    "outputId": "d224afe4-1118-488e-cda0-67d020eef7a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "UKIHhNVuX7eu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import mediapipe\n",
    "import cv2\n",
    "import os\n",
    "import urllib\n",
    "import torch\n",
    "import ast\n",
    "import pickle\n",
    "import evaluate\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List\n",
    "from tqdm import trange, tqdm\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from scipy.spatial.distance import euclidean\n",
    "from skimage.transform import rotate\n",
    "from facenet_pytorch import MTCNN as MTCNN\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection import KeypointRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.io import read_image\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    pipeline,\n",
    "    RobertaForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize directory variables. Change the Path object input to your project base directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0zxIfhvZaA2y"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "SAVE_DIR = DATA_DIR / \"saved_features\"\n",
    "TEXTS_DIR = DATA_DIR / \"texts\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "LABELS_DIR = DATA_DIR / \"labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FhK4yxGGeLsV"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize cuda device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxkroo-MvXsO",
    "outputId": "2cecfac5-37f1-4885-8a74-b0c4179e42c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize feature vectors size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "JuUydse3k58x"
   },
   "outputs": [],
   "source": [
    "FACE_EMBEDDING_SIZE = 1280\n",
    "ENG_TEXT_EMBEDDING_SIZE = 768\n",
    "GER_TEXT_EMBEDDING_SIZE = 768\n",
    "POSE_EMBEDDING_SIZE = 34\n",
    "SCENE_EMBEDDING_SIZE = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvUZbQ1gsg05"
   },
   "source": [
    "### Scene Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5o0GGrQ2sg06"
   },
   "outputs": [],
   "source": [
    "class SceneEmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    Extracts embedding based on scene recognition task\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.weights = ResNet50_Weights.DEFAULT\n",
    "        self.pretrained_model = resnet50(weights=self.weights)\n",
    "        self.pretrained_model.eval().to(DEVICE)\n",
    "        self.preprocess = self.weights.transforms()\n",
    "        self.feature_extractor = self.remove_last_layer()\n",
    "\n",
    "    def remove_last_layer(self):\n",
    "        modules = list(self.pretrained_model.children())[:-1]\n",
    "        model = nn.Sequential(*modules)\n",
    "        model.eval()\n",
    "        return model\n",
    "\n",
    "    def extract_embedding(self, image):\n",
    "        transformed_image = self.preprocess(image).unsqueeze(0)\n",
    "        return self.feature_extractor(transformed_image).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqSJWzTcejwm"
   },
   "source": [
    "### Pose Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "xhQ_BP_YpNbs"
   },
   "outputs": [],
   "source": [
    "class PoseEmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    Extracts embedding based on pose of the persons in the image. Each person is consisted of 17 keypoints\n",
    "    and they are used as a feature.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = torchvision.models.detection.keypointrcnn_resnet50_fpn(\n",
    "            weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT, num_keypoints=17\n",
    "        ).to(DEVICE)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def extract_embedding(self, image):\n",
    "        image = self.transform(image)\n",
    "        image = image.unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image)\n",
    "\n",
    "        keypoints_scores = outputs[0][\"keypoints_scores\"]\n",
    "        best_score = torch.mean(keypoints_scores, axis=1).argmax().item()\n",
    "        keypoints = outputs[0][\"keypoints\"][best_score, :, :2]\n",
    "        return keypoints.ravel()\n",
    "\n",
    "\n",
    "# p = PoseEmbeddingExtractor(device=DEVICE)\n",
    "# path = 'data/images/val/4965.jpg'\n",
    "# img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "# p.extract_embedding(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQgq9LD6nnis"
   },
   "source": [
    "### Face Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "iBk_w05clql7"
   },
   "outputs": [],
   "source": [
    "def get_model_path(model_name):\n",
    "    model_file = model_name + \".pt\"\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".hsemotions\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fpath = os.path.join(cache_dir, model_file)\n",
    "    if not os.path.isfile(fpath):\n",
    "        print(f\"{model_file} not exists\")\n",
    "        url = (\n",
    "            \"https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/\"\n",
    "            + model_file\n",
    "            + \"?raw=true\"\n",
    "        )\n",
    "        print(\"Downloading\", model_name, \"from\", url)\n",
    "        urllib.request.urlretrieve(url, fpath)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "class FaceAlignment:\n",
    "    \"\"\"\n",
    "    Aligns the face such that faces are simillar before passing to next steps\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_rotation_on_images(input_images, angles):\n",
    "        rotated_images = [\n",
    "            rotate(image, angle) for image, angle in zip(input_images, angles)\n",
    "        ]\n",
    "        return rotated_images\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_alignment_rotation_(eyes_coordinates):\n",
    "        angles = []\n",
    "        directions = []\n",
    "        for left_eye_coordinate, right_eye_coordinate in eyes_coordinates:\n",
    "\n",
    "            left_eye_x, left_eye_y = left_eye_coordinate\n",
    "            right_eye_x, right_eye_y = right_eye_coordinate\n",
    "\n",
    "            triangle_vertex = (\n",
    "                (right_eye_x, left_eye_y)\n",
    "                if left_eye_y > right_eye_y\n",
    "                else (left_eye_x, right_eye_y)\n",
    "            )\n",
    "            direction = (\n",
    "                -1 if left_eye_y > right_eye_y else 1\n",
    "            )  # rotate clockwise else counter-clockwise\n",
    "\n",
    "            # compute length of triangle edges\n",
    "            a = euclidean(left_eye_coordinate, triangle_vertex)\n",
    "            b = euclidean(right_eye_coordinate, triangle_vertex)\n",
    "            c = euclidean(right_eye_coordinate, left_eye_coordinate)\n",
    "\n",
    "            # cosine rule\n",
    "            if (\n",
    "                b != 0 and c != 0\n",
    "            ):  # this multiplication causes division by zero in cos_a calculation\n",
    "                cos_a = (b**2 + c**2 - a**2) / (2 * b * c)\n",
    "                angle = np.arccos(cos_a)  # angle in radian\n",
    "                angle = (angle * 180) / math.pi  # radian to degree\n",
    "            else:\n",
    "                angle = 0\n",
    "\n",
    "            angle = angle - 90 if direction == -1 else angle\n",
    "\n",
    "            angles.append(angle)\n",
    "            directions.append(direction)\n",
    "\n",
    "        return angles, directions\n",
    "\n",
    "\n",
    "class FaceDetection:\n",
    "    \"\"\"\n",
    "    Detects faces in the image using MTCNN netork.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name, minimum_confidence):\n",
    "\n",
    "        self.detected_faces_information = None\n",
    "        self.model_name = model_name\n",
    "        self.minimum_confidence = minimum_confidence\n",
    "        if model_name == \"MTCNN\":\n",
    "            detector_model = MTCNN(device=DEVICE)\n",
    "            self.detect_faces_function = lambda input_image: detector_model.detect(\n",
    "                input_image, landmarks=True\n",
    "            )\n",
    "\n",
    "    def extract_faces(self, input_image, return_detections_information=True):\n",
    "        self.detect_faces__(input_image)\n",
    "        faces = self.get_faces__(\n",
    "            input_image,\n",
    "        )\n",
    "        if return_detections_information:\n",
    "            return faces, self.detected_faces_information\n",
    "\n",
    "        else:\n",
    "            return faces\n",
    "\n",
    "    def detect_faces__(self, input_image):\n",
    "        detections = self.detect_faces_function(input_image)\n",
    "        detections = [\n",
    "            {\n",
    "                \"box\": detections[0][i],\n",
    "                \"confidence\": detections[1][i],\n",
    "                \"keypoints\": {\n",
    "                    \"left_eye\": detections[2][i][0],\n",
    "                    \"right_eye\": detections[2][i][1],\n",
    "                    \"nose\": detections[2][i][2],\n",
    "                    \"mouth_left\": detections[2][i][3],\n",
    "                    \"mouth_right\": detections[2][i][4],\n",
    "                },\n",
    "            }\n",
    "            for i in range(detections[0].shape[0])\n",
    "        ]\n",
    "        self.detected_faces_information = list(\n",
    "            filter(\n",
    "                lambda element: element[\"confidence\"] > self.minimum_confidence,\n",
    "                detections,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_detected_faces_information(self):\n",
    "        return self.detected_faces_information\n",
    "\n",
    "    def get_keypoints(\n",
    "        self,\n",
    "    ):\n",
    "        return list(\n",
    "            map(lambda element: element[\"keypoints\"], self.detected_faces_information)\n",
    "        )\n",
    "\n",
    "    def get_faces__(\n",
    "        self,\n",
    "        input_image,\n",
    "    ):\n",
    "        boxes = [\n",
    "            detection_information[\"box\"]\n",
    "            for detection_information in self.detected_faces_information\n",
    "        ]\n",
    "        y1y2x1x2 = [(int(y), int(y2), int(x), int(x2)) for x, y, x2, y2 in boxes]\n",
    "        faces = [input_image[y1:y2, x1:x2] for y1, y2, x1, x2 in y1y2x1x2]\n",
    "        return faces\n",
    "\n",
    "    def get_eyes_coordinates(\n",
    "        self,\n",
    "    ):\n",
    "        eyes_coordinates = [\n",
    "            (info[\"keypoints\"][\"left_eye\"], info[\"keypoints\"][\"right_eye\"])\n",
    "            for info in self.detected_faces_information\n",
    "        ]\n",
    "        return eyes_coordinates\n",
    "\n",
    "\n",
    "class FaceEmotionRecognizer:\n",
    "    \"\"\"\n",
    "    Finds emotion of a face image input.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_name=\"enet_b0_8_best_vgaf\"):\n",
    "        self.is_mtl = \"_mtl\" in model_name\n",
    "        if \"_7\" in model_name:\n",
    "            self.idx_to_class = {\n",
    "                0: \"Anger\",\n",
    "                1: \"Disgust\",\n",
    "                2: \"Fear\",\n",
    "                3: \"Happiness\",\n",
    "                4: \"Neutral\",\n",
    "                5: \"Sadness\",\n",
    "                6: \"Surprise\",\n",
    "            }\n",
    "        else:\n",
    "            self.idx_to_class = {\n",
    "                0: \"Anger\",\n",
    "                1: \"Contempt\",\n",
    "                2: \"Disgust\",\n",
    "                3: \"Fear\",\n",
    "                4: \"Happiness\",\n",
    "                5: \"Neutral\",\n",
    "                6: \"Sadness\",\n",
    "                7: \"Surprise\",\n",
    "            }\n",
    "\n",
    "        self.img_size = 224 if \"_b0_\" in model_name else 260\n",
    "        self.test_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.img_size, self.img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        path = get_model_path(model_name)\n",
    "\n",
    "        model = torch.load(path)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        if isinstance(model.classifier, torch.nn.Sequential):\n",
    "            self.classifier_weights = model.classifier[0].weight.data\n",
    "            self.classifier_bias = model.classifier[0].bias.data\n",
    "        else:\n",
    "            self.classifier_weights = model.classifier.weight.data\n",
    "            self.classifier_bias = model.classifier.bias.data\n",
    "\n",
    "        model.classifier = torch.nn.Identity()\n",
    "        self.model = model.eval()\n",
    "        # print(path, self.test_transforms)\n",
    "\n",
    "    def compute_probability(self, features):\n",
    "        return torch.matmul(features, self.classifier_weights.T) + self.classifier_bias\n",
    "\n",
    "    def extract_representations_from_faces(self, input_faces):\n",
    "        faces = [self.test_transforms(Image.fromarray(face)) for face in input_faces]\n",
    "        features = self.model(torch.stack(faces, dim=0).to(DEVICE))\n",
    "        return features\n",
    "\n",
    "    def predict_emotions_from_representations(\n",
    "        self, representations, logits=True, return_features=True\n",
    "    ):\n",
    "        scores = self.compute_probability(representations)\n",
    "        if self.is_mtl:\n",
    "            predictions_indices = torch.argmax(scores[:, :-2], dim=1)\n",
    "\n",
    "        else:\n",
    "            predictions_indices = torch.argmax(scores, dim=1)\n",
    "\n",
    "        if self.is_mtl:\n",
    "            x = scores[:, :-2]\n",
    "\n",
    "        else:\n",
    "            x = scores\n",
    "        pred = torch.argmax(x[0])\n",
    "\n",
    "        if not logits:\n",
    "            e_x = torch.exp(x - torch.max(x, dim=1)[:, None])\n",
    "            e_x = e_x / e_x.sum(dim=1)[:, None]\n",
    "            if self.is_mtl:\n",
    "                scores[:, :-2] = e_x\n",
    "            else:\n",
    "                scores = e_x\n",
    "\n",
    "        return [\n",
    "            self.idx_to_class[pred.item()] for pred in (predictions_indices)\n",
    "        ], scores\n",
    "\n",
    "\n",
    "class FaceNormalizer:\n",
    "    \"\"\"\n",
    "    Normalizes images for network consistency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mediapipe.solutions.face_mesh\n",
    "        face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "        mp_face_mesh = mediapipe.solutions.face_mesh\n",
    "        self.face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "        self.routes_idx = self.initialize__()\n",
    "\n",
    "    def initialize__(self):\n",
    "        df = pd.DataFrame(\n",
    "            list(self.mp_face_mesh.FACEMESH_FACE_OVAL), columns=[\"p1\", \"p2\"]\n",
    "        )\n",
    "        routes_idx = []\n",
    "\n",
    "        p1 = df.iloc[0][\"p1\"]\n",
    "        p2 = df.iloc[0][\"p2\"]\n",
    "\n",
    "        for i in range(0, df.shape[0]):\n",
    "            obj = df[df[\"p1\"] == p2]\n",
    "            p1 = obj[\"p1\"].values[0]\n",
    "            p2 = obj[\"p2\"].values[0]\n",
    "\n",
    "            route_idx = []\n",
    "            route_idx.append(p1)\n",
    "            route_idx.append(p2)\n",
    "            routes_idx.append(route_idx)\n",
    "\n",
    "        return routes_idx\n",
    "\n",
    "    def get_landmarks__(self, input_image: np.ndarray):\n",
    "        if input_image.dtype == np.float:\n",
    "            input_image = (input_image * 255).astype(np.uint8)\n",
    "\n",
    "        results = self.face_mesh.process(input_image)\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        routes = []\n",
    "        # for source_idx, target_idx in mp_face_mesh.FACEMESH_FACE_OVAL:\n",
    "        for source_idx, target_idx in self.routes_idx:\n",
    "            source = landmarks.landmark[source_idx]\n",
    "            target = landmarks.landmark[target_idx]\n",
    "\n",
    "            relative_source = (\n",
    "                int(input_image.shape[1] * source.x),\n",
    "                int(input_image.shape[0] * source.y),\n",
    "            )\n",
    "            relative_target = (\n",
    "                int(input_image.shape[1] * target.x),\n",
    "                int(input_image.shape[0] * target.y),\n",
    "            )\n",
    "\n",
    "            # cv2.line(img, relative_source, relative_target, (255, 255, 255), thickness = 2)\n",
    "\n",
    "            routes.append(relative_source)\n",
    "            routes.append(relative_target)\n",
    "\n",
    "        return routes\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_with_landmark_points__(input_image, landmarks):\n",
    "        mask = np.zeros((input_image.shape[0], input_image.shape[1]))\n",
    "        mask = cv2.fillConvexPoly(mask, np.array(landmarks), 1)\n",
    "        mask = mask.astype(bool)\n",
    "\n",
    "        out = np.zeros_like(input_image)\n",
    "        out[mask] = input_image[mask]\n",
    "        return out\n",
    "\n",
    "    def normalize_faces_image(self, input_images):\n",
    "        normalized_faces_images = [\n",
    "            self.normalize_with_landmark_points__(\n",
    "                input_image, self.get_landmarks__(input_image)\n",
    "            )\n",
    "            for input_image in input_images\n",
    "        ]\n",
    "        return normalized_faces_images\n",
    "\n",
    "\n",
    "class FaceEmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    Extracts embedding of an image, based on detected faces and their emotions. It consists of all necessary steps for\n",
    "    extracting embedding from an image.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.faces = None\n",
    "        self.normalized_rotated_faces = None\n",
    "        self.rotated_faces = None\n",
    "        self.rotation_angles = None\n",
    "        self.rotation_directions = None\n",
    "\n",
    "        fd = FaceDetection(\"MTCNN\", minimum_confidence=0.95)\n",
    "        self.face_detection_model: FaceDetection = fd\n",
    "        fa = FaceAlignment()\n",
    "        self.face_alignment_model: FaceAlignment = fa\n",
    "        fn = FaceNormalizer()\n",
    "        self.face_normalizer_model: FaceNormalizer = fn\n",
    "        model_name = \"enet_b0_8_best_afew\"\n",
    "        fer = FaceEmotionRecognizer(model_name)\n",
    "        self.face_emotion_recognition_model: FaceEmotionRecognizer = fer\n",
    "\n",
    "    def extract_embedding(self, input_image):\n",
    "        faces, detected_faces_information = self.face_detection_model.extract_faces(\n",
    "            input_image, return_detections_information=True\n",
    "        )\n",
    "\n",
    "        (\n",
    "            rotation_angles,\n",
    "            rotation_directions,\n",
    "        ) = self.face_alignment_model.compute_alignment_rotation_(\n",
    "            self.face_detection_model.get_eyes_coordinates()\n",
    "        )\n",
    "        rotated_faces = self.face_alignment_model.apply_rotation_on_images(\n",
    "            faces, rotation_angles\n",
    "        )\n",
    "        normalized_rotated_faces = self.face_normalizer_model.normalize_faces_image(\n",
    "            rotated_faces\n",
    "        )\n",
    "\n",
    "        normalized_rotated_faces_255 = [\n",
    "            (image * 255).astype(np.uint8) for image in normalized_rotated_faces\n",
    "        ]\n",
    "\n",
    "        representations = (\n",
    "            self.face_emotion_recognition_model.extract_representations_from_faces(\n",
    "                normalized_rotated_faces_255\n",
    "            )\n",
    "        )[\n",
    "            0\n",
    "        ]  # WARNING: 0 was not here\n",
    "        del normalized_rotated_faces_255\n",
    "        del normalized_rotated_faces\n",
    "        del rotated_faces\n",
    "        del rotation_angles\n",
    "        del rotation_directions\n",
    "        del faces\n",
    "        del detected_faces_information\n",
    "        # (\n",
    "        #     predictions,\n",
    "        #     scores,\n",
    "        # ) = self.face_emotion_recognition_model.predict_emotions_from_representations(\n",
    "        #     representations\n",
    "        # )\n",
    "\n",
    "        # self.faces = faces\n",
    "        # self.rotation_angles, self.rotation_directions = (\n",
    "        #     rotation_angles,\n",
    "        #     rotation_directions,\n",
    "        # )\n",
    "        # self.rotated_faces = rotated_faces\n",
    "        # self.normalized_rotated_faces = normalized_rotated_faces_255\n",
    "\n",
    "        return None, None, representations\n",
    "        # return preictions, scores, representations\n",
    "\n",
    "    def get_rotations_information(self):\n",
    "        return self.rotation_angles, self.rotation_directions\n",
    "\n",
    "    def get_faces(self):\n",
    "        return self.faces\n",
    "\n",
    "    def get_rotated_faces(self):\n",
    "        return self.rotated_faces\n",
    "\n",
    "    def get_normalized_rotated_faces(self):\n",
    "        return self.normalized_rotated_faces\n",
    "\n",
    "    def clear(self):\n",
    "        self.faces = None\n",
    "        self.normalized_rotated_faces = None\n",
    "        self.rotated_faces = None\n",
    "        self.rotation_angles = None\n",
    "        self.rotation_directions = None\n",
    "\n",
    "    def store_embeddings(self, file, embeddings):\n",
    "        with open(file, \"wb\") as file_out:\n",
    "            pickle.dump(\n",
    "                {\"embeddings\": embeddings}, file_out, protocol=pickle.HIGHEST_PROTOCOL\n",
    "            )\n",
    "\n",
    "    def load_embeddings(self, file):\n",
    "        with open(file, \"rb\") as file_in:\n",
    "            stored_data = pickle.load(file_in)\n",
    "            stored_embeddings = stored_data[\"embeddings\"]\n",
    "\n",
    "        return stored_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1E_norgnuy0"
   },
   "source": [
    "### Text Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Txucy8AKlr6Y"
   },
   "outputs": [],
   "source": [
    "class EnglishTextEmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    Extracts embedding of the text using [CLS] token of a Roberta based model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"pysentimiento/robertuito-sentiment-analysis\",\n",
    "        show_progress_bar=True,\n",
    "        to_tensor=True,\n",
    "        max_length=128,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.model_name)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "            self.model_name, num_labels=3, output_hidden_states=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            task=\"sentiment-analysis\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "    def extract_embedding(\n",
    "        self,\n",
    "        input_batch_sentences,\n",
    "    ):\n",
    "        encoded_input = self.tokenizer(\n",
    "            input_batch_sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            hidden_states = model_output[\"hidden_states\"]\n",
    "            last_layer_hidden_states = hidden_states[\n",
    "                12\n",
    "            ]  # 12 = len(hidden_states) , dim = (batch_size, seq_len, 768)\n",
    "            cls_hidden_state = last_layer_hidden_states[:, 0, :]\n",
    "\n",
    "        return cls_hidden_state\n",
    "\n",
    "    def get_labels(self, input_batch_sentences):\n",
    "        return self.generator(input_batch_sentences)\n",
    "\n",
    "\n",
    "class GermanTextEmbeddingExtractor:\n",
    "    \"\"\"\n",
    "    Extracts embedding of the text using [CLS] token of a Bert based model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"oliverguhr/german-sentiment-bert\",\n",
    "        max_length=128,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            self.model_name, output_hidden_states=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            task=\"sentiment-analysis\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "            device=0,\n",
    "        )\n",
    "        self.clean_chars = re.compile(r\"[^A-Za-züöäÖÜÄß ]\", re.MULTILINE)\n",
    "        self.clean_http_urls = re.compile(r\"https*\\S+\", re.MULTILINE)\n",
    "        self.clean_at_mentions = re.compile(r\"@\\S+\", re.MULTILINE)\n",
    "\n",
    "    def predict_sentiment(self, texts: List[str]) -> List[str]:\n",
    "        texts = [self.clean_text(text) for text in texts]\n",
    "        # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "        # truncation=True limits number of tokens to model's limitations (512)\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        encoded = encoded.to(self.device)\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**encoded)\n",
    "\n",
    "        label_ids = torch.argmax(logits[0], axis=1)\n",
    "        return [self.model.config.id2label[label_id.item()] for label_id in label_ids]\n",
    "\n",
    "    def replace_numbers(self, text: str) -> str:\n",
    "        return (\n",
    "            text.replace(\"0\", \" null\")\n",
    "            .replace(\"1\", \" eins\")\n",
    "            .replace(\"2\", \" zwei\")\n",
    "            .replace(\"3\", \" drei\")\n",
    "            .replace(\"4\", \" vier\")\n",
    "            .replace(\"5\", \" fünf\")\n",
    "            .replace(\"6\", \" sechs\")\n",
    "            .replace(\"7\", \" sieben\")\n",
    "            .replace(\"8\", \" acht\")\n",
    "            .replace(\"9\", \" neun\")\n",
    "        )\n",
    "\n",
    "    def clean_text(self, text: str) -> str:\n",
    "        text = text.replace(\"\\n\", \" \")\n",
    "        text = self.clean_http_urls.sub(\"\", text)\n",
    "        text = self.clean_at_mentions.sub(\"\", text)\n",
    "        text = self.replace_numbers(text)\n",
    "        text = self.clean_chars.sub(\"\", text)  # use only text chars\n",
    "        text = \" \".join(\n",
    "            text.split()\n",
    "        )  # substitute multiple whitespace with single whitespace\n",
    "        text = text.strip().lower()\n",
    "        return text\n",
    "\n",
    "    def extract_embedding(\n",
    "        self,\n",
    "        input_batch_sentences,\n",
    "    ):\n",
    "        encoded_input = self.tokenizer(\n",
    "            input_batch_sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            hidden_states = model_output[\"hidden_states\"]\n",
    "            last_layer_hidden_states = hidden_states[\n",
    "                12\n",
    "            ]  # 12 = len(hidden_states) , dim = (batch_size, seq_len, 768)\n",
    "            cls_hidden_state = last_layer_hidden_states[:, 0, :]\n",
    "\n",
    "        return cls_hidden_state\n",
    "\n",
    "    def get_labels(self, input_batch_sentences):\n",
    "        return self.predict_sentiment(input_batch_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWFhqHGgn4b2"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset class definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3WAIa5q1lr3m"
   },
   "outputs": [],
   "source": [
    "class MSCTDDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    MSCTD dataset.\n",
    "\n",
    "    It can be used with raw data, to extract embeddings or with saved features.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"train\",\n",
    "        data_size=None,\n",
    "        load=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split (str): val, train, test.\n",
    "            data_size (int): None for full dataset. If provided dataset size will be reduced to data_size.\n",
    "            load (bool): If false, all embeddings will be extracted and dataset works with bare text and image. If true, it loads all pre extracted embeddings.\n",
    "                         Warning: don't use load=False for training. Always try using with load=True for training, to speedup process.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.eng_text_file_path = TEXTS_DIR / \"english\" / f\"{split}.txt\"\n",
    "        self.ger_text_file_path = TEXTS_DIR / \"german\" / f\"{split}.txt\"\n",
    "        self.sentiment_file_path = LABELS_DIR / f\"sentiment_{split}.txt\"\n",
    "        self.image_dir = IMAGES_DIR / split\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.load = load\n",
    "\n",
    "        self.eng_texts = None\n",
    "        self.sentiments = None\n",
    "        self.indexes = None\n",
    "        self.face_embeddings = None\n",
    "        self.pose_embeddings = None\n",
    "        self.eng_text_embeddings = None\n",
    "        self.load_data()\n",
    "        self.face_embedding_extractor = FaceEmbeddingExtractor()\n",
    "        self.eng_text_embedding_extractor = EnglishTextEmbeddingExtractor()\n",
    "        self.ger_text_embedding_extractor = GermanTextEmbeddingExtractor()\n",
    "        self.pose_embedding_extractor = PoseEmbeddingExtractor()\n",
    "        self.scene_embedding_extractor = SceneEmbeddingExtractor()\n",
    "\n",
    "    # text is not valid\n",
    "    def load_data(self):\n",
    "        if self.load:\n",
    "            eng_texts = None\n",
    "            ger_texts = None\n",
    "            indexes = torch.load(SAVE_DIR / f\"indexes_{self.split}.pt\").to(DEVICE)\n",
    "            sentiments = torch.load(SAVE_DIR / f\"sentiments_{self.split}.pt\").to(DEVICE)\n",
    "            face_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"face_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "            pose_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"pose_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "            eng_text_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"eng_text_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "            ger_text_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"ger_text_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "            scene_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"scene_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            assert (\n",
    "                face_embeddings.shape[0] == pose_embeddings.shape[0]\n",
    "            ), \"ERROR:  face and pose list are not the same size in loading\"\n",
    "            assert (\n",
    "                pose_embeddings.shape[0] == eng_text_embeddings.shape[0]\n",
    "            ), \"ERROR: text and pose list are not the same size in loading\"\n",
    "            assert (\n",
    "                eng_text_embeddings.shape[0] == indexes.shape[0]\n",
    "            ), \"ERROR: text and index list are not the same size in loading\"\n",
    "            assert (\n",
    "                indexes.shape[0] == sentiments.shape[0]\n",
    "            ), \"ERROR: index and sentiment list are not the same size in loading\"\n",
    "\n",
    "            print(face_embeddings.shape)\n",
    "            print(pose_embeddings.shape)\n",
    "            print(eng_text_embeddings.shape)\n",
    "            print(ger_text_embeddings.shape)\n",
    "            print(indexes.shape)\n",
    "            print(sentiments.shape)\n",
    "\n",
    "        else:\n",
    "            with open(self.eng_text_file_path) as eng_text_file, open(\n",
    "                self.ger_text_file_path\n",
    "            ) as ger_text_file, open(self.sentiment_file_path) as sentiment_file:\n",
    "                sentiments = [int(t.strip()) for t in sentiment_file.readlines()]\n",
    "                eng_texts = [t.strip() for t in eng_text_file.readlines()]\n",
    "                ger_texts = [t.strip() for t in ger_text_file.readlines()]\n",
    "                indexes = range(len(sentiments))\n",
    "                # indexes = torch.load(SAVE_DIR / f\"indexes_{self.split}.pt\").to(DEVICE) #WARNING: REMOVE\n",
    "\n",
    "                face_embeddings = None\n",
    "                pose_embeddings = None\n",
    "                eng_text_embeddings = None\n",
    "                ger_text_embeddings = None\n",
    "                scene_embeddings = None\n",
    "\n",
    "        if self.data_size:\n",
    "            indexes = indexes[: self.data_size]\n",
    "            sentiments = sentiments[: self.data_size]\n",
    "            if not eng_texts is None:\n",
    "                eng_texts = eng_texts[: self.data_size]\n",
    "            if not ger_texts is None:\n",
    "                ger_texts = ger_texts[: self.data_size]\n",
    "            if not face_embeddings is None:\n",
    "                face_embeddings = face_embeddings[: self.data_size, :]\n",
    "            if not pose_embeddings is None:\n",
    "                pose_embeddings = pose_embeddings[: self.data_size, :]\n",
    "            if not scene_embeddings is None:\n",
    "                scene_embeddings = scene_embeddings[: self.data_size, :]\n",
    "            if not eng_text_embeddings is None:\n",
    "                eng_text_embeddings = eng_text_embeddings[: self.data_size, :]\n",
    "\n",
    "        self.eng_texts = eng_texts\n",
    "        self.ger_texts = ger_texts\n",
    "        self.sentiments = sentiments\n",
    "        self.indexes = indexes\n",
    "        self.face_embeddings = face_embeddings\n",
    "        self.pose_embeddings = pose_embeddings\n",
    "        self.eng_text_embeddings = eng_text_embeddings\n",
    "        self.ger_text_embeddings = ger_text_embeddings\n",
    "        self.scene_embeddings = scene_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indexes)\n",
    "\n",
    "    def get_face_embedding(self, image):\n",
    "        (\n",
    "            predictions,\n",
    "            scores,\n",
    "            representations,\n",
    "        ) = self.face_embedding_extractor.extract_embedding(image)\n",
    "        return representations\n",
    "\n",
    "    def get_pose_embedding(self, image):\n",
    "        return self.pose_embedding_extractor.extract_embedding(image)\n",
    "\n",
    "    def get_image_embeddings(self, index):\n",
    "        if self.load:\n",
    "            return self.face_embeddings[index], self.pose_embeddings[index]\n",
    "\n",
    "        image_name = self.image_dir / f\"{index}.jpg\"\n",
    "        image = cv2.cvtColor(cv2.imread(str(image_name)), cv2.COLOR_BGR2RGB)\n",
    "        face_embedding = self.get_face_embedding(image)\n",
    "        pose_embedding = self.get_pose_embedding(image)\n",
    "        return face_embedding, pose_embedding\n",
    "\n",
    "    def get_scene_embedding(self, index):\n",
    "        if self.load:\n",
    "            return self.scene_embeddings[index]\n",
    "        real_index = self.indexes[index]\n",
    "        image_name = str(self.image_dir) + f\"/{real_index}.jpg\"\n",
    "        image = read_image(image_name).to(DEVICE)\n",
    "        return self.scene_embedding_extractor.extract_embedding(image)\n",
    "\n",
    "    def get_sentiment(self, index):\n",
    "        return self.sentiments[index]\n",
    "\n",
    "    def get_eng_text(self, index):\n",
    "        if self.load:\n",
    "            return self.eng_text_embeddings[index]\n",
    "        text = self.eng_texts[index]\n",
    "        text = self.eng_text_embedding_extractor.extract_embedding([text])[0]\n",
    "        return text\n",
    "\n",
    "    def get_ger_text(self, index):\n",
    "        if self.load:\n",
    "            return self.ger_text_embeddings[index]\n",
    "        text = self.ger_texts[index]\n",
    "        text = self.ger_text_embedding_extractor.extract_embedding([text])[0]\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        try:\n",
    "            face_embedding, pose_embedding = self.get_image_embeddings(index)\n",
    "            scene_embedding = self.get_scene_embedding(index)\n",
    "        except Exception as e:\n",
    "            print(f\"error for split:{self.split} index: {index}\")\n",
    "            print(e)\n",
    "            face_embedding = torch.ones(FACE_EMBEDDING_SIZE).to(DEVICE) * -123\n",
    "            pose_embedding = torch.ones(POSE_EMBEDDING_SIZE).to(DEVICE) * -123\n",
    "\n",
    "        sentiment = self.get_sentiment(index)\n",
    "        eng_text_embedding = self.get_eng_text(index)\n",
    "        ger_text_embedding = self.get_ger_text(index)\n",
    "\n",
    "        sample = {\n",
    "            \"index\": self.indexes[index],\n",
    "            \"pose_embedding\": pose_embedding,\n",
    "            \"face_embedding\": face_embedding,\n",
    "            \"scene_embedding\": scene_embedding,\n",
    "            \"eng_text_embedding\": eng_text_embedding,\n",
    "            \"ger_text_embedding\": ger_text_embedding,\n",
    "            \"sentiment\": sentiment,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5woPZsGkhvVI"
   },
   "source": [
    "# Save features\n",
    "\n",
    "This step is crucial. First, you save all embeddings in this step. Dataset is initialized with load=False, and \n",
    "extracts all embeddings with their network. This step might take a while to complete. You should run this step for \n",
    "all data splits (train, test and val)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2lAmsLut-T7M"
   },
   "outputs": [],
   "source": [
    "SAVE = False\n",
    "if SAVE:\n",
    "    SAVE_SPLIT = \"test\"\n",
    "    SAVE_BATCH = 8\n",
    "    dataset = MSCTDDataSet(split=SAVE_SPLIT, load=True)\n",
    "    print(len(dataset))\n",
    "    dataloader = DataLoader(dataset, batch_size=SAVE_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S--Fju2pTrym"
   },
   "outputs": [],
   "source": [
    "def save_features(dataloader, split):\n",
    "    \"\"\"\n",
    "    Save features using a dataset with load = False. Later on you can use dataset with load=True for a\n",
    "    fast dataset for later trainings.\n",
    "    \"\"\"\n",
    "    stop_batch = None\n",
    "\n",
    "    for batch_index, batch in enumerate(tqdm(dataloader)):\n",
    "        errors = (batch[\"pose_embedding\"] == -123).all(dim=1)\n",
    "\n",
    "        torch.save(\n",
    "            batch[\"face_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"face_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"pose_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"pose_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"eng_text_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"eng_text_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"ger_text_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"ger_text_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"index\"][~errors],\n",
    "            SAVE_DIR / f\"indexes_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"sentiment\"][~errors],\n",
    "            SAVE_DIR / f\"sentiments_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"scene_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"scene_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "\n",
    "        assert (\n",
    "            batch[\"pose_embedding\"].shape[0] == batch[\"text_embedding\"].shape[0]\n",
    "        ), \"text and pose list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"face_embedding\"].shape[0] == batch[\"pose_embedding\"].shape[0]\n",
    "        ), \"face and pose list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"eng_text_embedding\"].shape[0] == batch[\"index\"].shape[0]\n",
    "        ), \"text and index list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"index\"].shape[0] == batch[\"sentiment\"].shape[0]\n",
    "        ), \"index and sentiment list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"ger_text_embedding\"].shape[0] == batch[\"index\"].shape[0]\n",
    "        ), \"index and sentiment list are not the same size in saving\"\n",
    "        if stop_batch and batch_index == stop_batch:\n",
    "            break\n",
    "\n",
    "    print(\"----------------------\")\n",
    "    len_batch = len(dataloader)\n",
    "    if stop_batch:\n",
    "        len_batch = stop_batch\n",
    "    print(len(dataloader))\n",
    "\n",
    "    def concat_batches(name=\"face_embedding\"):\n",
    "        batches = []\n",
    "        for i in range(len_batch):\n",
    "            batches.append(torch.load(SAVE_DIR / f\"{name}_{split}_{i}.pt\"))\n",
    "        batches = torch.cat(batches, dim=0)\n",
    "        print(batches.shape)\n",
    "        torch.save(batches, SAVE_DIR / f\"{name}_{split}.pt\")\n",
    "        del batches\n",
    "\n",
    "    for name in [\n",
    "        \"face_embeddings\",\n",
    "        \"pose_embeddings\",\n",
    "        \"eng_text_embeddings\",\n",
    "        \"ger_text_embeddings\",\n",
    "        \"text_embeddings\",\n",
    "        \"scene_embeddings\",\n",
    "        \"sentiments\",\n",
    "        \"indexes\",\n",
    "    ]:\n",
    "        concat_batches(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "1lJ7MYxih8eC"
   },
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    save_features(dataloader, SAVE_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kuJ9SyzUjGc6"
   },
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    del dataset\n",
    "    del dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure embeddings are created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRixo4HQj7ns",
    "outputId": "3525e747-dac7-4cf4-ea39-771fc93af6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 462K\n",
      "281K face_embeddings_val.pt  1.5K sentiments_val.pt\n",
      "8.5K pose_embeddings_val.pt  169K text_embeddings_val.pt\n",
      "1.5K real_indexes_val.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -sh backup\n",
    "# %ls -sh data/saved_features | grep face_embeddings_test.pt\n",
    "%ls -sh data/saved_features/*_val.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete extra files if your storage is full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "H8yiHYR4zRyN"
   },
   "outputs": [],
   "source": [
    "# %rm data/saved_features/ger_text_embeddings_val_*.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check size of created embedding files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcKT0wDo6aJu",
    "outputId": "f6f272f7-fcf8-4cdc-f281-00f07c9adbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169K\tdata/saved_features/text_embeddings_val.pt\n"
     ]
    }
   ],
   "source": [
    "!du data/saved_features/text_embeddings_val.pt -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backup embeddings if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoD3xPvejhVz",
    "outputId": "caecb0bb-4956-401c-9e41-2b81b6820cfd"
   },
   "outputs": [],
   "source": [
    "# CHANGE VAL TO SPLIT\n",
    "%mkdir backup\n",
    "!cp data/saved_features/face_embeddings_val.pt backup\n",
    "!cp data/saved_features/pose_embeddings_val.pt backup\n",
    "!cp data/saved_features/real_indexes_val.pt backup\n",
    "!cp data/saved_features/eng_text_embeddings_val.pt backup\n",
    "!cp data/saved_features/ger_text_embeddings_val.pt backup\n",
    "!cp data/saved_features/sentiments_val.pt backup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8_-tAeMhMUG"
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2B5IZCzhUAv"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "precision_macro = evaluate.load(\"precision\")\n",
    "precision_micro = evaluate.load(\"precision\")\n",
    "precision_weighted = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1_macro = evaluate.load(\"f1\")\n",
    "f1_micro = evaluate.load(\"f1\")\n",
    "f1_weighted = evaluate.load(\"f1\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "recall_macro = evaluate.load(\"recall\")\n",
    "recall_micro = evaluate.load(\"recall\")\n",
    "recall_weighted = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def validate(model, dataloader, loss_fn, language=\"eng\"):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data_pair_index, batch in enumerate(dataloader):\n",
    "        # print(\"--------------\", data_pair_index, \"-------------\")\n",
    "        errors = (batch[\"pose_embedding\"] == -123).all(dim=1)\n",
    "        assert torch.all(~errors).item()\n",
    "\n",
    "        text_embedding = batch[f\"{language}_text_embedding\"]\n",
    "        face_embedding = batch[\"face_embedding\"]\n",
    "        pose_embedding = batch[\"pose_embedding\"]\n",
    "        scene_embedding = batch[\"scene_embedding\"]\n",
    "        labels = batch[\"sentiment\"]\n",
    "        inputs = torch.cat(\n",
    "            (face_embedding, text_embedding, pose_embedding, scene_embedding), 1\n",
    "        )\n",
    "        logits = model(inputs)\n",
    "        probs = logits.argmax(dim=1)\n",
    "\n",
    "        accuracy.add_batch(predictions=probs, references=labels)\n",
    "        precision.add_batch(predictions=probs, references=labels)\n",
    "        precision_macro.add_batch(predictions=probs, references=labels)\n",
    "        precision_micro.add_batch(predictions=probs, references=labels)\n",
    "        precision_weighted.add_batch(predictions=probs, references=labels)\n",
    "        f1.add_batch(predictions=probs, references=labels)\n",
    "        f1_macro.add_batch(predictions=probs, references=labels)\n",
    "        f1_micro.add_batch(predictions=probs, references=labels)\n",
    "        f1_weighted.add_batch(predictions=probs, references=labels)\n",
    "        recall.add_batch(predictions=probs, references=labels)\n",
    "        recall_macro.add_batch(predictions=probs, references=labels)\n",
    "        recall_micro.add_batch(predictions=probs, references=labels)\n",
    "        recall_weighted.add_batch(predictions=probs, references=labels)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(accuracy.compute())\n",
    "    print(precision.compute(average=None))\n",
    "    print(precision_macro.compute(average=\"macro\"))\n",
    "    print(precision_micro.compute(average=\"micro\"))\n",
    "    print(precision_weighted.compute(average=\"weighted\"))\n",
    "    print(f1.compute(average=None))\n",
    "    print(f1_macro.compute(average=\"macro\"))\n",
    "    print(f1_micro.compute(average=\"micro\"))\n",
    "    print(f1_weighted.compute(average=\"weighted\"))\n",
    "    print(recall.compute(average=None))\n",
    "    print(recall_macro.compute(average=\"macro\"))\n",
    "    print(recall_micro.compute(average=\"micro\"))\n",
    "    print(recall_weighted.compute(average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta6yhrOr0SjZ"
   },
   "outputs": [],
   "source": [
    "class SimpleDenseNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear network as the classification head of our network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_classes, embedding_dimension):\n",
    "        super(SimpleDenseNetwork, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=self.embedding_dimension,\n",
    "                out_features=512,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=128, out_features=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BHPJSCmn_4Y"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "bXFra0LEuGTb"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_workers = 1\n",
    "EPOCHS = 30\n",
    "embedding_dimension = (\n",
    "    FACE_EMBEDDING_SIZE\n",
    "    + ENG_TEXT_EMBEDDING_SIZE\n",
    "    + POSE_EMBEDDING_SIZE\n",
    "    + SCENE_EMBEDDING_SIZE\n",
    ")\n",
    "\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.001\n",
    "data_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbxK8X8ftyIM",
    "outputId": "8a520388-eae4-43b2-d83a-755f8e219e4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12816, 1280])\n",
      "torch.Size([12816, 34])\n",
      "torch.Size([12816, 768])\n",
      "torch.Size([12816, 768])\n",
      "torch.Size([12816])\n",
      "torch.Size([12816])\n",
      "torch.Size([3334, 1280])\n",
      "torch.Size([3334, 34])\n",
      "torch.Size([3334, 768])\n",
      "torch.Size([3334, 768])\n",
      "torch.Size([3334])\n",
      "torch.Size([3334])\n",
      "torch.Size([3331, 1280])\n",
      "torch.Size([3331, 34])\n",
      "torch.Size([3331, 768])\n",
      "torch.Size([3331, 768])\n",
      "torch.Size([3331])\n",
      "torch.Size([3331])\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_and_dataloder(split, batch_size, data_size=None):\n",
    "    dataset = MSCTDDataSet(split, data_size=data_size, load=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataset, dataloader\n",
    "\n",
    "\n",
    "train_dataset, train_dataloader = get_dataset_and_dataloder(\"train\", BATCH_SIZE)\n",
    "val_dataset, val_dataloader = get_dataset_and_dataloder(\"val\", BATCH_SIZE)\n",
    "test_dataset, test_dataloader = get_dataset_and_dataloder(\"test\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "RtzGPVTplryF"
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch_index, model, dataloader, loss_fn, optimizer, language=\"eng\"):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_index, batch in enumerate(tqdm(dataloader)):\n",
    "        errors = (batch[\"pose_embedding\"] == -123).all(dim=1)\n",
    "        assert torch.all(~errors).item()\n",
    "        text_embedding = batch[f\"{language}_text_embedding\"]\n",
    "        face_embedding = batch[\"face_embedding\"]\n",
    "        pose_embedding = batch[\"pose_embedding\"]\n",
    "        scene_embedding = batch[\"scene_embedding\"]\n",
    "        labels = batch[\"sentiment\"]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        inputs = torch.cat(\n",
    "            (face_embedding, text_embedding, pose_embedding, scene_embedding), 1\n",
    "        )\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch loss: \", running_loss)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, train_dataloader, val_dataloader, language=\"eng\"):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"--------------epoch: \", epoch, \"-------------\")\n",
    "        model.train()\n",
    "        train_epoch(epoch, model, train_dataloader, loss_fn, optimizer, language)\n",
    "        model.eval()\n",
    "        validate(model, val_dataloader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oWLiQFmvtvyd"
   },
   "outputs": [],
   "source": [
    "model = SimpleDenseNetwork(n_classes=3, embedding_dimension=embedding_dimension).to(\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "InvQze-IzLf0",
    "outputId": "959357f0-9c65-4feb-aae0-02d4d7637eaf"
   },
   "outputs": [],
   "source": [
    "train_model(model, EPOCHS, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v90Uk-4akcFR",
    "outputId": "02b2441b-0fff-40c0-f7c7-54d6ed03c27e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.5813261968480523}\n",
      "{'precision': array([0.41584158, 0.6522285 , 0.62485482])}\n",
      "{'precision': 0.5643083012737713}\n",
      "{'precision': 0.5813261968480523}\n",
      "{'precision': 0.5828709957741022}\n",
      "{'f1': array([0.42495784, 0.67511371, 0.57570894])}\n",
      "{'f1': 0.5585934956483021}\n",
      "{'f1': 0.5813261968480523}\n",
      "{'f1': 0.5806041595247571}\n",
      "{'recall': array([0.43448276, 0.6996633 , 0.53373016])}\n",
      "{'recall': 0.5559020700991526}\n",
      "{'recall': 0.5818072650855599}\n",
      "{'recall': 0.5818072650855599}\n"
     ]
    }
   ],
   "source": [
    "validate(model, test_dataloader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9oiWGTHeparu"
   },
   "outputs": [],
   "source": [
    "torch.save(model, SAVE_DIR / \"eng_model.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine tuning model on german."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8e5v7Tl7brS",
    "outputId": "f3ce5bda-d841-4276-853d-3351db685bb7"
   },
   "outputs": [],
   "source": [
    "FINE_TUNE_EPOCHS = 10\n",
    "ger_model = torch.load(SAVE_DIR / \"eng_model.pt\")\n",
    "train_model(ger_model, FINE_TUNE_EPOCHS, train_dataloader, val_dataloader, \"ger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QxXanL5G-m4u",
    "outputId": "9928425f-ecda-4ade-dbd6-67aec9581f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.3524467126988892}\n",
      "{'precision': array([0.25990854, 0.48034516, 0.34016393])}\n",
      "{'precision': 0.36013920973636754}\n",
      "{'precision': 0.3524467126988892}\n",
      "{'precision': 0.38192645072553466}\n",
      "{'f1': array([0.31544866, 0.39730373, 0.33569262])}\n",
      "{'f1': 0.3494816682189696}\n",
      "{'f1': 0.3524467126988892}\n",
      "{'f1': 0.357882790881482}\n",
      "{'recall': array([0.40117647, 0.33874239, 0.33133733])}\n",
      "{'recall': 0.35708539648222154}\n",
      "{'recall': 0.3524467126988892}\n",
      "{'recall': 0.3524467126988892}\n"
     ]
    }
   ],
   "source": [
    "validate(model, test_dataloader, nn.CrossEntropyLoss(), \"ger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l0vpgkaN-qIP"
   },
   "outputs": [],
   "source": [
    "torch.save(model, SAVE_DIR / \"ger_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Full.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.4 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "de7383211b4ffff70067782259e5c1c9d8bdc7fbe1ab4077ac9d43b5a6892f49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
