{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjAnNBpam-Jg"
      },
      "outputs": [],
      "source": [
        "!pip install scipy scikit-image torch tqdm transformers mediapipe opencv-python torchvision numpy pandas timm evaluate facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5KkoGi4r12v",
        "outputId": "5bc9c7da-f8d8-42be-d201-5e8ce75d08da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount(\"/content/drive\")\n",
        "project_path = Path(\"/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B75co_pTBvdO",
        "outputId": "2a4ecc8e-6f68-4aa2-cbc8-3133dea55aa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2e9lCQoCIwo",
        "outputId": "1f49593a-bd20-401f-928d-3d905e1f2f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct_indexes    image_index_test.txt   sentiment_train.txt\n",
            "dev.zip\t\t   image_index_train.txt  sentiment_val.txt\n",
            "english_test.txt   image_index_val.txt\t  test.zip\n",
            "english_train.txt  images\t\t  train_ende.zip\n",
            "english_val.txt    saved_features\n",
            "error_indexes\t   sentiment_test.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r_Wnev6CBnm"
      },
      "outputs": [],
      "source": [
        "!unzip dev.zip\n",
        "!unzip test.zip\n",
        "!unzip train_ende.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-rhhJ0rETml"
      },
      "outputs": [],
      "source": [
        "%mv dev/ images/val\n",
        "%mv test/ images/test\n",
        "%mv train_ende/ images/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ_jlOxeD5bk",
        "outputId": "04248d64-8b7b-4dc7-9e96-1a50997c101a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5068\n"
          ]
        }
      ],
      "source": [
        "%ls images/test -1 | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ2DTtLSXNso",
        "outputId": "63c78e2c-f8ad-4d06-ab9a-0df5c9fdfa2a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxkroo-MvXsO",
        "outputId": "52269e3b-b588-4c55-b79e-19e6445f937c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import transforms as transforms\n",
        "from torchvision.models.detection import KeypointRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "\n",
        "class PoseEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        device='cpu'\n",
        "    ):\n",
        "        self.model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT,num_keypoints=17).to(device)\n",
        "        self.model.eval()\n",
        "        self.device = device\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def extract_embedding(self, image):\n",
        "        image = self.transform(image)\n",
        "        image = image.unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(image)\n",
        "        \n",
        "        keypoints_scores = outputs[0]['keypoints_scores']\n",
        "        best_score = torch.mean(keypoints_scores, axis=1).argmax().item()\n",
        "        keypoints = outputs[0]['keypoints'][best_score,:,:2]\n",
        "        return keypoints.ravel()\n",
        "\n",
        "# p = PoseEmbeddingExtractor(device=device)\n",
        "# path = 'data/images/val/4965.jpg'\n",
        "# img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "# p.extract_embedding(img).shape"
      ],
      "metadata": {
        "id": "xhQ_BP_YpNbs"
      },
      "execution_count": 210,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys, os, torch, cv2\n",
        "from pathlib import Path\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def remove_non_poses(input_dir, split):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    pee = PoseEmbeddingExtractor(device=device)\n",
        "    file_name = f\"pose_error_{split}.txt\"\n",
        "    os.makedirs(\"./data/error_indexes\", exist_ok=True)\n",
        "    non_pose_files = open(f\"./data/error_indexes/{file_name}\", \"w\")\n",
        "    img_pattern = os.path.join(input_dir, \"*.jpg\")\n",
        "    images = glob(img_pattern)\n",
        "    for image_path in tqdm(images):\n",
        "        try:\n",
        "            img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "            pee.extract_embedding(img)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            img_id = image_path.split(\"/\")[-1].split(\".\")[0]\n",
        "            non_pose_files.write(f\"{img_id}\")\n",
        "            non_pose_files.write(os.linesep)\n",
        "\n",
        "\n",
        "split = \"test\"\n",
        "input_dir = project_path / \"data\" / \"images\" / split\n",
        "remove_non_poses(input_dir, split)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TacWJSYfBda",
        "outputId": "d75b597b-463f-459d-a7d6-ca0814cb6bc5"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 533/5067 [02:15<11:11,  6.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 857/5067 [03:05<13:11,  5.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 22%|██▏       | 1121/5067 [03:48<11:01,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 1208/5067 [04:02<09:44,  6.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 1867/5067 [05:51<08:41,  6.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 38%|███▊      | 1948/5067 [06:06<08:57,  5.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 2013/5067 [06:16<08:04,  6.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 2020/5067 [06:18<08:25,  6.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|███▉      | 2025/5067 [06:18<08:04,  6.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 43%|████▎     | 2184/5067 [06:45<07:39,  6.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 2327/5067 [07:08<06:45,  6.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 2341/5067 [07:10<06:47,  6.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 2357/5067 [07:12<06:34,  6.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 2359/5067 [07:13<06:26,  7.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 47%|████▋     | 2366/5067 [07:14<06:50,  6.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 2653/5067 [07:57<05:46,  6.96it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 2922/5067 [08:39<05:07,  6.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 3065/5067 [09:01<05:00,  6.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 3537/5067 [10:13<03:50,  6.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 3584/5067 [10:20<03:50,  6.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 4275/5067 [12:07<02:05,  6.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|████████▉ | 4536/5067 [12:47<01:22,  6.44it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 92%|█████████▏| 4663/5067 [13:06<00:59,  6.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████▉| 5066/5067 [14:09<00:00,  6.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "argmax(): Expected reduction dim to be specified for input.numel() == 0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5067/5067 [14:09<00:00,  5.97it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQgq9LD6nnis"
      },
      "source": [
        "#Face Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "iBk_w05clql7"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "import math\n",
        "from skimage.transform import rotate\n",
        "from mtcnn import MTCNN\n",
        "from facenet_pytorch import MTCNN as MTCNN2\n",
        "import mediapipe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import urllib\n",
        "\n",
        "\n",
        "def get_model_path(model_name):\n",
        "    model_file = model_name + \".pt\"\n",
        "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".hsemotions\")\n",
        "    # cache_dir = \"emotion_models\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fpath = os.path.join(cache_dir, model_file)\n",
        "    if not os.path.isfile(fpath):\n",
        "        print(f\"{model_file} not exists\")\n",
        "        url = (\n",
        "            \"https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/\"\n",
        "            + model_file\n",
        "            + \"?raw=true\"\n",
        "        )\n",
        "        print(\"Downloading\", model_name, \"from\", url)\n",
        "        urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class FaceAlignment:\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rotation_on_images(input_images, angles):\n",
        "        rotated_images = [\n",
        "            rotate(image, angle) for image, angle in zip(input_images, angles)\n",
        "        ]\n",
        "        return rotated_images\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_alignment_rotation_(eyes_coordinates):\n",
        "        angles = []\n",
        "        directions = []\n",
        "        for left_eye_coordinate, right_eye_coordinate in eyes_coordinates:\n",
        "\n",
        "            left_eye_x, left_eye_y = left_eye_coordinate\n",
        "            right_eye_x, right_eye_y = right_eye_coordinate\n",
        "\n",
        "            triangle_vertex = (\n",
        "                (right_eye_x, left_eye_y)\n",
        "                if left_eye_y > right_eye_y\n",
        "                else (left_eye_x, right_eye_y)\n",
        "            )\n",
        "            direction = (\n",
        "                -1 if left_eye_y > right_eye_y else 1\n",
        "            )  # rotate clockwise else counter-clockwise\n",
        "\n",
        "            # compute length of triangle edges\n",
        "            a = euclidean(left_eye_coordinate, triangle_vertex)\n",
        "            b = euclidean(right_eye_coordinate, triangle_vertex)\n",
        "            c = euclidean(right_eye_coordinate, left_eye_coordinate)\n",
        "\n",
        "            # cosine rule\n",
        "            if (\n",
        "                b != 0 and c != 0\n",
        "            ):  # this multiplication causes division by zero in cos_a calculation\n",
        "                cos_a = (b**2 + c**2 - a**2) / (2 * b * c)\n",
        "                angle = np.arccos(cos_a)  # angle in radian\n",
        "                angle = (angle * 180) / math.pi  # radian to degree\n",
        "            else:\n",
        "                angle = 0\n",
        "\n",
        "            angle = angle - 90 if direction == -1 else angle\n",
        "\n",
        "            angles.append(angle)\n",
        "            directions.append(direction)\n",
        "\n",
        "        return angles, directions\n",
        "\n",
        "\n",
        "class FaceDetection:\n",
        "\n",
        "    # first call extract_face\n",
        "    def __init__(self, model_name, minimum_confidence):\n",
        "\n",
        "        self.detected_faces_information = None\n",
        "        self.model_name = model_name\n",
        "        self.minimum_confidence = minimum_confidence\n",
        "        if model_name == \"MTCNN\":\n",
        "            detector_model = MTCNN2(device=device)\n",
        "            self.detect_faces_function = (\n",
        "                lambda input_image: detector_model.detect(input_image, landmarks=True)\n",
        "            )\n",
        "\n",
        "    def extract_faces(self, input_image, return_detections_information=True):\n",
        "        self.detect_faces__(input_image)\n",
        "        faces = self.get_faces__(\n",
        "            input_image,\n",
        "        )\n",
        "        if return_detections_information:\n",
        "            return faces, self.detected_faces_information\n",
        "\n",
        "        else:\n",
        "            return faces\n",
        "\n",
        "    def detect_faces__(self, input_image):\n",
        "        detections = self.detect_faces_function(input_image)\n",
        "        detections = [\n",
        "            {\n",
        "                'box': detections[0][i],\n",
        "                'confidence': detections[1][i],\n",
        "                'keypoints': {\n",
        "                    'left_eye': detections[2][i][0],\n",
        "                    'right_eye': detections[2][i][1],\n",
        "                    'nose': detections[2][i][2],\n",
        "                    'mouth_left': detections[2][i][3], \n",
        "                    'mouth_right': detections[2][i][4]\n",
        "                }\n",
        "              \n",
        "            }\n",
        "            for i in range(detections[0].shape[0])]\n",
        "        self.detected_faces_information = list(\n",
        "            filter(\n",
        "                lambda element: element[\"confidence\"] > self.minimum_confidence,\n",
        "                detections,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_detected_faces_information(self):\n",
        "        return self.detected_faces_information\n",
        "\n",
        "    def get_keypoints(\n",
        "        self,\n",
        "    ):\n",
        "        return list(\n",
        "            map(lambda element: element[\"keypoints\"], self.detected_faces_information)\n",
        "        )\n",
        "\n",
        "    def get_faces__(\n",
        "        self,\n",
        "        input_image,\n",
        "    ):\n",
        "        boxes = [\n",
        "            detection_information[\"box\"]\n",
        "            for detection_information in self.detected_faces_information\n",
        "        ]\n",
        "        y1y2x1x2 = [(int(y), int(y2), int(x), int(x2)) for x, y, x2, y2 in boxes]\n",
        "        faces = [input_image[y1:y2, x1:x2] for y1, y2, x1, x2 in y1y2x1x2]\n",
        "        return faces\n",
        "\n",
        "    def get_eyes_coordinates(\n",
        "        self,\n",
        "    ):\n",
        "        eyes_coordinates = [\n",
        "            (info[\"keypoints\"][\"left_eye\"], info[\"keypoints\"][\"right_eye\"])\n",
        "            for info in self.detected_faces_information\n",
        "        ]\n",
        "        return eyes_coordinates\n",
        "\n",
        "\n",
        "class FaceEmotionRecognizer:\n",
        "    # supported values of model_name: enet_b0_8_best_vgaf, enet_b0_8_best_afew, enet_b2_8, enet_b0_8_va_mtl, enet_b2_7\n",
        "    def __init__(self, device, model_name=\"enet_b0_8_best_vgaf\"):\n",
        "        self.device = device\n",
        "        self.is_mtl = \"_mtl\" in model_name\n",
        "        if \"_7\" in model_name:\n",
        "            self.idx_to_class = {\n",
        "                0: \"Anger\",\n",
        "                1: \"Disgust\",\n",
        "                2: \"Fear\",\n",
        "                3: \"Happiness\",\n",
        "                4: \"Neutral\",\n",
        "                5: \"Sadness\",\n",
        "                6: \"Surprise\",\n",
        "            }\n",
        "        else:\n",
        "            self.idx_to_class = {\n",
        "                0: \"Anger\",\n",
        "                1: \"Contempt\",\n",
        "                2: \"Disgust\",\n",
        "                3: \"Fear\",\n",
        "                4: \"Happiness\",\n",
        "                5: \"Neutral\",\n",
        "                6: \"Sadness\",\n",
        "                7: \"Surprise\",\n",
        "            }\n",
        "\n",
        "        self.img_size = 224 if \"_b0_\" in model_name else 260\n",
        "        self.test_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((self.img_size, self.img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        path = get_model_path(model_name)\n",
        "\n",
        "        model = torch.load(path)\n",
        "        model = model.to(device)\n",
        "\n",
        "        if isinstance(model.classifier, torch.nn.Sequential):\n",
        "            self.classifier_weights = model.classifier[0].weight.data\n",
        "            self.classifier_bias = model.classifier[0].bias.data\n",
        "        else:\n",
        "            self.classifier_weights = model.classifier.weight.data\n",
        "            self.classifier_bias = model.classifier.bias.data\n",
        "\n",
        "        model.classifier = torch.nn.Identity()\n",
        "        self.model = model.eval()\n",
        "        # print(path, self.test_transforms)\n",
        "\n",
        "    def compute_probability(self, features):\n",
        "        return torch.matmul(features, self.classifier_weights.T) + self.classifier_bias\n",
        "\n",
        "    def extract_representations_from_faces(self, input_faces):\n",
        "        faces = [self.test_transforms(Image.fromarray(face)) for face in input_faces]\n",
        "        features = self.model(torch.stack(faces, dim=0).to(self.device))\n",
        "        return features\n",
        "\n",
        "    def predict_emotions_from_representations(\n",
        "        self, representations, logits=True, return_features=True\n",
        "    ):\n",
        "        scores = self.compute_probability(representations)\n",
        "        if self.is_mtl:\n",
        "            predictions_indices = torch.argmax(scores[:, :-2], dim=1)\n",
        "\n",
        "        else:\n",
        "            predictions_indices = torch.argmax(scores, dim=1)\n",
        "\n",
        "        if self.is_mtl:\n",
        "            x = scores[:, :-2]\n",
        "\n",
        "        else:\n",
        "            x = scores\n",
        "        pred = torch.argmax(x[0])\n",
        "\n",
        "        if not logits:\n",
        "            e_x = torch.exp(x - torch.max(x, dim=1)[:, None])\n",
        "            e_x = e_x / e_x.sum(dim=1)[:, None]\n",
        "            if self.is_mtl:\n",
        "                scores[:, :-2] = e_x\n",
        "            else:\n",
        "                scores = e_x\n",
        "\n",
        "        return [\n",
        "            self.idx_to_class[pred.item()] for pred in (predictions_indices)\n",
        "        ], scores\n",
        "\n",
        "\n",
        "class FaceNormalizer:\n",
        "    def __init__(self):\n",
        "        self.mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "        face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "        mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "        self.face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "        self.routes_idx = self.initialize__()\n",
        "\n",
        "    def initialize__(self):\n",
        "        df = pd.DataFrame(\n",
        "            list(self.mp_face_mesh.FACEMESH_FACE_OVAL), columns=[\"p1\", \"p2\"]\n",
        "        )\n",
        "        routes_idx = []\n",
        "\n",
        "        p1 = df.iloc[0][\"p1\"]\n",
        "        p2 = df.iloc[0][\"p2\"]\n",
        "\n",
        "        for i in range(0, df.shape[0]):\n",
        "            obj = df[df[\"p1\"] == p2]\n",
        "            p1 = obj[\"p1\"].values[0]\n",
        "            p2 = obj[\"p2\"].values[0]\n",
        "\n",
        "            route_idx = []\n",
        "            route_idx.append(p1)\n",
        "            route_idx.append(p2)\n",
        "            routes_idx.append(route_idx)\n",
        "\n",
        "        return routes_idx\n",
        "\n",
        "    def get_landmarks__(self, input_image: np.ndarray):\n",
        "        if input_image.dtype == np.float:\n",
        "            input_image = (input_image * 255).astype(np.uint8)\n",
        "\n",
        "        results = self.face_mesh.process(input_image)\n",
        "        landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "        routes = []\n",
        "        # for source_idx, target_idx in mp_face_mesh.FACEMESH_FACE_OVAL:\n",
        "        for source_idx, target_idx in self.routes_idx:\n",
        "            source = landmarks.landmark[source_idx]\n",
        "            target = landmarks.landmark[target_idx]\n",
        "\n",
        "            relative_source = (\n",
        "                int(input_image.shape[1] * source.x),\n",
        "                int(input_image.shape[0] * source.y),\n",
        "            )\n",
        "            relative_target = (\n",
        "                int(input_image.shape[1] * target.x),\n",
        "                int(input_image.shape[0] * target.y),\n",
        "            )\n",
        "\n",
        "            # cv2.line(img, relative_source, relative_target, (255, 255, 255), thickness = 2)\n",
        "\n",
        "            routes.append(relative_source)\n",
        "            routes.append(relative_target)\n",
        "\n",
        "        return routes\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_with_landmark_points__(input_image, landmarks):\n",
        "        mask = np.zeros((input_image.shape[0], input_image.shape[1]))\n",
        "        mask = cv2.fillConvexPoly(mask, np.array(landmarks), 1)\n",
        "        mask = mask.astype(bool)\n",
        "\n",
        "        out = np.zeros_like(input_image)\n",
        "        out[mask] = input_image[mask]\n",
        "        return out\n",
        "\n",
        "    def normalize_faces_image(self, input_images):\n",
        "        normalized_faces_images = [\n",
        "            self.normalize_with_landmark_points__(\n",
        "                input_image, self.get_landmarks__(input_image)\n",
        "            )\n",
        "            for input_image in input_images\n",
        "        ]\n",
        "        return normalized_faces_images\n",
        "\n",
        "\n",
        "class FaceEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        device='cuda'\n",
        "    ):\n",
        "        self.faces = None\n",
        "        self.normalized_rotated_faces = None\n",
        "        self.rotated_faces = None\n",
        "        self.rotation_angles = None\n",
        "        self.rotation_directions = None\n",
        "\n",
        "        fd = FaceDetection(\"MTCNN\", minimum_confidence=0.95)\n",
        "        self.face_detection_model: FaceDetection = fd\n",
        "        fa = FaceAlignment()\n",
        "        self.face_alignment_model: FaceAlignment = fa\n",
        "        fn = FaceNormalizer()\n",
        "        self.face_normalizer_model: FaceNormalizer = fn\n",
        "        model_name = \"enet_b0_8_best_afew\"\n",
        "        fer = FaceEmotionRecognizer(device, model_name)\n",
        "        self.face_emotion_recognition_model: FaceEmotionRecognizer = fer\n",
        "\n",
        "\n",
        "    def extract_embedding(self, input_image):\n",
        "        faces, detected_faces_information = self.face_detection_model.extract_faces(\n",
        "            input_image, return_detections_information=True\n",
        "        )\n",
        "\n",
        "        (\n",
        "            rotation_angles,\n",
        "            rotation_directions,\n",
        "        ) = self.face_alignment_model.compute_alignment_rotation_(\n",
        "            self.face_detection_model.get_eyes_coordinates()\n",
        "        )\n",
        "        rotated_faces = self.face_alignment_model.apply_rotation_on_images(\n",
        "            faces, rotation_angles\n",
        "        )\n",
        "        normalized_rotated_faces = self.face_normalizer_model.normalize_faces_image(\n",
        "            rotated_faces\n",
        "        )\n",
        "\n",
        "        normalized_rotated_faces_255 = [\n",
        "            (image * 255).astype(np.uint8) for image in normalized_rotated_faces\n",
        "        ]\n",
        "\n",
        "        representations = (\n",
        "            self.face_emotion_recognition_model.extract_representations_from_faces(\n",
        "                normalized_rotated_faces_255\n",
        "            )\n",
        "        )[0] #WARNING: 0 was not here\n",
        "        del normalized_rotated_faces_255\n",
        "        del normalized_rotated_faces\n",
        "        del rotated_faces\n",
        "        del rotation_angles\n",
        "        del rotation_directions\n",
        "        del faces\n",
        "        del detected_faces_information\n",
        "        # (\n",
        "        #     predictions,\n",
        "        #     scores,\n",
        "        # ) = self.face_emotion_recognition_model.predict_emotions_from_representations(\n",
        "        #     representations\n",
        "        # )\n",
        "\n",
        "        # self.faces = faces\n",
        "        # self.rotation_angles, self.rotation_directions = (\n",
        "        #     rotation_angles,\n",
        "        #     rotation_directions,\n",
        "        # )\n",
        "        # self.rotated_faces = rotated_faces\n",
        "        # self.normalized_rotated_faces = normalized_rotated_faces_255\n",
        "\n",
        "        return None, None, representations\n",
        "        # return preictions, scores, representations\n",
        "\n",
        "    def get_rotations_information(self):\n",
        "        return self.rotation_angles, self.rotation_directions\n",
        "\n",
        "    def get_faces(self):\n",
        "        return self.faces\n",
        "\n",
        "    def get_rotated_faces(self):\n",
        "        return self.rotated_faces\n",
        "\n",
        "    def get_normalized_rotated_faces(self):\n",
        "        return self.normalized_rotated_faces\n",
        "\n",
        "    def clear(self):\n",
        "        self.faces = None\n",
        "        self.normalized_rotated_faces = None\n",
        "        self.rotated_faces = None\n",
        "        self.rotation_angles = None\n",
        "        self.rotation_directions = None\n",
        "\n",
        "    def store_embeddings(self, file, embeddings):\n",
        "        with open(file, \"wb\") as file_out:\n",
        "            pickle.dump(\n",
        "                {\"embeddings\": embeddings}, file_out, protocol=pickle.HIGHEST_PROTOCOL\n",
        "            )\n",
        "\n",
        "    def load_embeddings(self, file):\n",
        "        with open(file, \"rb\") as file_in:\n",
        "            stored_data = pickle.load(file_in)\n",
        "            stored_embeddings = stored_data[\"embeddings\"]\n",
        "\n",
        "        return stored_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1E_norgnuy0"
      },
      "source": [
        "#Text Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "Txucy8AKlr6Y"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from transformers import RobertaForSequenceClassification\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class TextEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"pysentimiento/robertuito-sentiment-analysis\",\n",
        "        show_progress_bar=True,\n",
        "        to_tensor=True,\n",
        "        max_length=128,\n",
        "        device='cuda'\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "            self.model_name, num_labels=3, output_hidden_states=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.generator = pipeline(\n",
        "            task=\"sentiment-analysis\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "    def extract_embedding(\n",
        "        self,\n",
        "        input_batch_sentences,\n",
        "    ):\n",
        "        encoded_input = self.tokenizer(\n",
        "            input_batch_sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model_output = self.model(**encoded_input)\n",
        "            hidden_states = model_output[\"hidden_states\"]\n",
        "            last_layer_hidden_states = hidden_states[\n",
        "                12\n",
        "            ]  # 12 = len(hidden_states) , dim = (batch_size, seq_len, 768)\n",
        "            cls_hidden_state = last_layer_hidden_states[:, 0, :]\n",
        "\n",
        "        return cls_hidden_state\n",
        "\n",
        "    def get_labels(self, input_batch_sentences):\n",
        "        return self.generator(input_batch_sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWFhqHGgn4b2"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FACE_EMBEDDING_SIZE = 1280\n",
        "TEXT_EMBEDDING_SIZE = 768\n",
        "POSE_EMBEDDING_SIZE = 34\n",
        "SCENE_EMBEDDING_SIZE = None\n"
      ],
      "metadata": {
        "id": "JuUydse3k58x"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!free"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n-M-mOdJIJd",
        "outputId": "180e1a63-6c94-455f-ae7d-10d994c11251"
      },
      "execution_count": 228,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13298572     9226444      182716       25804     3889412     6754464\n",
            "Swap:             0           0           0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 229,
      "metadata": {
        "id": "3WAIa5q1lr3m"
      },
      "outputs": [],
      "source": [
        "import os, cv2, torch, ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class MSCTDDataSet(Dataset):\n",
        "    \"\"\"MSCTD dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, base_path=\"data/\", split=\"train\", data_size=None, load=False, raw=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_path (str or path): path to data folder\n",
        "            split (str): dev, train, test\n",
        "        \"\"\"\n",
        "        if isinstance(base_path, str):\n",
        "            base_path = Path(base_path)\n",
        "        self.base_path = base_path\n",
        "        self.load_path = base_path / 'saved_features'\n",
        "        self.split = split\n",
        "        self.text_file_path = base_path / f\"english_{split}.txt\"\n",
        "        self.seq_file_path = base_path / f\"image_index_{split}.txt\"\n",
        "        self.sentiment_file_path = base_path / f\"sentiment_{split}.txt\"\n",
        "        self.image_dir = base_path / \"images\" / split\n",
        "        self.correct_indexes_file_path = base_path / \"correct_indexes\" / f\"correct_indexes_{split}.txt\"\n",
        "\n",
        "        self.data_size = data_size\n",
        "        self.load = load\n",
        "        self.raw = raw\n",
        "\n",
        "        self.texts = None\n",
        "        self.sentiments = None\n",
        "        self.indexes = None\n",
        "        self.face_embeddings = None\n",
        "        self.pose_embeddings = None\n",
        "        self.text_embeddings = None\n",
        "        self.load_data()\n",
        "        self.face_embedding_extractor = FaceEmbeddingExtractor(device=device)\n",
        "        self.text_embedding_extractor = TextEmbeddingExtractor(device=device)\n",
        "        self.pose_embedding_extractor = PoseEmbeddingExtractor(device=device)\n",
        "\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.text_file_path) as text_file, open(self.sentiment_file_path) as sentiment_file, open(self.correct_indexes_file_path) as correct_file:\n",
        "            texts = [t.strip() for t in text_file.readlines()]\n",
        "            sentiments = [int(t.strip()) for t in sentiment_file.readlines()]\n",
        "            face_embeddings = None\n",
        "            pose_embeddings = None\n",
        "            text_embeddings = None\n",
        "            corrects = [int(c.strip()) for c in correct_file.readlines()]\n",
        "            if self.load:\n",
        "                try:\n",
        "                    face_embeddings = torch.load(self.save_path / f'face_embeddings_{self.split}.pt')\n",
        "                    pose_embeddings = torch.load(self.save_path / f'pose_embeddings_{self.split}.pt')\n",
        "                    text_embeddings = torch.load(self.save_path / f'text_embeddings_{self.split}.pt')\n",
        "                    corrects = torch.load(self.save_path / f'real_indexes_{self.split}.pt')\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                    print('Warning: passed load=True but not embedding file was located. Not loading')\n",
        "\n",
        "            correct_texts = [texts[i] for i in corrects]\n",
        "            correct_sentiments = [sentiments[i] for i in corrects]\n",
        "        # with open(self.image_index_path) as f:\n",
        "        #     images = [ast.literal_eval(t.strip()) for t in f.readlines()]\n",
        "\n",
        "        if self.data_size:\n",
        "            correct_texts = correct_texts[: self.data_size]\n",
        "            correct_sentiments =correct_sentiments[: self.data_size]\n",
        "            if face_embeddings:\n",
        "                face_embeddings = face_embeddings[:self.data_size,:]\n",
        "            if pose_embeddings:\n",
        "                pose_embeddings = pose_embeddings[:self.data_size,:]\n",
        "            if text_embeddings:\n",
        "                face_embeddings = text_embeddings[:self.data_size,:]\n",
        "            # images = images[: self.data_size]\n",
        "\n",
        "\n",
        "        self.texts = correct_texts\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.sentiments = correct_sentiments\n",
        "        self.indexes = corrects\n",
        "        self.face_embeddings = face_embeddings\n",
        "        self.pose_embeddings = pose_embeddings\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def get_face_embedding(self, index, image):\n",
        "        if self.load:\n",
        "            return self.face_embeddings[index]\n",
        "        (\n",
        "            predictions,\n",
        "            scores,\n",
        "            representations,\n",
        "        ) = self.face_embedding_extractor.extract_embedding(image)\n",
        "        return representations\n",
        "\n",
        "    def get_pose_embedding(self, index, image):\n",
        "        if self.load:\n",
        "            return self.pose_embeddings[index]\n",
        "        return self.pose_embedding_extractor.extract_embedding(image)\n",
        "\n",
        "    def get_image_embeddings(self, index):\n",
        "        image = None\n",
        "        real_index = self.indexes[index]\n",
        "        image_name = self.image_dir / f\"{real_index}.jpg\"\n",
        "        if not self.load:\n",
        "            image = cv2.cvtColor(cv2.imread(str(image_name)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        face_embedding = self.get_face_embedding(index, image)\n",
        "        pose_embedding = self.get_pose_embedding(index, image)\n",
        "        return face_embedding, pose_embedding\n",
        "\n",
        "    def get_sentiment(self, index):\n",
        "        return self.sentiments[index]\n",
        "\n",
        "\n",
        "\n",
        "    def get_text(self, index):\n",
        "        if self.load and not self.text_embeddings is None:\n",
        "            print('loading txt')\n",
        "            return self.text_embeddings[index]\n",
        "        text = self.texts[index]\n",
        "        text = self.text_embedding_extractor.extract_embedding([text])[0]\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        try:\n",
        "            face_embedding, pose_embedding = self.get_image_embeddings(index)\n",
        "        except Exception as e:\n",
        "            print(f'error for split:{self.split} index: {index}')\n",
        "            print(e)\n",
        "            face_embedding = torch.ones(FACE_EMBEDDING_SIZE).to(device)*-123\n",
        "            pose_embedding = torch.ones(POSE_EMBEDDING_SIZE).to(device)*-123\n",
        "\n",
        "        sentiment = self.get_sentiment(index)\n",
        "        text = self.get_text(index)\n",
        "        sample = {\"real_index\": self.indexes[index], \"pose_embedding\": pose_embedding, \"face_embedding\": face_embedding, \"text_embedding\": text, \"sentiment\": sentiment}\n",
        "        if self.raw:\n",
        "            sample[\"text\"] = self.texts[index]\n",
        "            # sample[\"image\"] = pass \n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save features"
      ],
      "metadata": {
        "id": "5woPZsGkhvVI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %mkdir data/saved_features/\n",
        "# %mkdir backups/"
      ],
      "metadata": {
        "id": "_5Ap_qX2mTwo"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "SAVE_SPLIT = \"test\"\n",
        "SAVE_BATCH = 8\n",
        "dataset = MSCTDDataSet(base_path=project_path / \"data/\", split = SAVE_SPLIT, load=False)\n",
        "print(len(dataset))\n",
        "dataloader = DataLoader(dataset, batch_size=SAVE_BATCH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lAmsLut-T7M",
        "outputId": "21544114-a794-4908-f1c9-88817fca3c5c"
      },
      "execution_count": 231,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3478\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_features(dataloader, split):\n",
        "    save_path = project_path / 'data' / 'saved_features'\n",
        "    stop_batch = None\n",
        "\n",
        "    for batch_index, batch in enumerate(tqdm(dataloader)):\n",
        "        # print(batch[\"face_embedding\"].shape)\n",
        "        # print(batch[\"text_embedding\"].shape)\n",
        "        # print(batch[\"real_index\"].shape)\n",
        "        errors = (batch[\"pose_embedding\"]==-123).all(dim=1)\n",
        "\n",
        "        torch.save(batch[\"face_embedding\"][~errors], save_path / f'face_embeddings_{split}_{batch_index}.pt')\n",
        "        torch.save(batch[\"pose_embedding\"][~errors], save_path / f'pose_embeddings_{split}_{batch_index}.pt')\n",
        "        torch.save(batch[\"text_embedding\"][~errors], save_path / f'text_embeddings_{split}_{batch_index}.pt')\n",
        "        torch.save(batch[\"real_index\"][~errors], save_path / f'real_indexes_{split}_{batch_index}.pt')\n",
        "        if stop_batch and batch_index==stop_batch:\n",
        "          break\n",
        "\n",
        "\n",
        "    print('----------------------')\n",
        "    print(len(dataloader))\n",
        "    len_batch = len(dataloader)\n",
        "    if stop_batch:\n",
        "        len_batch = stop_batch\n",
        "    face_embeddings = []\n",
        "    for i in range(len_batch):\n",
        "        face_embeddings.append(torch.load(save_path / f'face_embeddings_{split}_{i}.pt'))\n",
        "    face_embeddings = torch.cat(face_embeddings, dim=0)\n",
        "    print(face_embeddings.shape)\n",
        "    torch.save(face_embeddings, save_path / f'face_embeddings_{split}.pt')\n",
        "    del face_embeddings\n",
        "\n",
        "    pose_embeddings = []\n",
        "    for i in range(len_batch):\n",
        "        pose_embeddings.append(torch.load(save_path / f'pose_embeddings_{split}_{i}.pt'))\n",
        "    pose_embeddings = torch.cat(pose_embeddings, dim=0)\n",
        "    print(pose_embeddings.shape)\n",
        "    torch.save(pose_embeddings, save_path / f'pose_embeddings_{split}.pt')\n",
        "    del pose_embeddings\n",
        "\n",
        "    text_embeddings = []\n",
        "    for i in range(len_batch):\n",
        "        text_embeddings.append(torch.load(save_path / f'text_embeddings_{split}_{i}.pt'))\n",
        "    text_embeddings = torch.cat(text_embeddings, dim=0)\n",
        "    print(text_embeddings.shape)\n",
        "    torch.save(text_embeddings, save_path / f'text_embeddings_{split}.pt')\n",
        "    del text_embeddings\n",
        "\n",
        "    real_indexes = []\n",
        "    for i in range(len_batch):\n",
        "        real_indexes.append(torch.load(save_path / f'real_indexes_{split}_{i}.pt'))\n",
        "    real_indexes = torch.cat(real_indexes, dim=0)\n",
        "    print(real_indexes.shape)\n",
        "    torch.save(real_indexes, save_path / f'real_indexes_{split}.pt')\n",
        "    del real_indexes\n",
        "\n"
      ],
      "metadata": {
        "id": "S--Fju2pTrym"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_features(dataloader, SAVE_SPLIT)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lJ7MYxih8eC",
        "outputId": "f8329aec-8dfa-49c9-9dba-6191e8d0f295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/435 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:294: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  0%|          | 1/435 [00:01<13:30,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 9\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 2/435 [00:03<12:24,  1.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 15\n",
            "('Cannot warp empty image with dimensions', (0, 317, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|          | 3/435 [00:05<12:16,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 22\n",
            "('Cannot warp empty image with dimensions', (0, 250, 3))\n",
            "error for split:test index: 23\n",
            "('Cannot warp empty image with dimensions', (0, 250, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 13/435 [00:23<12:46,  1.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 105\n",
            "('Cannot warp empty image with dimensions', (0, 259, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  4%|▎         | 16/435 [00:28<12:31,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 132\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▍         | 20/435 [00:36<14:31,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 163\n",
            "('Cannot warp empty image with dimensions', (0, 190, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  6%|▌         | 24/435 [00:46<16:00,  2.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 196\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  6%|▌         | 25/435 [00:48<14:53,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 200\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 206\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▊         | 37/435 [01:16<16:01,  2.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 296\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  9%|▉         | 39/435 [01:21<16:34,  2.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 317\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 40/435 [01:23<16:15,  2.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 320\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 11%|█         | 48/435 [01:43<16:06,  2.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 383\n",
            "('Cannot warp empty image with dimensions', (112, 0, 3))\n",
            "error for split:test index: 388\n",
            "('Cannot warp empty image with dimensions', (203, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 11%|█▏        | 49/435 [01:45<14:36,  2.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 396\n",
            "('Cannot warp empty image with dimensions', (0, 196, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 12%|█▏        | 52/435 [01:51<12:56,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 421\n",
            "('Cannot warp empty image with dimensions', (0, 349, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 12%|█▏        | 53/435 [01:52<12:19,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 425\n",
            "('Cannot warp empty image with dimensions', (0, 310, 3))\n",
            "error for split:test index: 430\n",
            "('Cannot warp empty image with dimensions', (58, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 55/435 [01:56<12:37,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 438\n",
            "('Cannot warp empty image with dimensions', (0, 286, 3))\n",
            "error for split:test index: 439\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 441\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 57/435 [02:01<13:24,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 462\n",
            "('Cannot warp empty image with dimensions', (0, 366, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 14%|█▎        | 59/435 [02:05<13:44,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 471\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 474\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 475\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 476\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 60/435 [02:07<11:57,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 479\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 16%|█▌        | 69/435 [02:26<12:17,  2.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 555\n",
            "('Cannot warp empty image with dimensions', (0, 462, 3))\n",
            "error for split:test index: 558\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 74/435 [02:38<14:57,  2.49s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 594\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 597\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 76/435 [02:43<14:37,  2.44s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 612\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 18%|█▊        | 80/435 [02:54<15:42,  2.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 644\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 98/435 [03:33<12:23,  2.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 789\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 23%|██▎       | 100/435 [03:40<15:45,  2.82s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 801\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 101/435 [03:42<15:22,  2.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 813\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 23%|██▎       | 102/435 [03:45<15:26,  2.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 816\n",
            "('Cannot warp empty image with dimensions', (114, 0, 3))\n",
            "error for split:test index: 817\n",
            "('Cannot warp empty image with dimensions', (115, 0, 3))\n",
            "error for split:test index: 818\n",
            "('Cannot warp empty image with dimensions', (129, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▎       | 103/435 [03:48<14:59,  2.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 828\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 24%|██▍       | 104/435 [03:50<14:31,  2.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 838\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▍       | 107/435 [03:56<12:14,  2.24s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 859\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▌       | 111/435 [04:05<12:18,  2.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 891\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 26%|██▋       | 115/435 [04:14<11:42,  2.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 920\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 923\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 28%|██▊       | 120/435 [04:24<11:13,  2.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 964\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 28%|██▊       | 121/435 [04:26<10:47,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 974\n",
            "('Cannot warp empty image with dimensions', (0, 295, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 29%|██▊       | 125/435 [04:33<09:46,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1003\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 29%|██▉       | 126/435 [04:35<09:44,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1010\n",
            "('Cannot warp empty image with dimensions', (0, 166, 3))\n",
            "error for split:test index: 1011\n",
            "('Cannot warp empty image with dimensions', (0, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|██▉       | 129/435 [04:42<10:58,  2.15s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1035\n",
            "('Cannot warp empty image with dimensions', (0, 160, 3))\n",
            "error for split:test index: 1036\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 131/435 [04:46<10:27,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1047\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 132/435 [04:48<10:22,  2.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1056\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|███       | 134/435 [04:52<09:51,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1070\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1071\n",
            "('Cannot warp empty image with dimensions', (423, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███       | 135/435 [04:54<09:47,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1083\n",
            "('Cannot warp empty image with dimensions', (0, 355, 3))\n",
            "error for split:test index: 1084\n",
            "('Cannot warp empty image with dimensions', (0, 331, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 136/435 [04:56<09:47,  1.97s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1094\n",
            "('Cannot warp empty image with dimensions', (114, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 31%|███▏      | 137/435 [04:58<09:53,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1097\n",
            "('Cannot warp empty image with dimensions', (133, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 32%|███▏      | 138/435 [05:00<11:08,  2.25s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1108\n",
            "('Cannot warp empty image with dimensions', (149, 0, 3))\n",
            "error for split:test index: 1109\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 143/435 [05:11<10:14,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1145\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 145/435 [05:15<09:33,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1159\n",
            "('Cannot warp empty image with dimensions', (0, 185, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 34%|███▍      | 150/435 [05:26<10:22,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1202\n",
            "('Cannot warp empty image with dimensions', (0, 232, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▌      | 155/435 [05:35<09:14,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1240\n",
            "('Cannot warp empty image with dimensions', (0, 288, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 159/435 [05:42<08:39,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1274\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1278\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 161/435 [05:47<09:08,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1293\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 162/435 [05:49<09:38,  2.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1300\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1301\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1302\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 37%|███▋      | 163/435 [05:51<08:58,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1304\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 169/435 [06:03<09:22,  2.11s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1353\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 39%|███▉      | 171/435 [06:08<10:33,  2.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1371\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 172/435 [06:11<10:33,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1378\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 176/435 [06:21<10:16,  2.38s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1408\n",
            "('Cannot warp empty image with dimensions', (270, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████▏     | 180/435 [06:29<09:12,  2.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1446\n",
            "('Cannot warp empty image with dimensions', (276, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 44%|████▍     | 192/435 [06:53<08:04,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1538\n",
            "('Cannot warp empty image with dimensions', (0, 295, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▍     | 194/435 [06:57<07:47,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1552\n",
            "('Cannot warp empty image with dimensions', (0, 434, 3))\n",
            "error for split:test index: 1556\n",
            "('Cannot warp empty image with dimensions', (0, 0, 3))\n",
            "error for split:test index: 1557\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▍     | 195/435 [06:58<07:13,  1.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1561\n",
            "('Cannot warp empty image with dimensions', (0, 375, 3))\n",
            "error for split:test index: 1563\n",
            "('Cannot warp empty image with dimensions', (0, 394, 3))\n",
            "error for split:test index: 1566\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 196/435 [07:00<06:48,  1.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1569\n",
            "('Cannot warp empty image with dimensions', (0, 322, 3))\n",
            "error for split:test index: 1570\n",
            "('Cannot warp empty image with dimensions', (0, 347, 3))\n",
            "error for split:test index: 1572\n",
            "('Cannot warp empty image with dimensions', (0, 341, 3))\n",
            "error for split:test index: 1574\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 197/435 [07:01<06:18,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1577\n",
            "('Cannot warp empty image with dimensions', (0, 426, 3))\n",
            "error for split:test index: 1580\n",
            "('Cannot warp empty image with dimensions', (0, 425, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 198/435 [07:03<06:24,  1.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1584\n",
            "('Cannot warp empty image with dimensions', (0, 387, 3))\n",
            "error for split:test index: 1586\n",
            "('Cannot warp empty image with dimensions', (0, 370, 3))\n",
            "error for split:test index: 1588\n",
            "('Cannot warp empty image with dimensions', (0, 424, 3))\n",
            "error for split:test index: 1590\n",
            "('Cannot warp empty image with dimensions', (0, 403, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 199/435 [07:04<06:06,  1.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1593\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 46%|████▌     | 200/435 [07:06<06:13,  1.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1599\n",
            "('Cannot warp empty image with dimensions', (0, 482, 3))\n",
            "error for split:test index: 1603\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 208/435 [07:22<07:06,  1.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1666\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 49%|████▊     | 211/435 [07:27<07:02,  1.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1689\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▊     | 212/435 [07:29<06:51,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1699\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 218/435 [07:41<07:11,  1.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1744\n",
            "('Cannot warp empty image with dimensions', (87, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 51%|█████▏    | 224/435 [07:54<07:51,  2.23s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1796\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 225/435 [07:56<08:01,  2.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1802\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 226/435 [07:59<08:05,  2.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1812\n",
            "('Cannot warp empty image with dimensions', (84, 0, 3))\n",
            "error for split:test index: 1813\n",
            "('Cannot warp empty image with dimensions', (0, 258, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 52%|█████▏    | 228/435 [08:02<07:13,  2.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1826\n",
            "('Cannot warp empty image with dimensions', (0, 293, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 53%|█████▎    | 230/435 [08:06<06:46,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1840\n",
            "('Cannot warp empty image with dimensions', (0, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 231/435 [08:08<06:32,  1.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1852\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1853\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 53%|█████▎    | 232/435 [08:09<05:58,  1.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1855\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1856\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 54%|█████▎    | 233/435 [08:11<05:54,  1.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1864\n",
            "('Cannot warp empty image with dimensions', (0, 174, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▍    | 238/435 [08:21<06:49,  2.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1904\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 1907\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 239/435 [08:23<06:13,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1913\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 56%|█████▌    | 242/435 [08:28<05:41,  1.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1940\n",
            "('Cannot warp empty image with dimensions', (0, 134, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 243/435 [08:29<05:26,  1.70s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1946\n",
            "('Cannot warp empty image with dimensions', (0, 172, 3))\n",
            "error for split:test index: 1947\n",
            "('Cannot warp empty image with dimensions', (0, 169, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 244/435 [08:31<05:07,  1.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 1956\n",
            "('Cannot warp empty image with dimensions', (0, 214, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 57%|█████▋    | 250/435 [08:42<05:59,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2000\n",
            "('Cannot warp empty image with dimensions', (225, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 252/435 [08:46<05:58,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2016\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 59%|█████▉    | 257/435 [08:56<05:56,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2059\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|█████▉    | 259/435 [09:00<05:45,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2071\n",
            "('Cannot warp empty image with dimensions', (34, 0, 3))\n",
            "error for split:test index: 2072\n",
            "('Cannot warp empty image with dimensions', (33, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|█████▉    | 260/435 [09:02<05:40,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2080\n",
            "('Cannot warp empty image with dimensions', (0, 150, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 62%|██████▏   | 270/435 [09:21<05:06,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2163\n",
            "('Cannot warp empty image with dimensions', (64, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 64%|██████▍   | 279/435 [09:38<04:57,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2235\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2236\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▍   | 280/435 [09:40<04:49,  1.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2240\n",
            "('Cannot warp empty image with dimensions', (0, 62, 3))\n",
            "error for split:test index: 2243\n",
            "('Cannot warp empty image with dimensions', (0, 57, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 281/435 [09:41<04:42,  1.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2252\n",
            "('Cannot warp empty image with dimensions', (247, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▍   | 282/435 [09:43<04:40,  1.83s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2258\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 284/435 [09:48<05:01,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2273\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2277\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 287/435 [09:54<05:01,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2300\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2301\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 66%|██████▌   | 288/435 [09:55<04:55,  2.01s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2308\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 292/435 [10:04<05:11,  2.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2335\n",
            "('Cannot warp empty image with dimensions', (0, 28, 3))\n",
            "error for split:test index: 2339\n",
            "('Cannot warp empty image with dimensions', (0, 286, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 67%|██████▋   | 293/435 [10:06<04:49,  2.04s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2342\n",
            "('Cannot warp empty image with dimensions', (0, 406, 3))\n",
            "error for split:test index: 2343\n",
            "('Cannot warp empty image with dimensions', (0, 431, 3))\n",
            "error for split:test index: 2346\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2350\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 294/435 [10:08<04:38,  1.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2353\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 68%|██████▊   | 295/435 [10:10<04:53,  2.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2360\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 305/435 [10:31<04:28,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2442\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 71%|███████   | 308/435 [10:37<04:17,  2.03s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2469\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 71%|███████   | 309/435 [10:39<04:00,  1.91s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2471\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 73%|███████▎  | 316/435 [10:53<04:03,  2.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2531\n",
            "('Cannot warp empty image with dimensions', (245, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 317/435 [10:55<03:48,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2539\n",
            "('Cannot warp empty image with dimensions', (0, 364, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 318/435 [10:56<03:29,  1.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2542\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2543\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 319/435 [10:58<03:28,  1.80s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2554\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 74%|███████▍  | 322/435 [11:03<03:20,  1.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2574\n",
            "('Cannot warp empty image with dimensions', (0, 298, 3))\n",
            "error for split:test index: 2575\n",
            "('Cannot warp empty image with dimensions', (0, 367, 3))\n",
            "error for split:test index: 2577\n",
            "('Cannot warp empty image with dimensions', (0, 413, 3))\n",
            "error for split:test index: 2578\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2579\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2580\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2581\n",
            "('Cannot warp empty image with dimensions', (0, 474, 3))\n",
            "error for split:test index: 2582\n",
            "('Cannot warp empty image with dimensions', (0, 402, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 323/435 [11:04<02:45,  1.47s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2583\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2586\n",
            "('Cannot warp empty image with dimensions', (0, 299, 3))\n",
            "error for split:test index: 2587\n",
            "('Cannot warp empty image with dimensions', (0, 290, 3))\n",
            "error for split:test index: 2588\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 74%|███████▍  | 324/435 [11:06<02:47,  1.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2595\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 327/435 [11:12<03:29,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2618\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2621\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 328/435 [11:14<03:26,  1.93s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2629\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 76%|███████▌  | 330/435 [11:18<03:30,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2642\n",
            "('Cannot warp empty image with dimensions', (0, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 76%|███████▌  | 331/435 [11:20<03:23,  1.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2647\n",
            "('Cannot warp empty image with dimensions', (0, 112, 3))\n",
            "error for split:test index: 2652\n",
            "stack expects a non-empty TensorList\n",
            "error for split:test index: 2653\n",
            "('Cannot warp empty image with dimensions', (110, 0, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 78%|███████▊  | 339/435 [11:36<03:18,  2.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2712\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|███████▉  | 346/435 [11:51<03:09,  2.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2769\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 349/435 [11:57<02:51,  2.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2798\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 81%|████████  | 351/435 [12:01<02:51,  2.05s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2809\n",
            "'NoneType' object is not subscriptable\n",
            "error for split:test index: 2813\n",
            "('Cannot warp empty image with dimensions', (0, 194, 3))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 81%|████████  | 352/435 [12:03<02:42,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2816\n",
            "stack expects a non-empty TensorList\n",
            "error for split:test index: 2817\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 82%|████████▏ | 355/435 [12:08<02:35,  1.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2844\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 360/435 [12:18<02:26,  1.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2885\n",
            "stack expects a non-empty TensorList\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▎ | 364/435 [12:25<02:11,  1.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2911\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 365/435 [12:28<02:13,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2926\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 84%|████████▍ | 366/435 [12:29<02:11,  1.90s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "error for split:test index: 2934\n",
            "'NoneType' object is not subscriptable\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▍ | 369/435 [12:36<02:17,  2.08s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHANGE VAL TO SPLIT\n",
        "!cp data/saved_features/face_embeddings_val.pt backup\n",
        "!cp data/saved_features/pose_embeddings_val.pt backup\n",
        "!cp data/saved_features/real_indexes_val.pt backup\n",
        "!cp data/saved_features/text_embeddings_val.pt backup"
      ],
      "metadata": {
        "id": "CoD3xPvejhVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -sh backup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRixo4HQj7ns",
        "outputId": "30a601e2-af9f-4f59-f271-7a927e0f4fdc"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access 'backup': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls data/saved_features"
      ],
      "metadata": {
        "id": "3glQ-cNPcHUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!du data/saved_features/face_embeddings_val.pt -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcKT0wDo6aJu",
        "outputId": "aa3ef5b8-c768-4728-fc1b-270bf2664336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "17M\tdata/saved_features/face_embeddings_val.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save_path = project_path / 'data' / 'saved_features'\n",
        "# split = \"val\"\n",
        "# face_embeddings = []\n",
        "# for i in range(3):\n",
        "#     face_embeddings.append(torch.load(save_path / f'face_embeddings_{split}_{i}.pt'))\n",
        "# face_embeddings = torch.cat(face_embeddings, dim=0)\n",
        "# print(face_embeddings.shape)\n",
        "# torch.save(face_embeddings, save_path / f'face_embeddings_{split}_{batch_index}.pt')\n",
        "# del face_embeddings"
      ],
      "metadata": {
        "id": "bBSVsSK9cNAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del dataset\n",
        "del dataloader"
      ],
      "metadata": {
        "id": "jhltrQPAiuAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7phGAYdn7LL"
      },
      "source": [
        "#Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCCY1P7Jlr01"
      },
      "outputs": [],
      "source": [
        "class MSCTDDataLoader:\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    if isinstance(data, dict):\n",
        "        return {k: to_device(v, device) for k, v in data.items()}\n",
        "    if isinstance(data, str):\n",
        "        return data\n",
        "    return data.to(device)\n",
        "\n",
        "# ds = MSCTDDataSet(base_path=project_path + \"data/\", dataset_type = \"val\", load=True)\n",
        "# dl = DataLoader(ds, batch_size=10)\n",
        "# dl = MSCTDDataLoader(dl, device)\n",
        "# for x in dl:\n",
        "#   print(x)\n",
        "#   print(x['face_embedding'].shape)\n",
        "#   print(x['text_embedding'].shape)\n",
        "#   print(x['real_index'])\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta6yhrOr0SjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SimpleDenseNetwork(nn.Module):\n",
        "    def __init__(self, n_classes, embedding_dimension):\n",
        "        super(SimpleDenseNetwork, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.embedding_dimension,\n",
        "                out_features=512,\n",
        "            ),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=512, out_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=128, out_features=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Softmax(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        x = input_batch\n",
        "        x = self.fc(x)\n",
        "        output_batch = x\n",
        "\n",
        "        return output_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHPJSCmn_4Y"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFra0LEuGTb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "num_workers = 1\n",
        "EPOCHS = 1\n",
        "# embedding_dimension = 2048 + 34\n",
        "embedding_dimension = FACE_EMBEDDING_SIZE + TEXT_EMBEDDING_SIZE + POSE_EMBEDDING_SIZE # + SCENE_EMBEDDING_SIZE\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "data_size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtzGPVTplryF"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def train_epoch(epoch_index, model, dataloader, loss_fn, optimizer):\n",
        "    running_loss = 0.0\n",
        "    # last_loss = 0.0\n",
        "\n",
        "    for data_pair_index, batch in enumerate(dataloader):\n",
        "        print(\"--------------\", data_pair_index, \"-------------\")\n",
        "        errors = (batch[\"pose_embeding\"]==-123).all(dim=1)\n",
        "        text_embedding = batch[\"text_embedding\"][~errors]\n",
        "        face_embedding = batch[\"face_embedding\"][[~errors]]\n",
        "        pose_embeding = batch[\"pose_embeding\"][[~errors]]\n",
        "\n",
        "        labels = batch[\"sentiment\"]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(torch.cat((face_embedding, text_embedding, pose_embeding), 1))\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        # if data_pair_index % 1000 == 999:\n",
        "        #     last_loss = running_loss / 1000  # loss per batch\n",
        "        #     print(\"  batch {} loss: {}\".format(data_pair_index + 1, last_loss))\n",
        "        #     tb_x = epoch_index * len(dataloader) + data_pair_index + 1\n",
        "        #     print(\"Loss/train\", last_loss, tb_x)\n",
        "        #     running_loss = 0.0\n",
        "    print('Batch loss: ', running_loss)\n",
        "    # return last_loss\n",
        "\n",
        "\n",
        "def train_model(model, epochs, train_dataloader, val_dataloader):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    for epoch in trange(epochs):\n",
        "        model.train()\n",
        "        train_epoch(epoch, model, train_dataloader, loss_fn, optimizer)\n",
        "        model.eval()\n",
        "        validate(model, val_dataloader, loss_fn)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWLiQFmvtvyd"
      },
      "outputs": [],
      "source": [
        "model = SimpleDenseNetwork(n_classes=3, embedding_dimension=embedding_dimension).to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbxK8X8ftyIM"
      },
      "outputs": [],
      "source": [
        "val_dataset = MSCTDDataSet(project_path + \"data/\", \"val\", data_size=data_size, load=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "# val_dataloader = MSCTDDataLoader(val_dataloader, device)\n",
        "\n",
        "test_dataset = MSCTDDataSet(project_path + \"data/\", \"test\", load=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "# test_dataloader = MSCTDDataLoader(test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InvQze-IzLf0",
        "outputId": "db339e96-0ccb-470b-e31a-355a10a86f77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:276: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- 0 -------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- 1 -------------\n",
            "-------------- 2 -------------\n",
            "-------------- 3 -------------\n",
            "-------------- 4 -------------\n"
          ]
        }
      ],
      "source": [
        "model = train_model(model, EPOCHS, val_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8_-tAeMhMUG"
      },
      "source": [
        "#Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2B5IZCzhUAv"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "\n",
        "def validate(model, dataloader, loss_fn):\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "    for data_pair_index, batch in enumerate(dataloader):\n",
        "        print(\"--------------\", data_pair_index, \"-------------\")\n",
        "        text_embedding = batch[\"text_embedding\"]\n",
        "        face_embedding = batch[\"face\"]\n",
        "        # pose_embeding = batch[\"pose_embeding\"]\n",
        "        labels = batch[\"sentiment\"]\n",
        "\n",
        "        logits = model(torch.cat((text_embedding, face_embedding), 1))\n",
        "        # print(outputs)\n",
        "        accuracy.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
        "        precision.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        running_loss += loss.item()\n",
        "        # print(running_loss)\n",
        "        # print('true answer',labels)\n",
        "        # print('prediction',logits.argmax(dim=1))\n",
        "        # if data_pair_index==2:\n",
        "        #   break\n",
        "    print(accuracy.compute())\n",
        "    print(precision.compute(average=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v90Uk-4akcFR"
      },
      "outputs": [],
      "source": [
        "validate(model, test_dataloader, nn.CrossEntropyLoss())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "h7phGAYdn7LL"
      ],
      "name": "Full.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}