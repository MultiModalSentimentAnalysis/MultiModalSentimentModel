{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjAnNBpam-Jg"
      },
      "outputs": [],
      "source": [
        "!pip install scipy scikit-image torch tqdm transformers mediapipe opencv-python torchvision numpy pandas timm evaluate facenet-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5KkoGi4r12v",
        "outputId": "71825895-18a0-4294-d89b-15e6fd77d555"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "drive.mount(\"/content/drive\")\n",
        "project_path = Path(\"/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B75co_pTBvdO",
        "outputId": "940829d0-9ffd-4244-9458-fb78b9e80e71"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2e9lCQoCIwo",
        "outputId": "e31af207-7802-493c-9765-83921c42cec2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "correct_indexes    image_index_test.txt   sentiment_train.txt\n",
            "dev.zip\t\t   image_index_train.txt  sentiment_val.txt\n",
            "english_test.txt   image_index_val.txt\t  test.zip\n",
            "english_train.txt  images\t\t  train_ende.zip\n",
            "english_val.txt    saved_features\n",
            "error_indexes\t   sentiment_test.txt\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r_Wnev6CBnm"
      },
      "outputs": [],
      "source": [
        "!unzip dev.zip\n",
        "!unzip test.zip\n",
        "!unzip train_ende.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-rhhJ0rETml"
      },
      "outputs": [],
      "source": [
        "%mv dev/ images/val\n",
        "%mv test/ images/test\n",
        "%mv train_ende/ images/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ_jlOxeD5bk",
        "outputId": "04248d64-8b7b-4dc7-9e96-1a50997c101a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5068\n"
          ]
        }
      ],
      "source": [
        "%ls images/test -1 | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQ2DTtLSXNso",
        "outputId": "98fa6a61-97c9-4604-821a-f90dbc70a532"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxkroo-MvXsO",
        "outputId": "56bdecbc-0482-4529-c33f-b46b9e011490"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xhQ_BP_YpNbs"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms import transforms as transforms\n",
        "from torchvision.models.detection import KeypointRCNN_ResNet50_FPN_Weights\n",
        "\n",
        "\n",
        "class PoseEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        device='cpu'\n",
        "    ):\n",
        "        self.model = torchvision.models.detection.keypointrcnn_resnet50_fpn(weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT,num_keypoints=17).to(device)\n",
        "        self.model.eval()\n",
        "        self.device = device\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def extract_embedding(self, image):\n",
        "        image = self.transform(image)\n",
        "        image = image.unsqueeze(0).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(image)\n",
        "        \n",
        "        keypoints_scores = outputs[0]['keypoints_scores']\n",
        "        best_score = torch.mean(keypoints_scores, axis=1).argmax().item()\n",
        "        keypoints = outputs[0]['keypoints'][best_score,:,:2]\n",
        "        return keypoints.ravel()\n",
        "\n",
        "# p = PoseEmbeddingExtractor(device=device)\n",
        "# path = 'data/images/val/4965.jpg'\n",
        "# img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
        "# p.extract_embedding(img).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8TacWJSYfBda"
      },
      "outputs": [],
      "source": [
        "import sys, os, torch, cv2\n",
        "from pathlib import Path\n",
        "\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def remove_non_poses(input_dir, split):\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    pee = PoseEmbeddingExtractor(device=device)\n",
        "    file_name = f\"pose_error_{split}.txt\"\n",
        "    os.makedirs(\"./data/error_indexes\", exist_ok=True)\n",
        "    non_pose_files = open(f\"./data/error_indexes/{file_name}\", \"w\")\n",
        "    img_pattern = os.path.join(input_dir, \"*.jpg\")\n",
        "    images = glob(img_pattern)\n",
        "    for image_path in tqdm(images):\n",
        "        try:\n",
        "            img = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
        "            pee.extract_embedding(img)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(e)\n",
        "            img_id = image_path.split(\"/\")[-1].split(\".\")[0]\n",
        "            non_pose_files.write(f\"{img_id}\")\n",
        "            non_pose_files.write(os.linesep)\n",
        "\n",
        "\n",
        "# split = \"test\"\n",
        "# input_dir = project_path / \"data\" / \"images\" / split\n",
        "# remove_non_poses(input_dir, split)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQgq9LD6nnis"
      },
      "source": [
        "#Face Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iBk_w05clql7"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "import math\n",
        "from skimage.transform import rotate\n",
        "from mtcnn import MTCNN\n",
        "from facenet_pytorch import MTCNN as MTCNN2\n",
        "import mediapipe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import urllib\n",
        "\n",
        "\n",
        "def get_model_path(model_name):\n",
        "    model_file = model_name + \".pt\"\n",
        "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".hsemotions\")\n",
        "    # cache_dir = \"emotion_models\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fpath = os.path.join(cache_dir, model_file)\n",
        "    if not os.path.isfile(fpath):\n",
        "        print(f\"{model_file} not exists\")\n",
        "        url = (\n",
        "            \"https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/\"\n",
        "            + model_file\n",
        "            + \"?raw=true\"\n",
        "        )\n",
        "        print(\"Downloading\", model_name, \"from\", url)\n",
        "        urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class FaceAlignment:\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rotation_on_images(input_images, angles):\n",
        "        rotated_images = [\n",
        "            rotate(image, angle) for image, angle in zip(input_images, angles)\n",
        "        ]\n",
        "        return rotated_images\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_alignment_rotation_(eyes_coordinates):\n",
        "        angles = []\n",
        "        directions = []\n",
        "        for left_eye_coordinate, right_eye_coordinate in eyes_coordinates:\n",
        "\n",
        "            left_eye_x, left_eye_y = left_eye_coordinate\n",
        "            right_eye_x, right_eye_y = right_eye_coordinate\n",
        "\n",
        "            triangle_vertex = (\n",
        "                (right_eye_x, left_eye_y)\n",
        "                if left_eye_y > right_eye_y\n",
        "                else (left_eye_x, right_eye_y)\n",
        "            )\n",
        "            direction = (\n",
        "                -1 if left_eye_y > right_eye_y else 1\n",
        "            )  # rotate clockwise else counter-clockwise\n",
        "\n",
        "            # compute length of triangle edges\n",
        "            a = euclidean(left_eye_coordinate, triangle_vertex)\n",
        "            b = euclidean(right_eye_coordinate, triangle_vertex)\n",
        "            c = euclidean(right_eye_coordinate, left_eye_coordinate)\n",
        "\n",
        "            # cosine rule\n",
        "            if (\n",
        "                b != 0 and c != 0\n",
        "            ):  # this multiplication causes division by zero in cos_a calculation\n",
        "                cos_a = (b**2 + c**2 - a**2) / (2 * b * c)\n",
        "                angle = np.arccos(cos_a)  # angle in radian\n",
        "                angle = (angle * 180) / math.pi  # radian to degree\n",
        "            else:\n",
        "                angle = 0\n",
        "\n",
        "            angle = angle - 90 if direction == -1 else angle\n",
        "\n",
        "            angles.append(angle)\n",
        "            directions.append(direction)\n",
        "\n",
        "        return angles, directions\n",
        "\n",
        "\n",
        "class FaceDetection:\n",
        "\n",
        "    # first call extract_face\n",
        "    def __init__(self, model_name, minimum_confidence):\n",
        "\n",
        "        self.detected_faces_information = None\n",
        "        self.model_name = model_name\n",
        "        self.minimum_confidence = minimum_confidence\n",
        "        if model_name == \"MTCNN\":\n",
        "            detector_model = MTCNN2(device=device)\n",
        "            self.detect_faces_function = (\n",
        "                lambda input_image: detector_model.detect(input_image, landmarks=True)\n",
        "            )\n",
        "\n",
        "    def extract_faces(self, input_image, return_detections_information=True):\n",
        "        self.detect_faces__(input_image)\n",
        "        faces = self.get_faces__(\n",
        "            input_image,\n",
        "        )\n",
        "        if return_detections_information:\n",
        "            return faces, self.detected_faces_information\n",
        "\n",
        "        else:\n",
        "            return faces\n",
        "\n",
        "    def detect_faces__(self, input_image):\n",
        "        detections = self.detect_faces_function(input_image)\n",
        "        detections = [\n",
        "            {\n",
        "                'box': detections[0][i],\n",
        "                'confidence': detections[1][i],\n",
        "                'keypoints': {\n",
        "                    'left_eye': detections[2][i][0],\n",
        "                    'right_eye': detections[2][i][1],\n",
        "                    'nose': detections[2][i][2],\n",
        "                    'mouth_left': detections[2][i][3], \n",
        "                    'mouth_right': detections[2][i][4]\n",
        "                }\n",
        "              \n",
        "            }\n",
        "            for i in range(detections[0].shape[0])]\n",
        "        self.detected_faces_information = list(\n",
        "            filter(\n",
        "                lambda element: element[\"confidence\"] > self.minimum_confidence,\n",
        "                detections,\n",
        "            )\n",
        "        )\n",
        "\n",
        "\n",
        "    def get_detected_faces_information(self):\n",
        "        return self.detected_faces_information\n",
        "\n",
        "    def get_keypoints(\n",
        "        self,\n",
        "    ):\n",
        "        return list(\n",
        "            map(lambda element: element[\"keypoints\"], self.detected_faces_information)\n",
        "        )\n",
        "\n",
        "    def get_faces__(\n",
        "        self,\n",
        "        input_image,\n",
        "    ):\n",
        "        boxes = [\n",
        "            detection_information[\"box\"]\n",
        "            for detection_information in self.detected_faces_information\n",
        "        ]\n",
        "        y1y2x1x2 = [(int(y), int(y2), int(x), int(x2)) for x, y, x2, y2 in boxes]\n",
        "        faces = [input_image[y1:y2, x1:x2] for y1, y2, x1, x2 in y1y2x1x2]\n",
        "        return faces\n",
        "\n",
        "    def get_eyes_coordinates(\n",
        "        self,\n",
        "    ):\n",
        "        eyes_coordinates = [\n",
        "            (info[\"keypoints\"][\"left_eye\"], info[\"keypoints\"][\"right_eye\"])\n",
        "            for info in self.detected_faces_information\n",
        "        ]\n",
        "        return eyes_coordinates\n",
        "\n",
        "\n",
        "class FaceEmotionRecognizer:\n",
        "    # supported values of model_name: enet_b0_8_best_vgaf, enet_b0_8_best_afew, enet_b2_8, enet_b0_8_va_mtl, enet_b2_7\n",
        "    def __init__(self, device, model_name=\"enet_b0_8_best_vgaf\"):\n",
        "        self.device = device\n",
        "        self.is_mtl = \"_mtl\" in model_name\n",
        "        if \"_7\" in model_name:\n",
        "            self.idx_to_class = {\n",
        "                0: \"Anger\",\n",
        "                1: \"Disgust\",\n",
        "                2: \"Fear\",\n",
        "                3: \"Happiness\",\n",
        "                4: \"Neutral\",\n",
        "                5: \"Sadness\",\n",
        "                6: \"Surprise\",\n",
        "            }\n",
        "        else:\n",
        "            self.idx_to_class = {\n",
        "                0: \"Anger\",\n",
        "                1: \"Contempt\",\n",
        "                2: \"Disgust\",\n",
        "                3: \"Fear\",\n",
        "                4: \"Happiness\",\n",
        "                5: \"Neutral\",\n",
        "                6: \"Sadness\",\n",
        "                7: \"Surprise\",\n",
        "            }\n",
        "\n",
        "        self.img_size = 224 if \"_b0_\" in model_name else 260\n",
        "        self.test_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((self.img_size, self.img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        path = get_model_path(model_name)\n",
        "\n",
        "        model = torch.load(path)\n",
        "        model = model.to(device)\n",
        "\n",
        "        if isinstance(model.classifier, torch.nn.Sequential):\n",
        "            self.classifier_weights = model.classifier[0].weight.data\n",
        "            self.classifier_bias = model.classifier[0].bias.data\n",
        "        else:\n",
        "            self.classifier_weights = model.classifier.weight.data\n",
        "            self.classifier_bias = model.classifier.bias.data\n",
        "\n",
        "        model.classifier = torch.nn.Identity()\n",
        "        self.model = model.eval()\n",
        "        # print(path, self.test_transforms)\n",
        "\n",
        "    def compute_probability(self, features):\n",
        "        return torch.matmul(features, self.classifier_weights.T) + self.classifier_bias\n",
        "\n",
        "    def extract_representations_from_faces(self, input_faces):\n",
        "        faces = [self.test_transforms(Image.fromarray(face)) for face in input_faces]\n",
        "        features = self.model(torch.stack(faces, dim=0).to(self.device))\n",
        "        return features\n",
        "\n",
        "    def predict_emotions_from_representations(\n",
        "        self, representations, logits=True, return_features=True\n",
        "    ):\n",
        "        scores = self.compute_probability(representations)\n",
        "        if self.is_mtl:\n",
        "            predictions_indices = torch.argmax(scores[:, :-2], dim=1)\n",
        "\n",
        "        else:\n",
        "            predictions_indices = torch.argmax(scores, dim=1)\n",
        "\n",
        "        if self.is_mtl:\n",
        "            x = scores[:, :-2]\n",
        "\n",
        "        else:\n",
        "            x = scores\n",
        "        pred = torch.argmax(x[0])\n",
        "\n",
        "        if not logits:\n",
        "            e_x = torch.exp(x - torch.max(x, dim=1)[:, None])\n",
        "            e_x = e_x / e_x.sum(dim=1)[:, None]\n",
        "            if self.is_mtl:\n",
        "                scores[:, :-2] = e_x\n",
        "            else:\n",
        "                scores = e_x\n",
        "\n",
        "        return [\n",
        "            self.idx_to_class[pred.item()] for pred in (predictions_indices)\n",
        "        ], scores\n",
        "\n",
        "\n",
        "class FaceNormalizer:\n",
        "    def __init__(self):\n",
        "        self.mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "        face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "        mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "        self.face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "        self.routes_idx = self.initialize__()\n",
        "\n",
        "    def initialize__(self):\n",
        "        df = pd.DataFrame(\n",
        "            list(self.mp_face_mesh.FACEMESH_FACE_OVAL), columns=[\"p1\", \"p2\"]\n",
        "        )\n",
        "        routes_idx = []\n",
        "\n",
        "        p1 = df.iloc[0][\"p1\"]\n",
        "        p2 = df.iloc[0][\"p2\"]\n",
        "\n",
        "        for i in range(0, df.shape[0]):\n",
        "            obj = df[df[\"p1\"] == p2]\n",
        "            p1 = obj[\"p1\"].values[0]\n",
        "            p2 = obj[\"p2\"].values[0]\n",
        "\n",
        "            route_idx = []\n",
        "            route_idx.append(p1)\n",
        "            route_idx.append(p2)\n",
        "            routes_idx.append(route_idx)\n",
        "\n",
        "        return routes_idx\n",
        "\n",
        "    def get_landmarks__(self, input_image: np.ndarray):\n",
        "        if input_image.dtype == np.float:\n",
        "            input_image = (input_image * 255).astype(np.uint8)\n",
        "\n",
        "        results = self.face_mesh.process(input_image)\n",
        "        landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "        routes = []\n",
        "        # for source_idx, target_idx in mp_face_mesh.FACEMESH_FACE_OVAL:\n",
        "        for source_idx, target_idx in self.routes_idx:\n",
        "            source = landmarks.landmark[source_idx]\n",
        "            target = landmarks.landmark[target_idx]\n",
        "\n",
        "            relative_source = (\n",
        "                int(input_image.shape[1] * source.x),\n",
        "                int(input_image.shape[0] * source.y),\n",
        "            )\n",
        "            relative_target = (\n",
        "                int(input_image.shape[1] * target.x),\n",
        "                int(input_image.shape[0] * target.y),\n",
        "            )\n",
        "\n",
        "            # cv2.line(img, relative_source, relative_target, (255, 255, 255), thickness = 2)\n",
        "\n",
        "            routes.append(relative_source)\n",
        "            routes.append(relative_target)\n",
        "\n",
        "        return routes\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_with_landmark_points__(input_image, landmarks):\n",
        "        mask = np.zeros((input_image.shape[0], input_image.shape[1]))\n",
        "        mask = cv2.fillConvexPoly(mask, np.array(landmarks), 1)\n",
        "        mask = mask.astype(bool)\n",
        "\n",
        "        out = np.zeros_like(input_image)\n",
        "        out[mask] = input_image[mask]\n",
        "        return out\n",
        "\n",
        "    def normalize_faces_image(self, input_images):\n",
        "        normalized_faces_images = [\n",
        "            self.normalize_with_landmark_points__(\n",
        "                input_image, self.get_landmarks__(input_image)\n",
        "            )\n",
        "            for input_image in input_images\n",
        "        ]\n",
        "        return normalized_faces_images\n",
        "\n",
        "\n",
        "class FaceEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        device='cuda'\n",
        "    ):\n",
        "        self.faces = None\n",
        "        self.normalized_rotated_faces = None\n",
        "        self.rotated_faces = None\n",
        "        self.rotation_angles = None\n",
        "        self.rotation_directions = None\n",
        "\n",
        "        fd = FaceDetection(\"MTCNN\", minimum_confidence=0.95)\n",
        "        self.face_detection_model: FaceDetection = fd\n",
        "        fa = FaceAlignment()\n",
        "        self.face_alignment_model: FaceAlignment = fa\n",
        "        fn = FaceNormalizer()\n",
        "        self.face_normalizer_model: FaceNormalizer = fn\n",
        "        model_name = \"enet_b0_8_best_afew\"\n",
        "        fer = FaceEmotionRecognizer(device, model_name)\n",
        "        self.face_emotion_recognition_model: FaceEmotionRecognizer = fer\n",
        "\n",
        "\n",
        "    def extract_embedding(self, input_image):\n",
        "        faces, detected_faces_information = self.face_detection_model.extract_faces(\n",
        "            input_image, return_detections_information=True\n",
        "        )\n",
        "\n",
        "        (\n",
        "            rotation_angles,\n",
        "            rotation_directions,\n",
        "        ) = self.face_alignment_model.compute_alignment_rotation_(\n",
        "            self.face_detection_model.get_eyes_coordinates()\n",
        "        )\n",
        "        rotated_faces = self.face_alignment_model.apply_rotation_on_images(\n",
        "            faces, rotation_angles\n",
        "        )\n",
        "        normalized_rotated_faces = self.face_normalizer_model.normalize_faces_image(\n",
        "            rotated_faces\n",
        "        )\n",
        "\n",
        "        normalized_rotated_faces_255 = [\n",
        "            (image * 255).astype(np.uint8) for image in normalized_rotated_faces\n",
        "        ]\n",
        "\n",
        "        representations = (\n",
        "            self.face_emotion_recognition_model.extract_representations_from_faces(\n",
        "                normalized_rotated_faces_255\n",
        "            )\n",
        "        )[0] #WARNING: 0 was not here\n",
        "        del normalized_rotated_faces_255\n",
        "        del normalized_rotated_faces\n",
        "        del rotated_faces\n",
        "        del rotation_angles\n",
        "        del rotation_directions\n",
        "        del faces\n",
        "        del detected_faces_information\n",
        "        # (\n",
        "        #     predictions,\n",
        "        #     scores,\n",
        "        # ) = self.face_emotion_recognition_model.predict_emotions_from_representations(\n",
        "        #     representations\n",
        "        # )\n",
        "\n",
        "        # self.faces = faces\n",
        "        # self.rotation_angles, self.rotation_directions = (\n",
        "        #     rotation_angles,\n",
        "        #     rotation_directions,\n",
        "        # )\n",
        "        # self.rotated_faces = rotated_faces\n",
        "        # self.normalized_rotated_faces = normalized_rotated_faces_255\n",
        "\n",
        "        return None, None, representations\n",
        "        # return preictions, scores, representations\n",
        "\n",
        "    def get_rotations_information(self):\n",
        "        return self.rotation_angles, self.rotation_directions\n",
        "\n",
        "    def get_faces(self):\n",
        "        return self.faces\n",
        "\n",
        "    def get_rotated_faces(self):\n",
        "        return self.rotated_faces\n",
        "\n",
        "    def get_normalized_rotated_faces(self):\n",
        "        return self.normalized_rotated_faces\n",
        "\n",
        "    def clear(self):\n",
        "        self.faces = None\n",
        "        self.normalized_rotated_faces = None\n",
        "        self.rotated_faces = None\n",
        "        self.rotation_angles = None\n",
        "        self.rotation_directions = None\n",
        "\n",
        "    def store_embeddings(self, file, embeddings):\n",
        "        with open(file, \"wb\") as file_out:\n",
        "            pickle.dump(\n",
        "                {\"embeddings\": embeddings}, file_out, protocol=pickle.HIGHEST_PROTOCOL\n",
        "            )\n",
        "\n",
        "    def load_embeddings(self, file):\n",
        "        with open(file, \"rb\") as file_in:\n",
        "            stored_data = pickle.load(file_in)\n",
        "            stored_embeddings = stored_data[\"embeddings\"]\n",
        "\n",
        "        return stored_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1E_norgnuy0"
      },
      "source": [
        "#Text Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Txucy8AKlr6Y"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from transformers import RobertaForSequenceClassification\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class TextEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"pysentimiento/robertuito-sentiment-analysis\",\n",
        "        show_progress_bar=True,\n",
        "        to_tensor=True,\n",
        "        max_length=128,\n",
        "        device='cuda'\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "            self.model_name, num_labels=3, output_hidden_states=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        self.generator = pipeline(\n",
        "            task=\"sentiment-analysis\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "    def extract_embedding(\n",
        "        self,\n",
        "        input_batch_sentences,\n",
        "    ):\n",
        "        encoded_input = self.tokenizer(\n",
        "            input_batch_sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model_output = self.model(**encoded_input)\n",
        "            hidden_states = model_output[\"hidden_states\"]\n",
        "            last_layer_hidden_states = hidden_states[\n",
        "                12\n",
        "            ]  # 12 = len(hidden_states) , dim = (batch_size, seq_len, 768)\n",
        "            cls_hidden_state = last_layer_hidden_states[:, 0, :]\n",
        "\n",
        "        return cls_hidden_state\n",
        "\n",
        "    def get_labels(self, input_batch_sentences):\n",
        "        return self.generator(input_batch_sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWFhqHGgn4b2"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JuUydse3k58x"
      },
      "outputs": [],
      "source": [
        "FACE_EMBEDDING_SIZE = 1280\n",
        "TEXT_EMBEDDING_SIZE = 768\n",
        "POSE_EMBEDDING_SIZE = 34\n",
        "SCENE_EMBEDDING_SIZE = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n-M-mOdJIJd",
        "outputId": "c72bb925-2920-43f1-86bf-28950f47f209"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:       13298572     4821020     2330164       15572     6147388     9495956\n",
            "Swap:             0           0           0\n"
          ]
        }
      ],
      "source": [
        "!free"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "3WAIa5q1lr3m"
      },
      "outputs": [],
      "source": [
        "import os, cv2, torch, ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import trange\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class MSCTDDataSet(Dataset):\n",
        "    \"\"\"MSCTD dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, base_path=\"data/\", split=\"train\", data_size=None, load=False, raw=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_path (str or path): path to data folder\n",
        "            split (str): dev, train, test\n",
        "        \"\"\"\n",
        "        if isinstance(base_path, str):\n",
        "            base_path = Path(base_path)\n",
        "        self.base_path = base_path\n",
        "        self.load_path = base_path / 'saved_features'\n",
        "        self.split = split\n",
        "        self.text_file_path = base_path / f\"english_{split}.txt\"\n",
        "        self.seq_file_path = base_path / f\"image_index_{split}.txt\"\n",
        "        self.sentiment_file_path = base_path / f\"sentiment_{split}.txt\"\n",
        "        self.image_dir = base_path / \"images\" / split\n",
        "        self.correct_indexes_file_path = base_path / \"correct_indexes\" / f\"correct_indexes_{split}.txt\"\n",
        "\n",
        "        self.data_size = data_size\n",
        "        self.load = load\n",
        "        self.raw = raw\n",
        "\n",
        "        self.texts = None\n",
        "        self.sentiments = None\n",
        "        self.indexes = None\n",
        "        self.face_embeddings = None\n",
        "        self.pose_embeddings = None\n",
        "        self.text_embeddings = None\n",
        "        self.load_data()\n",
        "        self.face_embedding_extractor = FaceEmbeddingExtractor(device=device)\n",
        "        self.text_embedding_extractor = TextEmbeddingExtractor(device=device)\n",
        "        self.pose_embedding_extractor = PoseEmbeddingExtractor(device=device)\n",
        "\n",
        "\n",
        "\n",
        "    def load_data(self):\n",
        "        with open(self.text_file_path) as text_file, open(self.sentiment_file_path) as sentiment_file, open(self.correct_indexes_file_path) as correct_file:\n",
        "            texts = [t.strip() for t in text_file.readlines()]\n",
        "            sentiments = [int(t.strip()) for t in sentiment_file.readlines()]\n",
        "            face_embeddings = None\n",
        "            pose_embeddings = None\n",
        "            text_embeddings = None\n",
        "            corrects = [int(c.strip()) for c in correct_file.readlines()]\n",
        "            if self.load:\n",
        "                try:\n",
        "                    face_embeddings = torch.load(self.load_path / f'face_embeddings_{self.split}.pt')\n",
        "                    pose_embeddings = torch.load(self.load_path / f'pose_embeddings_{self.split}.pt')\n",
        "                    text_embeddings = torch.load(self.load_path / f'text_embeddings_{self.split}.pt')\n",
        "                    corrects = torch.load(self.load_path / f'real_indexes_{self.split}.pt')\n",
        "                except Exception as e:\n",
        "                    print(e)\n",
        "                    print('Warning: passed load=True but not embedding file was located. Not loading')\n",
        "\n",
        "            correct_texts = [texts[i] for i in corrects]\n",
        "            correct_sentiments = [sentiments[i] for i in corrects]\n",
        "        # with open(self.image_index_path) as f:\n",
        "        #     images = [ast.literal_eval(t.strip()) for t in f.readlines()]\n",
        "\n",
        "        if self.data_size:\n",
        "            correct_texts = correct_texts[: self.data_size]\n",
        "            correct_sentiments =correct_sentiments[: self.data_size]\n",
        "            if not face_embeddings is None:\n",
        "                face_embeddings = face_embeddings[:self.data_size,:]\n",
        "            if not pose_embeddings is None:\n",
        "                pose_embeddings = pose_embeddings[:self.data_size,:]\n",
        "            if not text_embeddings is None:\n",
        "                text_embeddings = text_embeddings[:self.data_size,:]\n",
        "            # images = images[: self.data_size]\n",
        "\n",
        "\n",
        "        self.texts = correct_texts\n",
        "        self.sentiments = correct_sentiments\n",
        "        self.indexes = corrects\n",
        "        self.text_embeddings = text_embeddings\n",
        "        self.face_embeddings = face_embeddings\n",
        "        self.pose_embeddings = pose_embeddings\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def get_face_embedding(self, index, image):\n",
        "        if self.load:\n",
        "            return self.face_embeddings[index]\n",
        "        (\n",
        "            predictions,\n",
        "            scores,\n",
        "            representations,\n",
        "        ) = self.face_embedding_extractor.extract_embedding(image)\n",
        "        return representations\n",
        "\n",
        "    def get_pose_embedding(self, index, image):\n",
        "        if self.load:\n",
        "            return self.pose_embeddings[index]\n",
        "        return self.pose_embedding_extractor.extract_embedding(image)\n",
        "\n",
        "    def get_image_embeddings(self, index):\n",
        "        image = None\n",
        "        real_index = self.indexes[index]\n",
        "        image_name = self.image_dir / f\"{real_index}.jpg\"\n",
        "        if not self.load:\n",
        "            image = cv2.cvtColor(cv2.imread(str(image_name)), cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        face_embedding = self.get_face_embedding(index, image)\n",
        "        pose_embedding = self.get_pose_embedding(index, image)\n",
        "        return face_embedding, pose_embedding\n",
        "\n",
        "    def get_sentiment(self, index):\n",
        "        return self.sentiments[index]\n",
        "\n",
        "\n",
        "\n",
        "    def get_text(self, index):\n",
        "        if self.load and not self.text_embeddings is None:\n",
        "            return self.text_embeddings[index]\n",
        "        text = self.texts[index]\n",
        "        text = self.text_embedding_extractor.extract_embedding([text])[0]\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        try:\n",
        "            face_embedding, pose_embedding = self.get_image_embeddings(index)\n",
        "        except Exception as e:\n",
        "            print(f'error for split:{self.split} index: {index}')\n",
        "            print(e)\n",
        "            face_embedding = torch.ones(FACE_EMBEDDING_SIZE).to(device)*-123\n",
        "            pose_embedding = torch.ones(POSE_EMBEDDING_SIZE).to(device)*-123\n",
        "\n",
        "        sentiment = self.get_sentiment(index)\n",
        "        text = self.get_text(index)\n",
        "        sample = {\"real_index\": self.indexes[index], \"pose_embedding\": pose_embedding, \"face_embedding\": face_embedding, \"text_embedding\": text, \"sentiment\": sentiment}\n",
        "        if self.raw:\n",
        "            sample[\"text\"] = self.texts[index]\n",
        "            # sample[\"image\"] = pass \n",
        "\n",
        "        return sample\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5woPZsGkhvVI"
      },
      "source": [
        "# Save features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_5Ap_qX2mTwo"
      },
      "outputs": [],
      "source": [
        "# %mkdir data/saved_features/\n",
        "# %mkdir backups/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2lAmsLut-T7M"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "SAVE = False\n",
        "if SAVE:\n",
        "    SAVE_SPLIT = \"test\"\n",
        "    SAVE_BATCH = 8\n",
        "    dataset = MSCTDDataSet(base_path=project_path / \"data/\", split = SAVE_SPLIT, load=False)\n",
        "    print(len(dataset))\n",
        "    dataloader = DataLoader(dataset, batch_size=SAVE_BATCH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S--Fju2pTrym"
      },
      "outputs": [],
      "source": [
        "def save_features(dataloader, split):\n",
        "    save_path = project_path / 'data' / 'saved_features'\n",
        "    stop_batch = None\n",
        "\n",
        "    for batch_index, batch in enumerate(tqdm(dataloader)):\n",
        "        # print(batch[\"face_embedding\"].shape)\n",
        "        # print(batch[\"text_embedding\"].shape)\n",
        "        # print(batch[\"real_index\"].shape)\n",
        "        errors = (batch[\"pose_embedding\"]==-123).all(dim=1)\n",
        "\n",
        "        torch.save(batch[\"face_embedding\"][~errors], save_path / f'face_embeddings_{split}_{batch_index}.pt')\n",
        "        torch.save(batch[\"pose_embedding\"][~errors], save_path / f'pose_embeddings_{split}_{batch_index}.pt')\n",
        "        torch.save(batch[\"text_embedding\"][~errors], save_path / f'text_embeddings_{split}_{batch_index}.pt')\n",
        "        torch.save(batch[\"real_index\"][~errors], save_path / f'real_indexes_{split}_{batch_index}.pt')\n",
        "        if stop_batch and batch_index==stop_batch:\n",
        "          break\n",
        "\n",
        "\n",
        "    print('----------------------')\n",
        "    print(len(dataloader))\n",
        "    len_batch = len(dataloader)\n",
        "    if stop_batch:\n",
        "        len_batch = stop_batch\n",
        "    face_embeddings = []\n",
        "    for i in range(len_batch):\n",
        "        face_embeddings.append(torch.load(save_path / f'face_embeddings_{split}_{i}.pt'))\n",
        "    face_embeddings = torch.cat(face_embeddings, dim=0)\n",
        "    print(face_embeddings.shape)\n",
        "    torch.save(face_embeddings, save_path / f'face_embeddings_{split}.pt')\n",
        "    del face_embeddings\n",
        "\n",
        "    pose_embeddings = []\n",
        "    for i in range(len_batch):\n",
        "        pose_embeddings.append(torch.load(save_path / f'pose_embeddings_{split}_{i}.pt'))\n",
        "    pose_embeddings = torch.cat(pose_embeddings, dim=0)\n",
        "    print(pose_embeddings.shape)\n",
        "    torch.save(pose_embeddings, save_path / f'pose_embeddings_{split}.pt')\n",
        "    del pose_embeddings\n",
        "\n",
        "    text_embeddings = []\n",
        "    for i in range(len_batch):\n",
        "        text_embeddings.append(torch.load(save_path / f'text_embeddings_{split}_{i}.pt'))\n",
        "    text_embeddings = torch.cat(text_embeddings, dim=0)\n",
        "    print(text_embeddings.shape)\n",
        "    torch.save(text_embeddings, save_path / f'text_embeddings_{split}.pt')\n",
        "    del text_embeddings\n",
        "\n",
        "    real_indexes = []\n",
        "    for i in range(len_batch):\n",
        "        real_indexes.append(torch.load(save_path / f'real_indexes_{split}_{i}.pt'))\n",
        "    real_indexes = torch.cat(real_indexes, dim=0)\n",
        "    print(real_indexes.shape)\n",
        "    torch.save(real_indexes, save_path / f'real_indexes_{split}.pt')\n",
        "    del real_indexes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "1lJ7MYxih8eC"
      },
      "outputs": [],
      "source": [
        "if SAVE:\n",
        "    save_features(dataloader, SAVE_SPLIT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CoD3xPvejhVz"
      },
      "outputs": [],
      "source": [
        "# # CHANGE VAL TO SPLIT\n",
        "# !cp data/saved_features/face_embeddings_val.pt backup\n",
        "# !cp data/saved_features/pose_embeddings_val.pt backup\n",
        "# !cp data/saved_features/real_indexes_val.pt backup\n",
        "# !cp data/saved_features/text_embeddings_val.pt backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRixo4HQj7ns",
        "outputId": "5fcee866-19ec-45e8-85b1-8c92a0b246af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ls: cannot access 'backup': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!ls -sh backup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3glQ-cNPcHUK",
        "outputId": "11d57ade-2787-458c-8460-7dbfb957e91b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "face_embeddings_val.pt\n",
            "pose_embeddings_val.pt\n",
            "real_indexes_val.pt\n",
            "text_embeddings_val.pt\n"
          ]
        }
      ],
      "source": [
        "%ls data/saved_features | grep val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcKT0wDo6aJu",
        "outputId": "c37fcea6-15d0-4c57-dc16-ffa6705a2d0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "du: cannot access 'data/saved_features/text_embeddings_val.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!du data/saved_features/text_embeddings_val.pt -h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBSVsSK9cNAo"
      },
      "outputs": [],
      "source": [
        "# save_path = project_path / 'data' / 'saved_features'\n",
        "# split = \"val\"\n",
        "# face_embeddings = []\n",
        "# for i in range(3):\n",
        "#     face_embeddings.append(torch.load(save_path / f'face_embeddings_{split}_{i}.pt'))\n",
        "# face_embeddings = torch.cat(face_embeddings, dim=0)\n",
        "# print(face_embeddings.shape)\n",
        "# torch.save(face_embeddings, save_path / f'face_embeddings_{split}_{batch_index}.pt')\n",
        "# del face_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhltrQPAiuAJ"
      },
      "outputs": [],
      "source": [
        "del dataset\n",
        "del dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7phGAYdn7LL"
      },
      "source": [
        "#Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "HCCY1P7Jlr01"
      },
      "outputs": [],
      "source": [
        "class MSCTDDataLoader:\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    if isinstance(data, dict):\n",
        "        return {k: to_device(v, device) for k, v in data.items()}\n",
        "    if isinstance(data, str):\n",
        "        return data\n",
        "    return data.to(device)\n",
        "\n",
        "# ds = MSCTDDataSet(base_path=project_path + \"data/\", dataset_type = \"val\", load=True)\n",
        "# dl = DataLoader(ds, batch_size=10)\n",
        "# dl = MSCTDDataLoader(dl, device)\n",
        "# for x in dl:\n",
        "#   print(x)\n",
        "#   print(x['face_embedding'].shape)\n",
        "#   print(x['text_embedding'].shape)\n",
        "#   print(x['real_index'])\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Ta6yhrOr0SjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class SimpleDenseNetwork(nn.Module):\n",
        "    def __init__(self, n_classes, embedding_dimension):\n",
        "        super(SimpleDenseNetwork, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.embedding_dimension,\n",
        "                out_features=512,\n",
        "            ),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=512, out_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=128, out_features=3),\n",
        "            # nn.ReLU(inplace=True),\n",
        "            # nn.Softmax(dim=0),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        x = input_batch\n",
        "        x = self.fc(x)\n",
        "        output_batch = x\n",
        "\n",
        "        return output_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHPJSCmn_4Y"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "bXFra0LEuGTb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "num_workers = 1\n",
        "EPOCHS = 200\n",
        "# embedding_dimension = 2048 + 34\n",
        "embedding_dimension = FACE_EMBEDDING_SIZE + TEXT_EMBEDDING_SIZE + POSE_EMBEDDING_SIZE # + SCENE_EMBEDDING_SIZE\n",
        "\n",
        "learning_rate = 0.001\n",
        "momentum = 0.001\n",
        "data_size = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbxK8X8ftyIM",
        "outputId": "09e223cf-158f-4739-f66b-4373f00ce13a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3461, 1280])\n",
            "torch.Size([3461, 34])\n",
            "torch.Size([3461, 768])\n",
            "torch.Size([3240, 1280])\n",
            "torch.Size([3240, 34])\n",
            "torch.Size([3240, 768])\n"
          ]
        }
      ],
      "source": [
        "val_dataset = MSCTDDataSet(project_path / \"data\", \"val\", data_size=data_size, load=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "val_dataloader = MSCTDDataLoader(val_dataloader, device)\n",
        "\n",
        "test_dataset = MSCTDDataSet(project_path / \"data\", \"test\", load=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "test_dataloader = MSCTDDataLoader(test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "RtzGPVTplryF"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "def train_epoch(epoch_index, model, dataloader, loss_fn, optimizer):\n",
        "    running_loss = 0.0\n",
        "    # last_loss = 0.0\n",
        "\n",
        "    for data_pair_index, batch in enumerate(dataloader):\n",
        "        # print(\"--------------\", data_pair_index, \"-------------\")\n",
        "        errors = (batch[\"pose_embedding\"]==-123).all(dim=1)\n",
        "        text_embedding = batch[\"text_embedding\"][~errors]\n",
        "        face_embedding = batch[\"face_embedding\"][~errors]\n",
        "        pose_embedding = batch[\"pose_embedding\"][~errors]\n",
        "        labels = batch[\"sentiment\"][~errors]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # print(text_embedding)\n",
        "        # print(pose_embedding)\n",
        "        # print(face_embedding)\n",
        "        # print(text_embedding.shape)\n",
        "        # print(pose_embedding.shape)\n",
        "        # print(face_embedding.shape)\n",
        "        outputs = model(torch.cat((face_embedding, text_embedding, pose_embedding), 1))\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        # print(next(model.parameters()).grad)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        # if data_pair_index % 1000 == 999:\n",
        "        #     last_loss = running_loss / 1000  # loss per batch\n",
        "        #     print(\"  batch {} loss: {}\".format(data_pair_index + 1, last_loss))\n",
        "        #     tb_x = epoch_index * len(dataloader) + data_pair_index + 1\n",
        "        #     print(\"Loss/train\", last_loss, tb_x)\n",
        "        #     running_loss = 0.0\n",
        "    print('Epoch loss: ', running_loss)\n",
        "    # return last_loss\n",
        "\n",
        "\n",
        "def train_model(model, epochs, train_dataloader, val_dataloader):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(epochs):\n",
        "        print(\"--------------epoch: \", epoch, \"-------------\")\n",
        "        model.train()\n",
        "        train_epoch(epoch, model, train_dataloader, loss_fn, optimizer)\n",
        "        model.eval()\n",
        "        validate(model, val_dataloader, loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "oWLiQFmvtvyd"
      },
      "outputs": [],
      "source": [
        "model = SimpleDenseNetwork(n_classes=3, embedding_dimension=embedding_dimension).to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "InvQze-IzLf0",
        "outputId": "4a7cf38b-ecd8-4a7d-e7c6-60032c4317e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------epoch:  0 -------------\n",
            "Epoch loss:  102.55570125579834\n",
            "{'accuracy': 0.516358024691358}\n",
            "--------------epoch:  1 -------------\n",
            "Epoch loss:  115.83331173658371\n",
            "{'accuracy': 0.3317901234567901}\n",
            "--------------epoch:  2 -------------\n",
            "Epoch loss:  112.0691020488739\n",
            "{'accuracy': 0.5354938271604939}\n",
            "--------------epoch:  3 -------------\n",
            "Epoch loss:  115.20065319538116\n",
            "{'accuracy': 0.3345679012345679}\n",
            "--------------epoch:  4 -------------\n",
            "Epoch loss:  115.4446986913681\n",
            "{'accuracy': 0.42407407407407405}\n",
            "--------------epoch:  5 -------------\n",
            "Epoch loss:  111.48898482322693\n",
            "{'accuracy': 0.48148148148148145}\n",
            "--------------epoch:  6 -------------\n",
            "Epoch loss:  105.72664844989777\n",
            "{'accuracy': 0.5645061728395062}\n",
            "--------------epoch:  7 -------------\n",
            "Epoch loss:  112.43236154317856\n",
            "{'accuracy': 0.4604938271604938}\n",
            "--------------epoch:  8 -------------\n",
            "Epoch loss:  110.98285675048828\n",
            "{'accuracy': 0.3787037037037037}\n",
            "--------------epoch:  9 -------------\n",
            "Epoch loss:  111.6313825249672\n",
            "{'accuracy': 0.39228395061728394}\n",
            "--------------epoch:  10 -------------\n",
            "Epoch loss:  111.64676547050476\n",
            "{'accuracy': 0.44135802469135804}\n",
            "--------------epoch:  11 -------------\n",
            "Epoch loss:  112.66027837991714\n",
            "{'accuracy': 0.44969135802469135}\n",
            "--------------epoch:  12 -------------\n",
            "Epoch loss:  111.56000506877899\n",
            "{'accuracy': 0.504320987654321}\n",
            "--------------epoch:  13 -------------\n",
            "Epoch loss:  110.40358489751816\n",
            "{'accuracy': 0.483641975308642}\n",
            "--------------epoch:  14 -------------\n",
            "Epoch loss:  110.60562413930893\n",
            "{'accuracy': 0.4722222222222222}\n",
            "--------------epoch:  15 -------------\n",
            "Epoch loss:  110.0516722202301\n",
            "{'accuracy': 0.41358024691358025}\n",
            "--------------epoch:  16 -------------\n",
            "Epoch loss:  110.28221291303635\n",
            "{'accuracy': 0.462037037037037}\n",
            "--------------epoch:  17 -------------\n",
            "Epoch loss:  110.59531486034393\n",
            "{'accuracy': 0.4666666666666667}\n",
            "--------------epoch:  18 -------------\n",
            "Epoch loss:  109.68705677986145\n",
            "{'accuracy': 0.4580246913580247}\n",
            "--------------epoch:  19 -------------\n",
            "Epoch loss:  109.68706113100052\n",
            "{'accuracy': 0.41790123456790124}\n",
            "--------------epoch:  20 -------------\n",
            "Epoch loss:  107.89270842075348\n",
            "{'accuracy': 0.5330246913580247}\n",
            "--------------epoch:  21 -------------\n",
            "Epoch loss:  109.52342736721039\n",
            "{'accuracy': 0.4478395061728395}\n",
            "--------------epoch:  22 -------------\n",
            "Epoch loss:  108.10037630796432\n",
            "{'accuracy': 0.47129629629629627}\n",
            "--------------epoch:  23 -------------\n",
            "Epoch loss:  110.72366100549698\n",
            "{'accuracy': 0.5055555555555555}\n",
            "--------------epoch:  24 -------------\n",
            "Epoch loss:  107.38803488016129\n",
            "{'accuracy': 0.5006172839506173}\n",
            "--------------epoch:  25 -------------\n",
            "Epoch loss:  108.8967153429985\n",
            "{'accuracy': 0.4919753086419753}\n",
            "--------------epoch:  26 -------------\n",
            "Epoch loss:  107.68694907426834\n",
            "{'accuracy': 0.5166666666666667}\n",
            "--------------epoch:  27 -------------\n",
            "Epoch loss:  107.26208651065826\n",
            "{'accuracy': 0.4660493827160494}\n",
            "--------------epoch:  28 -------------\n",
            "Epoch loss:  109.20099568367004\n",
            "{'accuracy': 0.44506172839506175}\n",
            "--------------epoch:  29 -------------\n",
            "Epoch loss:  109.75227165222168\n",
            "{'accuracy': 0.4506172839506173}\n",
            "--------------epoch:  30 -------------\n",
            "Epoch loss:  109.90336859226227\n",
            "{'accuracy': 0.4604938271604938}\n",
            "--------------epoch:  31 -------------\n",
            "Epoch loss:  109.01670491695404\n",
            "{'accuracy': 0.5216049382716049}\n",
            "--------------epoch:  32 -------------\n",
            "Epoch loss:  107.20917642116547\n",
            "{'accuracy': 0.4169753086419753}\n",
            "--------------epoch:  33 -------------\n",
            "Epoch loss:  110.51469486951828\n",
            "{'accuracy': 0.5074074074074074}\n",
            "--------------epoch:  34 -------------\n",
            "Epoch loss:  108.10253858566284\n",
            "{'accuracy': 0.5006172839506173}\n",
            "--------------epoch:  35 -------------\n",
            "Epoch loss:  107.5518679022789\n",
            "{'accuracy': 0.445679012345679}\n",
            "--------------epoch:  36 -------------\n",
            "Epoch loss:  108.54833871126175\n",
            "{'accuracy': 0.412962962962963}\n",
            "--------------epoch:  37 -------------\n",
            "Epoch loss:  106.71655434370041\n",
            "{'accuracy': 0.483641975308642}\n",
            "--------------epoch:  38 -------------\n",
            "Epoch loss:  107.79082989692688\n",
            "{'accuracy': 0.4287037037037037}\n",
            "--------------epoch:  39 -------------\n",
            "Epoch loss:  106.72868949174881\n",
            "{'accuracy': 0.44722222222222224}\n",
            "--------------epoch:  40 -------------\n",
            "Epoch loss:  108.95052069425583\n",
            "{'accuracy': 0.49166666666666664}\n",
            "--------------epoch:  41 -------------\n",
            "Epoch loss:  106.86150538921356\n",
            "{'accuracy': 0.4524691358024691}\n",
            "--------------epoch:  42 -------------\n",
            "Epoch loss:  107.2148699760437\n",
            "{'accuracy': 0.49907407407407406}\n",
            "--------------epoch:  43 -------------\n",
            "Epoch loss:  108.54196548461914\n",
            "{'accuracy': 0.37160493827160496}\n",
            "--------------epoch:  44 -------------\n",
            "Epoch loss:  107.85941922664642\n",
            "{'accuracy': 0.4669753086419753}\n",
            "--------------epoch:  45 -------------\n",
            "Epoch loss:  107.55172979831696\n",
            "{'accuracy': 0.4484567901234568}\n",
            "--------------epoch:  46 -------------\n",
            "Epoch loss:  106.93629759550095\n",
            "{'accuracy': 0.48549382716049383}\n",
            "--------------epoch:  47 -------------\n",
            "Epoch loss:  107.20190173387527\n",
            "{'accuracy': 0.5135802469135803}\n",
            "--------------epoch:  48 -------------\n",
            "Epoch loss:  106.87234479188919\n",
            "{'accuracy': 0.44166666666666665}\n",
            "--------------epoch:  49 -------------\n",
            "Epoch loss:  107.60737496614456\n",
            "{'accuracy': 0.36944444444444446}\n",
            "--------------epoch:  50 -------------\n",
            "Epoch loss:  108.13091379404068\n",
            "{'accuracy': 0.5391975308641975}\n",
            "--------------epoch:  51 -------------\n",
            "Epoch loss:  106.04466193914413\n",
            "{'accuracy': 0.39783950617283953}\n",
            "--------------epoch:  52 -------------\n",
            "Epoch loss:  108.48330354690552\n",
            "{'accuracy': 0.5033950617283951}\n",
            "--------------epoch:  53 -------------\n",
            "Epoch loss:  105.65530824661255\n",
            "{'accuracy': 0.4648148148148148}\n",
            "--------------epoch:  54 -------------\n",
            "Epoch loss:  109.25538331270218\n",
            "{'accuracy': 0.36882716049382713}\n",
            "--------------epoch:  55 -------------\n",
            "Epoch loss:  108.29530292749405\n",
            "{'accuracy': 0.39012345679012345}\n",
            "--------------epoch:  56 -------------\n",
            "Epoch loss:  106.44376975297928\n",
            "{'accuracy': 0.4962962962962963}\n",
            "--------------epoch:  57 -------------\n",
            "Epoch loss:  108.66664391756058\n",
            "{'accuracy': 0.4021604938271605}\n",
            "--------------epoch:  58 -------------\n",
            "Epoch loss:  107.10534292459488\n",
            "{'accuracy': 0.4669753086419753}\n",
            "--------------epoch:  59 -------------\n",
            "Epoch loss:  108.72280085086823\n",
            "{'accuracy': 0.5141975308641975}\n",
            "--------------epoch:  60 -------------\n",
            "Epoch loss:  106.33924025297165\n",
            "{'accuracy': 0.5098765432098765}\n",
            "--------------epoch:  61 -------------\n",
            "Epoch loss:  107.29983311891556\n",
            "{'accuracy': 0.4703703703703704}\n",
            "--------------epoch:  62 -------------\n",
            "Epoch loss:  105.969098508358\n",
            "{'accuracy': 0.49506172839506174}\n",
            "--------------epoch:  63 -------------\n",
            "Epoch loss:  107.14437657594681\n",
            "{'accuracy': 0.4324074074074074}\n",
            "--------------epoch:  64 -------------\n",
            "Epoch loss:  110.29200959205627\n",
            "{'accuracy': 0.3790123456790123}\n",
            "--------------epoch:  65 -------------\n",
            "Epoch loss:  107.29786849021912\n",
            "{'accuracy': 0.43333333333333335}\n",
            "--------------epoch:  66 -------------\n",
            "Epoch loss:  107.26640099287033\n",
            "{'accuracy': 0.38271604938271603}\n",
            "--------------epoch:  67 -------------\n",
            "Epoch loss:  107.4569982290268\n",
            "{'accuracy': 0.45925925925925926}\n",
            "--------------epoch:  68 -------------\n",
            "Epoch loss:  107.38795500993729\n",
            "{'accuracy': 0.48518518518518516}\n",
            "--------------epoch:  69 -------------\n",
            "Epoch loss:  107.34089994430542\n",
            "{'accuracy': 0.5018518518518519}\n",
            "--------------epoch:  70 -------------\n",
            "Epoch loss:  105.95229190587997\n",
            "{'accuracy': 0.4842592592592593}\n",
            "--------------epoch:  71 -------------\n",
            "Epoch loss:  107.17720055580139\n",
            "{'accuracy': 0.47160493827160493}\n",
            "--------------epoch:  72 -------------\n",
            "Epoch loss:  107.26899635791779\n",
            "{'accuracy': 0.5185185185185185}\n",
            "--------------epoch:  73 -------------\n",
            "Epoch loss:  105.39960300922394\n",
            "{'accuracy': 0.4546296296296296}\n",
            "--------------epoch:  74 -------------\n",
            "Epoch loss:  108.32824021577835\n",
            "{'accuracy': 0.30401234567901236}\n",
            "--------------epoch:  75 -------------\n",
            "Epoch loss:  112.02011966705322\n",
            "{'accuracy': 0.4617283950617284}\n",
            "--------------epoch:  76 -------------\n",
            "Epoch loss:  106.67778956890106\n",
            "{'accuracy': 0.44320987654320987}\n",
            "--------------epoch:  77 -------------\n",
            "Epoch loss:  106.74846678972244\n",
            "{'accuracy': 0.5382716049382716}\n",
            "--------------epoch:  78 -------------\n",
            "Epoch loss:  106.90965288877487\n",
            "{'accuracy': 0.43796296296296294}\n",
            "--------------epoch:  79 -------------\n",
            "Epoch loss:  105.14392691850662\n",
            "{'accuracy': 0.47962962962962963}\n",
            "--------------epoch:  80 -------------\n",
            "Epoch loss:  106.69814944267273\n",
            "{'accuracy': 0.47901234567901235}\n",
            "--------------epoch:  81 -------------\n",
            "Epoch loss:  106.8899199962616\n",
            "{'accuracy': 0.4367283950617284}\n",
            "--------------epoch:  82 -------------\n",
            "Epoch loss:  105.87299746274948\n",
            "{'accuracy': 0.44320987654320987}\n",
            "--------------epoch:  83 -------------\n",
            "Epoch loss:  106.97681337594986\n",
            "{'accuracy': 0.5030864197530864}\n",
            "--------------epoch:  84 -------------\n",
            "Epoch loss:  106.93324893712997\n",
            "{'accuracy': 0.39166666666666666}\n",
            "--------------epoch:  85 -------------\n",
            "Epoch loss:  107.5702173113823\n",
            "{'accuracy': 0.5305555555555556}\n",
            "--------------epoch:  86 -------------\n",
            "Epoch loss:  106.54569983482361\n",
            "{'accuracy': 0.4882716049382716}\n",
            "--------------epoch:  87 -------------\n",
            "Epoch loss:  105.69460606575012\n",
            "{'accuracy': 0.42993827160493825}\n",
            "--------------epoch:  88 -------------\n",
            "Epoch loss:  105.6621367931366\n",
            "{'accuracy': 0.5108024691358025}\n",
            "--------------epoch:  89 -------------\n",
            "Epoch loss:  104.81930214166641\n",
            "{'accuracy': 0.5126543209876543}\n",
            "--------------epoch:  90 -------------\n",
            "Epoch loss:  106.25995302200317\n",
            "{'accuracy': 0.5126543209876543}\n",
            "--------------epoch:  91 -------------\n",
            "Epoch loss:  106.22894555330276\n",
            "{'accuracy': 0.4802469135802469}\n",
            "--------------epoch:  92 -------------\n",
            "Epoch loss:  107.31214100122452\n",
            "{'accuracy': 0.48518518518518516}\n",
            "--------------epoch:  93 -------------\n",
            "Epoch loss:  105.83437913656235\n",
            "{'accuracy': 0.4959876543209877}\n",
            "--------------epoch:  94 -------------\n",
            "Epoch loss:  106.93736726045609\n",
            "{'accuracy': 0.45524691358024694}\n",
            "--------------epoch:  95 -------------\n",
            "Epoch loss:  106.27540284395218\n",
            "{'accuracy': 0.5046296296296297}\n",
            "--------------epoch:  96 -------------\n",
            "Epoch loss:  107.21103662252426\n",
            "{'accuracy': 0.5478395061728395}\n",
            "--------------epoch:  97 -------------\n",
            "Epoch loss:  105.7442883849144\n",
            "{'accuracy': 0.5052469135802469}\n",
            "--------------epoch:  98 -------------\n",
            "Epoch loss:  105.7076724767685\n",
            "{'accuracy': 0.4984567901234568}\n",
            "--------------epoch:  99 -------------\n",
            "Epoch loss:  105.50870287418365\n",
            "{'accuracy': 0.5052469135802469}\n",
            "--------------epoch:  100 -------------\n",
            "Epoch loss:  105.50572121143341\n",
            "{'accuracy': 0.4030864197530864}\n",
            "--------------epoch:  101 -------------\n",
            "Epoch loss:  106.03051716089249\n",
            "{'accuracy': 0.5234567901234568}\n",
            "--------------epoch:  102 -------------\n",
            "Epoch loss:  105.4029056429863\n",
            "{'accuracy': 0.36604938271604937}\n",
            "--------------epoch:  103 -------------\n",
            "Epoch loss:  105.50280785560608\n",
            "{'accuracy': 0.44753086419753085}\n",
            "--------------epoch:  104 -------------\n",
            "Epoch loss:  109.05285358428955\n",
            "{'accuracy': 0.3608024691358025}\n",
            "--------------epoch:  105 -------------\n",
            "Epoch loss:  109.1627396941185\n",
            "{'accuracy': 0.5407407407407407}\n",
            "--------------epoch:  106 -------------\n",
            "Epoch loss:  106.88317394256592\n",
            "{'accuracy': 0.5358024691358024}\n",
            "--------------epoch:  107 -------------\n",
            "Epoch loss:  104.97680073976517\n",
            "{'accuracy': 0.5367283950617284}\n",
            "--------------epoch:  108 -------------\n",
            "Epoch loss:  105.42991632223129\n",
            "{'accuracy': 0.4984567901234568}\n",
            "--------------epoch:  109 -------------\n",
            "Epoch loss:  105.11058217287064\n",
            "{'accuracy': 0.45493827160493827}\n",
            "--------------epoch:  110 -------------\n",
            "Epoch loss:  106.04999911785126\n",
            "{'accuracy': 0.5290123456790123}\n",
            "--------------epoch:  111 -------------\n",
            "Epoch loss:  105.89545738697052\n",
            "{'accuracy': 0.4626543209876543}\n",
            "--------------epoch:  112 -------------\n",
            "Epoch loss:  104.59701710939407\n",
            "{'accuracy': 0.4521604938271605}\n",
            "--------------epoch:  113 -------------\n",
            "Epoch loss:  106.08597671985626\n",
            "{'accuracy': 0.47129629629629627}\n",
            "--------------epoch:  114 -------------\n",
            "Epoch loss:  104.36857831478119\n",
            "{'accuracy': 0.5108024691358025}\n",
            "--------------epoch:  115 -------------\n",
            "Epoch loss:  105.40267813205719\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-124-614c768064e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-120-f0df2a06d236>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, epochs, train_dataloader, val_dataloader)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-84-a048cecac3a9>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(model, dataloader, loss_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# print(outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;31m# precision.add_batch(predictions=logits.argmax(dim=1), references=labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_feature_from_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 481\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_init_writer\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# Get cache file name and lock it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_file_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilelock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m                 \u001b[0mcache_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilelock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_cache_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# get ready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache_file_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilelock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilelock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/evaluate/module.py\u001b[0m in \u001b[0;36m_create_cache_file\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    268\u001b[0m             \u001b[0mfilelock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFileLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".lock\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m                 \u001b[0mfilelock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;31m# If we have reached the max number of attempts or we are not allow to find a free name (distributed setup)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/filelock.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, timeout, poll_intervall)\u001b[0m\n\u001b[1;32m    279\u001b[0m                         \u001b[0;34mf\"Lock {lock_id} not acquired on {lock_filename}, waiting {poll_intervall} seconds ...\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m                     )\n\u001b[0;32m--> 281\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll_intervall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;31m# Something did go wrong, so decrement the counter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "train_model(model, EPOCHS, val_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8_-tAeMhMUG"
      },
      "source": [
        "#Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "B2B5IZCzhUAv"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "# precision = evaluate.load(\"precision\")\n",
        "\n",
        "def validate(model, dataloader, loss_fn):\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "    for data_pair_index, batch in enumerate(dataloader):\n",
        "        # print(\"--------------\", data_pair_index, \"-------------\")\n",
        "        errors = (batch[\"pose_embedding\"]==-123).all(dim=1)\n",
        "        text_embedding = batch[\"text_embedding\"][~errors]\n",
        "        face_embedding = batch[\"face_embedding\"][~errors]\n",
        "        pose_embedding = batch[\"pose_embedding\"][~errors]\n",
        "        labels = batch[\"sentiment\"][~errors]\n",
        "\n",
        "        logits = model(torch.cat((face_embedding, text_embedding, pose_embedding), 1))\n",
        "\n",
        "        # print(outputs)\n",
        "        accuracy.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
        "        # precision.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        running_loss += loss.item()\n",
        "        # print(running_loss)\n",
        "        # print('true answer',labels)\n",
        "        # print('prediction',logits.argmax(dim=1))\n",
        "        # if data_pair_index==2:\n",
        "        #   break\n",
        "    print(accuracy.compute())\n",
        "    # print(precision.compute(average=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v90Uk-4akcFR"
      },
      "outputs": [],
      "source": [
        "validate(model, test_dataloader, nn.CrossEntropyLoss())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Full.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.11 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "de7383211b4ffff70067782259e5c1c9d8bdc7fbe1ab4077ac9d43b5a6892f49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
