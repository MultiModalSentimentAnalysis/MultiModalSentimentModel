{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sjAnNBpam-Jg"
   },
   "outputs": [],
   "source": [
    "!pip install scipy scikit-image torch tqdm transformers mediapipe opencv-python torchvision numpy pandas timm evaluate facenet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c5KkoGi4r12v",
    "outputId": "51e726fe-c71d-42e1-f382-c7f627f200ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B75co_pTBvdO",
    "outputId": "3dca57a6-b3ee-49b5-a335-3f10ea2aeb64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data\n"
     ]
    }
   ],
   "source": [
    "%cd drive/MyDrive/NLP/MultiModalEmotionRecognition/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p2e9lCQoCIwo",
    "outputId": "6a2323b5-4ce7-424b-94ea-50777ca8d27c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.zip  images  labels  README.md  test.zip  texts  train_ende.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5r_Wnev6CBnm"
   },
   "outputs": [],
   "source": [
    "!unzip dev.zip\n",
    "!unzip test.zip\n",
    "!unzip train_ende.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K-rhhJ0rETml"
   },
   "outputs": [],
   "source": [
    "%mv dev/ images/val\n",
    "%mv test/ images/test\n",
    "%mv train_ende/ images/train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIlQopjzdVRT",
    "outputId": "e46e2d5c-67b1-4e6e-ca03-224bc616da19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md  \u001b[0m\u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mval\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "%ls images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gJ_jlOxeD5bk",
    "outputId": "5f981aff-a550-48f3-de61-fb127ab36115"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20240\n",
      "5067\n",
      "5063\n"
     ]
    }
   ],
   "source": [
    "%ls images/train -1 | wc -l\n",
    "%ls images/test -1 | wc -l\n",
    "%ls images/val -1 | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hQ2DTtLSXNso",
    "outputId": "da712667-7ec4-4199-9c30-7712b092ec76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "UKIHhNVuX7eu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import math\n",
    "import mediapipe\n",
    "import cv2\n",
    "import os\n",
    "import urllib\n",
    "import torch\n",
    "import ast\n",
    "import pickle\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pathlib import Path\n",
    "from torchvision.transforms import transforms as transforms\n",
    "from torchvision.models.detection import KeypointRCNN_ResNet50_FPN_Weights\n",
    "from scipy.spatial.distance import euclidean\n",
    "from skimage.transform import rotate\n",
    "from facenet_pytorch import MTCNN as MTCNN\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from transformers import (\n",
    "    PreTrainedTokenizerFast,\n",
    "    pipeline,\n",
    "    RobertaForSequenceClassification,\n",
    ")\n",
    "from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "0zxIfhvZaA2y"
   },
   "outputs": [],
   "source": [
    "BASE_DIR = Path(\"/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\")\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "SAVE_DIR = DATA_DIR / \"saved_features\"\n",
    "TEXTS_DIR = DATA_DIR / \"texts\"\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "LABELS_DIR = DATA_DIR / \"labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "FhK4yxGGeLsV"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(SAVE_DIR):\n",
    "    os.makedirs(SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sxkroo-MvXsO",
    "outputId": "0acd883d-7bd7-439f-e90f-5f7335b4d300"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "JuUydse3k58x"
   },
   "outputs": [],
   "source": [
    "FACE_EMBEDDING_SIZE = 1280\n",
    "TEXT_EMBEDDING_SIZE = 768\n",
    "POSE_EMBEDDING_SIZE = 34\n",
    "SCENE_EMBEDDING_SIZE = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqSJWzTcejwm"
   },
   "source": [
    "# Pose Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xhQ_BP_YpNbs"
   },
   "outputs": [],
   "source": [
    "class PoseEmbeddingExtractor:\n",
    "    def __init__(self):\n",
    "        self.model = torchvision.models.detection.keypointrcnn_resnet50_fpn(\n",
    "            weights=KeypointRCNN_ResNet50_FPN_Weights.DEFAULT, num_keypoints=17\n",
    "        ).to(DEVICE)\n",
    "        self.model.eval()\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def extract_embedding(self, image):\n",
    "        image = self.transform(image)\n",
    "        image = image.unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(image)\n",
    "\n",
    "        keypoints_scores = outputs[0][\"keypoints_scores\"]\n",
    "        best_score = torch.mean(keypoints_scores, axis=1).argmax().item()\n",
    "        keypoints = outputs[0][\"keypoints\"][best_score, :, :2]\n",
    "        return keypoints.ravel()\n",
    "\n",
    "\n",
    "# p = PoseEmbeddingExtractor(device=DEVICE)\n",
    "# path = 'data/images/val/4965.jpg'\n",
    "# img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "# p.extract_embedding(img).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQgq9LD6nnis"
   },
   "source": [
    "#Face Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iBk_w05clql7"
   },
   "outputs": [],
   "source": [
    "def get_model_path(model_name):\n",
    "    model_file = model_name + \".pt\"\n",
    "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".hsemotions\")\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    fpath = os.path.join(cache_dir, model_file)\n",
    "    if not os.path.isfile(fpath):\n",
    "        print(f\"{model_file} not exists\")\n",
    "        url = (\n",
    "            \"https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/\"\n",
    "            + model_file\n",
    "            + \"?raw=true\"\n",
    "        )\n",
    "        print(\"Downloading\", model_name, \"from\", url)\n",
    "        urllib.request.urlretrieve(url, fpath)\n",
    "\n",
    "    return fpath\n",
    "\n",
    "\n",
    "class FaceAlignment:\n",
    "    def __init__(\n",
    "        self,\n",
    "    ):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def apply_rotation_on_images(input_images, angles):\n",
    "        rotated_images = [\n",
    "            rotate(image, angle) for image, angle in zip(input_images, angles)\n",
    "        ]\n",
    "        return rotated_images\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_alignment_rotation_(eyes_coordinates):\n",
    "        angles = []\n",
    "        directions = []\n",
    "        for left_eye_coordinate, right_eye_coordinate in eyes_coordinates:\n",
    "\n",
    "            left_eye_x, left_eye_y = left_eye_coordinate\n",
    "            right_eye_x, right_eye_y = right_eye_coordinate\n",
    "\n",
    "            triangle_vertex = (\n",
    "                (right_eye_x, left_eye_y)\n",
    "                if left_eye_y > right_eye_y\n",
    "                else (left_eye_x, right_eye_y)\n",
    "            )\n",
    "            direction = (\n",
    "                -1 if left_eye_y > right_eye_y else 1\n",
    "            )  # rotate clockwise else counter-clockwise\n",
    "\n",
    "            # compute length of triangle edges\n",
    "            a = euclidean(left_eye_coordinate, triangle_vertex)\n",
    "            b = euclidean(right_eye_coordinate, triangle_vertex)\n",
    "            c = euclidean(right_eye_coordinate, left_eye_coordinate)\n",
    "\n",
    "            # cosine rule\n",
    "            if (\n",
    "                b != 0 and c != 0\n",
    "            ):  # this multiplication causes division by zero in cos_a calculation\n",
    "                cos_a = (b**2 + c**2 - a**2) / (2 * b * c)\n",
    "                angle = np.arccos(cos_a)  # angle in radian\n",
    "                angle = (angle * 180) / math.pi  # radian to degree\n",
    "            else:\n",
    "                angle = 0\n",
    "\n",
    "            angle = angle - 90 if direction == -1 else angle\n",
    "\n",
    "            angles.append(angle)\n",
    "            directions.append(direction)\n",
    "\n",
    "        return angles, directions\n",
    "\n",
    "\n",
    "class FaceDetection:\n",
    "\n",
    "    # first call extract_face\n",
    "    def __init__(self, model_name, minimum_confidence):\n",
    "\n",
    "        self.detected_faces_information = None\n",
    "        self.model_name = model_name\n",
    "        self.minimum_confidence = minimum_confidence\n",
    "        if model_name == \"MTCNN\":\n",
    "            detector_model = MTCNN(device=DEVICE)\n",
    "            self.detect_faces_function = lambda input_image: detector_model.detect(\n",
    "                input_image, landmarks=True\n",
    "            )\n",
    "\n",
    "    def extract_faces(self, input_image, return_detections_information=True):\n",
    "        self.detect_faces__(input_image)\n",
    "        faces = self.get_faces__(\n",
    "            input_image,\n",
    "        )\n",
    "        if return_detections_information:\n",
    "            return faces, self.detected_faces_information\n",
    "\n",
    "        else:\n",
    "            return faces\n",
    "\n",
    "    def detect_faces__(self, input_image):\n",
    "        detections = self.detect_faces_function(input_image)\n",
    "        detections = [\n",
    "            {\n",
    "                \"box\": detections[0][i],\n",
    "                \"confidence\": detections[1][i],\n",
    "                \"keypoints\": {\n",
    "                    \"left_eye\": detections[2][i][0],\n",
    "                    \"right_eye\": detections[2][i][1],\n",
    "                    \"nose\": detections[2][i][2],\n",
    "                    \"mouth_left\": detections[2][i][3],\n",
    "                    \"mouth_right\": detections[2][i][4],\n",
    "                },\n",
    "            }\n",
    "            for i in range(detections[0].shape[0])\n",
    "        ]\n",
    "        self.detected_faces_information = list(\n",
    "            filter(\n",
    "                lambda element: element[\"confidence\"] > self.minimum_confidence,\n",
    "                detections,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def get_detected_faces_information(self):\n",
    "        return self.detected_faces_information\n",
    "\n",
    "    def get_keypoints(\n",
    "        self,\n",
    "    ):\n",
    "        return list(\n",
    "            map(lambda element: element[\"keypoints\"], self.detected_faces_information)\n",
    "        )\n",
    "\n",
    "    def get_faces__(\n",
    "        self,\n",
    "        input_image,\n",
    "    ):\n",
    "        boxes = [\n",
    "            detection_information[\"box\"]\n",
    "            for detection_information in self.detected_faces_information\n",
    "        ]\n",
    "        y1y2x1x2 = [(int(y), int(y2), int(x), int(x2)) for x, y, x2, y2 in boxes]\n",
    "        faces = [input_image[y1:y2, x1:x2] for y1, y2, x1, x2 in y1y2x1x2]\n",
    "        return faces\n",
    "\n",
    "    def get_eyes_coordinates(\n",
    "        self,\n",
    "    ):\n",
    "        eyes_coordinates = [\n",
    "            (info[\"keypoints\"][\"left_eye\"], info[\"keypoints\"][\"right_eye\"])\n",
    "            for info in self.detected_faces_information\n",
    "        ]\n",
    "        return eyes_coordinates\n",
    "\n",
    "\n",
    "class FaceEmotionRecognizer:\n",
    "    # supported values of model_name: enet_b0_8_best_vgaf, enet_b0_8_best_afew, enet_b2_8, enet_b0_8_va_mtl, enet_b2_7\n",
    "    def __init__(self, model_name=\"enet_b0_8_best_vgaf\"):\n",
    "        self.is_mtl = \"_mtl\" in model_name\n",
    "        if \"_7\" in model_name:\n",
    "            self.idx_to_class = {\n",
    "                0: \"Anger\",\n",
    "                1: \"Disgust\",\n",
    "                2: \"Fear\",\n",
    "                3: \"Happiness\",\n",
    "                4: \"Neutral\",\n",
    "                5: \"Sadness\",\n",
    "                6: \"Surprise\",\n",
    "            }\n",
    "        else:\n",
    "            self.idx_to_class = {\n",
    "                0: \"Anger\",\n",
    "                1: \"Contempt\",\n",
    "                2: \"Disgust\",\n",
    "                3: \"Fear\",\n",
    "                4: \"Happiness\",\n",
    "                5: \"Neutral\",\n",
    "                6: \"Sadness\",\n",
    "                7: \"Surprise\",\n",
    "            }\n",
    "\n",
    "        self.img_size = 224 if \"_b0_\" in model_name else 260\n",
    "        self.test_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((self.img_size, self.img_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        path = get_model_path(model_name)\n",
    "\n",
    "        model = torch.load(path)\n",
    "        model = model.to(DEVICE)\n",
    "\n",
    "        if isinstance(model.classifier, torch.nn.Sequential):\n",
    "            self.classifier_weights = model.classifier[0].weight.data\n",
    "            self.classifier_bias = model.classifier[0].bias.data\n",
    "        else:\n",
    "            self.classifier_weights = model.classifier.weight.data\n",
    "            self.classifier_bias = model.classifier.bias.data\n",
    "\n",
    "        model.classifier = torch.nn.Identity()\n",
    "        self.model = model.eval()\n",
    "        # print(path, self.test_transforms)\n",
    "\n",
    "    def compute_probability(self, features):\n",
    "        return torch.matmul(features, self.classifier_weights.T) + self.classifier_bias\n",
    "\n",
    "    def extract_representations_from_faces(self, input_faces):\n",
    "        faces = [self.test_transforms(Image.fromarray(face)) for face in input_faces]\n",
    "        features = self.model(torch.stack(faces, dim=0).to(DEVICE))\n",
    "        return features\n",
    "\n",
    "    def predict_emotions_from_representations(\n",
    "        self, representations, logits=True, return_features=True\n",
    "    ):\n",
    "        scores = self.compute_probability(representations)\n",
    "        if self.is_mtl:\n",
    "            predictions_indices = torch.argmax(scores[:, :-2], dim=1)\n",
    "\n",
    "        else:\n",
    "            predictions_indices = torch.argmax(scores, dim=1)\n",
    "\n",
    "        if self.is_mtl:\n",
    "            x = scores[:, :-2]\n",
    "\n",
    "        else:\n",
    "            x = scores\n",
    "        pred = torch.argmax(x[0])\n",
    "\n",
    "        if not logits:\n",
    "            e_x = torch.exp(x - torch.max(x, dim=1)[:, None])\n",
    "            e_x = e_x / e_x.sum(dim=1)[:, None]\n",
    "            if self.is_mtl:\n",
    "                scores[:, :-2] = e_x\n",
    "            else:\n",
    "                scores = e_x\n",
    "\n",
    "        return [\n",
    "            self.idx_to_class[pred.item()] for pred in (predictions_indices)\n",
    "        ], scores\n",
    "\n",
    "\n",
    "class FaceNormalizer:\n",
    "    def __init__(self):\n",
    "        self.mp_face_mesh = mediapipe.solutions.face_mesh\n",
    "        face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "\n",
    "        mp_face_mesh = mediapipe.solutions.face_mesh\n",
    "        self.face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
    "        self.routes_idx = self.initialize__()\n",
    "\n",
    "    def initialize__(self):\n",
    "        df = pd.DataFrame(\n",
    "            list(self.mp_face_mesh.FACEMESH_FACE_OVAL), columns=[\"p1\", \"p2\"]\n",
    "        )\n",
    "        routes_idx = []\n",
    "\n",
    "        p1 = df.iloc[0][\"p1\"]\n",
    "        p2 = df.iloc[0][\"p2\"]\n",
    "\n",
    "        for i in range(0, df.shape[0]):\n",
    "            obj = df[df[\"p1\"] == p2]\n",
    "            p1 = obj[\"p1\"].values[0]\n",
    "            p2 = obj[\"p2\"].values[0]\n",
    "\n",
    "            route_idx = []\n",
    "            route_idx.append(p1)\n",
    "            route_idx.append(p2)\n",
    "            routes_idx.append(route_idx)\n",
    "\n",
    "        return routes_idx\n",
    "\n",
    "    def get_landmarks__(self, input_image: np.ndarray):\n",
    "        if input_image.dtype == np.float:\n",
    "            input_image = (input_image * 255).astype(np.uint8)\n",
    "\n",
    "        results = self.face_mesh.process(input_image)\n",
    "        landmarks = results.multi_face_landmarks[0]\n",
    "\n",
    "        routes = []\n",
    "        # for source_idx, target_idx in mp_face_mesh.FACEMESH_FACE_OVAL:\n",
    "        for source_idx, target_idx in self.routes_idx:\n",
    "            source = landmarks.landmark[source_idx]\n",
    "            target = landmarks.landmark[target_idx]\n",
    "\n",
    "            relative_source = (\n",
    "                int(input_image.shape[1] * source.x),\n",
    "                int(input_image.shape[0] * source.y),\n",
    "            )\n",
    "            relative_target = (\n",
    "                int(input_image.shape[1] * target.x),\n",
    "                int(input_image.shape[0] * target.y),\n",
    "            )\n",
    "\n",
    "            # cv2.line(img, relative_source, relative_target, (255, 255, 255), thickness = 2)\n",
    "\n",
    "            routes.append(relative_source)\n",
    "            routes.append(relative_target)\n",
    "\n",
    "        return routes\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize_with_landmark_points__(input_image, landmarks):\n",
    "        mask = np.zeros((input_image.shape[0], input_image.shape[1]))\n",
    "        mask = cv2.fillConvexPoly(mask, np.array(landmarks), 1)\n",
    "        mask = mask.astype(bool)\n",
    "\n",
    "        out = np.zeros_like(input_image)\n",
    "        out[mask] = input_image[mask]\n",
    "        return out\n",
    "\n",
    "    def normalize_faces_image(self, input_images):\n",
    "        normalized_faces_images = [\n",
    "            self.normalize_with_landmark_points__(\n",
    "                input_image, self.get_landmarks__(input_image)\n",
    "            )\n",
    "            for input_image in input_images\n",
    "        ]\n",
    "        return normalized_faces_images\n",
    "\n",
    "\n",
    "class FaceEmbeddingExtractor:\n",
    "    def __init__(self):\n",
    "        self.faces = None\n",
    "        self.normalized_rotated_faces = None\n",
    "        self.rotated_faces = None\n",
    "        self.rotation_angles = None\n",
    "        self.rotation_directions = None\n",
    "\n",
    "        fd = FaceDetection(\"MTCNN\", minimum_confidence=0.95)\n",
    "        self.face_detection_model: FaceDetection = fd\n",
    "        fa = FaceAlignment()\n",
    "        self.face_alignment_model: FaceAlignment = fa\n",
    "        fn = FaceNormalizer()\n",
    "        self.face_normalizer_model: FaceNormalizer = fn\n",
    "        model_name = \"enet_b0_8_best_afew\"\n",
    "        fer = FaceEmotionRecognizer(model_name)\n",
    "        self.face_emotion_recognition_model: FaceEmotionRecognizer = fer\n",
    "\n",
    "    def extract_embedding(self, input_image):\n",
    "        faces, detected_faces_information = self.face_detection_model.extract_faces(\n",
    "            input_image, return_detections_information=True\n",
    "        )\n",
    "\n",
    "        (\n",
    "            rotation_angles,\n",
    "            rotation_directions,\n",
    "        ) = self.face_alignment_model.compute_alignment_rotation_(\n",
    "            self.face_detection_model.get_eyes_coordinates()\n",
    "        )\n",
    "        rotated_faces = self.face_alignment_model.apply_rotation_on_images(\n",
    "            faces, rotation_angles\n",
    "        )\n",
    "        normalized_rotated_faces = self.face_normalizer_model.normalize_faces_image(\n",
    "            rotated_faces\n",
    "        )\n",
    "\n",
    "        normalized_rotated_faces_255 = [\n",
    "            (image * 255).astype(np.uint8) for image in normalized_rotated_faces\n",
    "        ]\n",
    "\n",
    "        representations = (\n",
    "            self.face_emotion_recognition_model.extract_representations_from_faces(\n",
    "                normalized_rotated_faces_255\n",
    "            )\n",
    "        )[\n",
    "            0\n",
    "        ]  # WARNING: 0 was not here\n",
    "        del normalized_rotated_faces_255\n",
    "        del normalized_rotated_faces\n",
    "        del rotated_faces\n",
    "        del rotation_angles\n",
    "        del rotation_directions\n",
    "        del faces\n",
    "        del detected_faces_information\n",
    "        # (\n",
    "        #     predictions,\n",
    "        #     scores,\n",
    "        # ) = self.face_emotion_recognition_model.predict_emotions_from_representations(\n",
    "        #     representations\n",
    "        # )\n",
    "\n",
    "        # self.faces = faces\n",
    "        # self.rotation_angles, self.rotation_directions = (\n",
    "        #     rotation_angles,\n",
    "        #     rotation_directions,\n",
    "        # )\n",
    "        # self.rotated_faces = rotated_faces\n",
    "        # self.normalized_rotated_faces = normalized_rotated_faces_255\n",
    "\n",
    "        return None, None, representations\n",
    "        # return preictions, scores, representations\n",
    "\n",
    "    def get_rotations_information(self):\n",
    "        return self.rotation_angles, self.rotation_directions\n",
    "\n",
    "    def get_faces(self):\n",
    "        return self.faces\n",
    "\n",
    "    def get_rotated_faces(self):\n",
    "        return self.rotated_faces\n",
    "\n",
    "    def get_normalized_rotated_faces(self):\n",
    "        return self.normalized_rotated_faces\n",
    "\n",
    "    def clear(self):\n",
    "        self.faces = None\n",
    "        self.normalized_rotated_faces = None\n",
    "        self.rotated_faces = None\n",
    "        self.rotation_angles = None\n",
    "        self.rotation_directions = None\n",
    "\n",
    "    def store_embeddings(self, file, embeddings):\n",
    "        with open(file, \"wb\") as file_out:\n",
    "            pickle.dump(\n",
    "                {\"embeddings\": embeddings}, file_out, protocol=pickle.HIGHEST_PROTOCOL\n",
    "            )\n",
    "\n",
    "    def load_embeddings(self, file):\n",
    "        with open(file, \"rb\") as file_in:\n",
    "            stored_data = pickle.load(file_in)\n",
    "            stored_embeddings = stored_data[\"embeddings\"]\n",
    "\n",
    "        return stored_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1E_norgnuy0"
   },
   "source": [
    "#Text Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "Txucy8AKlr6Y"
   },
   "outputs": [],
   "source": [
    "class TextEmbeddingExtractor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name=\"pysentimiento/robertuito-sentiment-analysis\",\n",
    "        show_progress_bar=True,\n",
    "        to_tensor=True,\n",
    "        max_length=128,\n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.max_length = max_length\n",
    "        self.tokenizer = PreTrainedTokenizerFast.from_pretrained(self.model_name)\n",
    "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
    "            self.model_name, num_labels=3, output_hidden_states=True\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        self.generator = pipeline(\n",
    "            task=\"sentiment-analysis\",\n",
    "            model=self.model,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "\n",
    "    def extract_embedding(\n",
    "        self,\n",
    "        input_batch_sentences,\n",
    "    ):\n",
    "        encoded_input = self.tokenizer(\n",
    "            input_batch_sentences,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(DEVICE)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            hidden_states = model_output[\"hidden_states\"]\n",
    "            last_layer_hidden_states = hidden_states[\n",
    "                12\n",
    "            ]  # 12 = len(hidden_states) , dim = (batch_size, seq_len, 768)\n",
    "            cls_hidden_state = last_layer_hidden_states[:, 0, :]\n",
    "\n",
    "        return cls_hidden_state\n",
    "\n",
    "    def get_labels(self, input_batch_sentences):\n",
    "        return self.generator(input_batch_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWFhqHGgn4b2"
   },
   "source": [
    "#Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "3WAIa5q1lr3m"
   },
   "outputs": [],
   "source": [
    "class MSCTDDataSet(Dataset):\n",
    "    \"\"\"MSCTD dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        split=\"train\",\n",
    "        data_size=None,\n",
    "        load=False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            split (str): val, train, test.\n",
    "            data_size (int): None for full dataset. If provided dataset size will be reduced to data_size.\n",
    "            load (bool): If false, all embeddings will be extracted and dataset works with bare text and image. If true, it loads all pre extracted embeddings.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.text_file_path = TEXTS_DIR / f\"english_{split}.txt\"\n",
    "        self.sentiment_file_path = LABELS_DIR / f\"sentiment_{split}.txt\"\n",
    "        self.image_dir = IMAGES_DIR / split\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.load = load\n",
    "\n",
    "        self.texts = None\n",
    "        self.sentiments = None\n",
    "        self.indexes = None\n",
    "        self.face_embeddings = None\n",
    "        self.pose_embeddings = None\n",
    "        self.text_embeddings = None\n",
    "        self.load_data()\n",
    "        self.face_embedding_extractor = FaceEmbeddingExtractor()\n",
    "        self.text_embedding_extractor = TextEmbeddingExtractor()\n",
    "        self.pose_embedding_extractor = PoseEmbeddingExtractor()\n",
    "\n",
    "    # text is not valid\n",
    "    def load_data(self):\n",
    "        if self.load:\n",
    "            texts = None\n",
    "            indexes = torch.load(SAVE_DIR / f\"real_indexes_{self.split}.pt\").to(DEVICE)\n",
    "            sentiments = torch.load(SAVE_DIR / f\"sentiments_{self.split}.pt\").to(DEVICE)\n",
    "            face_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"face_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "            pose_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"pose_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "            text_embeddings = torch.load(\n",
    "                SAVE_DIR / f\"text_embeddings_{self.split}.pt\"\n",
    "            ).to(DEVICE)\n",
    "\n",
    "            assert (\n",
    "                face_embeddings.shape[0] == pose_embeddings.shape[0]\n",
    "            ), \"ERROR:  face and pose list are not the same size in loading\"\n",
    "            assert (\n",
    "                pose_embeddings.shape[0] == text_embeddings.shape[0]\n",
    "            ), \"ERROR: text and pose list are not the same size in loading\"\n",
    "            assert (\n",
    "                text_embeddings.shape[0] == indexes.shape[0]\n",
    "            ), \"ERROR: text and index list are not the same size in loading\"\n",
    "            assert (\n",
    "                indexes.shape[0] == sentiments.shape[0]\n",
    "            ), \"ERROR: index and sentiment list are not the same size in loading\"\n",
    "\n",
    "            print(face_embeddings.shape)\n",
    "            print(pose_embeddings.shape)\n",
    "            print(text_embeddings.shape)\n",
    "            print(indexes.shape)\n",
    "            print(sentiments.shape)\n",
    "\n",
    "        else:\n",
    "            with open(self.text_file_path) as text_file, open(\n",
    "                self.sentiment_file_path\n",
    "            ) as sentiment_file:\n",
    "                sentiments = [int(t.strip()) for t in sentiment_file.readlines()]\n",
    "                texts = [t.strip() for t in text_file.readlines()]\n",
    "                indexes = range(len(sentiments))\n",
    "                face_embeddings = None\n",
    "                pose_embeddings = None\n",
    "                text_embeddings = None\n",
    "\n",
    "        if self.data_size:\n",
    "            indexes = indexes[: self.data_size]\n",
    "            sentiments = sentiments[: self.data_size]\n",
    "            if not texts is None:\n",
    "                texts = texts[: self.data_size]\n",
    "            if not face_embeddings is None:\n",
    "                face_embeddings = face_embeddings[: self.data_size, :]\n",
    "            if not pose_embeddings is None:\n",
    "                pose_embeddings = pose_embeddings[: self.data_size, :]\n",
    "            if not text_embeddings is None:\n",
    "                text_embeddings = text_embeddings[: self.data_size, :]\n",
    "\n",
    "        self.texts = texts\n",
    "        self.sentiments = sentiments\n",
    "        self.indexes = indexes\n",
    "        self.face_embeddings = face_embeddings\n",
    "        self.pose_embeddings = pose_embeddings\n",
    "        self.text_embeddings = text_embeddings\n",
    "        # add assertion for this part\n",
    "        # assert face_embeddings.shape[0] == pose_embeddings.shape[0] , 'ERROR:  face and pose list are not the same size in loading'\n",
    "        # assert pose_embeddings.shape[0] == text_embeddings.shape[0] , 'ERROR: text and pose list are not the same size in loading'\n",
    "        # assert text_embeddings.shape[0] == indexes.shape[0] , 'ERROR: text and index list are not the same size in loading'\n",
    "        # assert indexes.shape[0] == sentiments.shape[0] , 'ERROR: index and sentiment list are not the same size in loading'\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.load:\n",
    "            return self.text_embeddings.shape[0]\n",
    "        return len(self.texts)\n",
    "\n",
    "    def get_face_embedding(self, image):\n",
    "        (\n",
    "            predictions,\n",
    "            scores,\n",
    "            representations,\n",
    "        ) = self.face_embedding_extractor.extract_embedding(image)\n",
    "        return representations\n",
    "\n",
    "    def get_pose_embedding(self, image):\n",
    "        return self.pose_embedding_extractor.extract_embedding(image)\n",
    "\n",
    "    def get_image_embeddings(self, index):\n",
    "        if self.load:\n",
    "            return self.face_embeddings[index], self.pose_embeddings[index]\n",
    "\n",
    "        image_name = self.image_dir / f\"{index}.jpg\"\n",
    "        image = cv2.cvtColor(cv2.imread(str(image_name)), cv2.COLOR_BGR2RGB)\n",
    "        face_embedding = self.get_face_embedding(image)\n",
    "        pose_embedding = self.get_pose_embedding(image)\n",
    "        return face_embedding, pose_embedding\n",
    "\n",
    "    def get_sentiment(self, index):\n",
    "        return self.sentiments[index]\n",
    "\n",
    "    def get_text(self, index):\n",
    "        if self.load:\n",
    "            return self.text_embeddings[index]\n",
    "        text = self.texts[index]\n",
    "        text = self.text_embedding_extractor.extract_embedding([text])[0]\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        try:\n",
    "            face_embedding, pose_embedding = self.get_image_embeddings(index)\n",
    "        except Exception as e:\n",
    "            print(f\"error for split:{self.split} index: {index}\")\n",
    "            print(e)\n",
    "            face_embedding = torch.ones(FACE_EMBEDDING_SIZE).to(DEVICE) * -123\n",
    "            pose_embedding = torch.ones(POSE_EMBEDDING_SIZE).to(DEVICE) * -123\n",
    "\n",
    "        sentiment = self.get_sentiment(index)\n",
    "        text_embedding = self.get_text(index)\n",
    "        sample = {\n",
    "            \"index\": self.indexes[index],\n",
    "            \"pose_embedding\": pose_embedding,\n",
    "            \"face_embedding\": face_embedding,\n",
    "            \"text_embedding\": text_embedding,\n",
    "            \"sentiment\": sentiment,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5woPZsGkhvVI"
   },
   "source": [
    "# Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2lAmsLut-T7M",
    "outputId": "d803ea1b-f44d-42ec-957b-cc18d56f78d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "SAVE = True\n",
    "if SAVE:\n",
    "    SAVE_SPLIT = \"val\"\n",
    "    SAVE_BATCH = 8\n",
    "    dataset = MSCTDDataSet(split=SAVE_SPLIT, data_size=100, load=False)\n",
    "    print(len(dataset))\n",
    "    dataloader = DataLoader(dataset, batch_size=SAVE_BATCH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "S--Fju2pTrym"
   },
   "outputs": [],
   "source": [
    "def save_features(dataloader, split):\n",
    "    stop_batch = None\n",
    "\n",
    "    for batch_index, batch in enumerate(tqdm(dataloader)):\n",
    "        # print('--------------------------------')\n",
    "        # print(batch_index)\n",
    "        # print(batch[\"face_embedding\"].shape)\n",
    "        # print(batch[\"text_embedding\"].shape)\n",
    "        # print(batch[\"real_index\"].shape)\n",
    "        # print(batch[\"sentiment\"].shape)\n",
    "        errors = (batch[\"pose_embedding\"] == -123).all(dim=1)\n",
    "\n",
    "        torch.save(\n",
    "            batch[\"face_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"face_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"pose_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"pose_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"text_embedding\"][~errors],\n",
    "            SAVE_DIR / f\"text_embeddings_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"index\"][~errors],\n",
    "            SAVE_DIR / f\"real_indexes_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        torch.save(\n",
    "            batch[\"sentiment\"][~errors],\n",
    "            SAVE_DIR / f\"sentiments_{split}_{batch_index}.pt\",\n",
    "        )\n",
    "        assert (\n",
    "            batch[\"pose_embedding\"].shape[0] == batch[\"text_embedding\"].shape[0]\n",
    "        ), \"text and pose list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"face_embedding\"].shape[0] == batch[\"pose_embedding\"].shape[0]\n",
    "        ), \"face and pose list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"text_embedding\"].shape[0] == batch[\"index\"].shape[0]\n",
    "        ), \"text and index list are not the same size in saving\"\n",
    "        assert (\n",
    "            batch[\"index\"].shape[0] == batch[\"sentiment\"].shape[0]\n",
    "        ), \"index and sentiment list are not the same size in saving\"\n",
    "\n",
    "        if stop_batch and batch_index == stop_batch:\n",
    "            break\n",
    "\n",
    "    print(\"----------------------\")\n",
    "    len_batch = len(dataloader)\n",
    "    if stop_batch:\n",
    "        len_batch = stop_batch\n",
    "    print(len(dataloader))\n",
    "\n",
    "    def concat_batches(name=\"face_embedding\"):\n",
    "        batches = []\n",
    "        for i in range(len_batch):\n",
    "            batches.append(torch.load(SAVE_DIR / f\"{name}_{split}_{i}.pt\"))\n",
    "        batches = torch.cat(batches, dim=0)\n",
    "        print(batches.shape)\n",
    "        torch.save(batches, SAVE_DIR / f\"{name}_{split}.pt\")\n",
    "        del batches\n",
    "\n",
    "    for name in [\n",
    "        \"face_embeddings\",\n",
    "        \"pose_embeddings\",\n",
    "        \"text_embeddings\",\n",
    "        \"sentiments\",\n",
    "        \"real_indexes\",\n",
    "    ]:\n",
    "        concat_batches(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1lJ7MYxih8eC"
   },
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    save_features(dataloader, SAVE_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "kuJ9SyzUjGc6"
   },
   "outputs": [],
   "source": [
    "if SAVE:\n",
    "    del dataset\n",
    "    del dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IylPiZuXin58"
   },
   "outputs": [],
   "source": [
    "%ls data/saved_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "CoD3xPvejhVz"
   },
   "outputs": [],
   "source": [
    "# CHANGE VAL TO SPLIT\n",
    "%mkdir backup\n",
    "!cp data/saved_features/face_embeddings_val.pt backup\n",
    "!cp data/saved_features/pose_embeddings_val.pt backup\n",
    "!cp data/saved_features/real_indexes_val.pt backup\n",
    "!cp data/saved_features/text_embeddings_val.pt backup\n",
    "!cp data/saved_features/sentiments_val.pt backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zRixo4HQj7ns",
    "outputId": "3525e747-dac7-4cf4-ea39-771fc93af6ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 462K\n",
      "281K face_embeddings_val.pt  1.5K sentiments_val.pt\n",
      "8.5K pose_embeddings_val.pt  169K text_embeddings_val.pt\n",
      "1.5K real_indexes_val.pt\n"
     ]
    }
   ],
   "source": [
    "!ls -sh backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3glQ-cNPcHUK",
    "outputId": "dab2f90e-f6bb-4bb8-c27f-1ec0bf363495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281K data/saved_features/face_embeddings_val.pt\n",
      "8.5K data/saved_features/pose_embeddings_val.pt\n",
      "1.5K data/saved_features/real_indexes_val.pt\n",
      "1.5K data/saved_features/sentiments_val.pt\n",
      "169K data/saved_features/text_embeddings_val.pt\n"
     ]
    }
   ],
   "source": [
    "# %ls -sh data/saved_features | grep face_embeddings_test.pt\n",
    "%ls -sh data/saved_features/*_val.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xcKT0wDo6aJu",
    "outputId": "f6f272f7-fcf8-4cdc-f281-00f07c9adbe2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169K\tdata/saved_features/text_embeddings_val.pt\n"
     ]
    }
   ],
   "source": [
    "!du data/saved_features/text_embeddings_val.pt -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7phGAYdn7LL"
   },
   "source": [
    "#Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HCCY1P7Jlr01"
   },
   "outputs": [],
   "source": [
    "class MSCTDDataLoader:\n",
    "    def __init__(self, dl):\n",
    "        self.dl = dl\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, DEVICE)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    return data.to(device)\n",
    "\n",
    "\n",
    "# ds = MSCTDDataSet(base_path=project_path + \"data/\", dataset_type = \"val\", load=True)\n",
    "# dl = DataLoader(ds, batch_size=10)\n",
    "# dl = MSCTDDataLoader(dl, device)\n",
    "# for x in dl:\n",
    "#   print(x)\n",
    "#   print(x['face_embedding'].shape)\n",
    "#   print(x['text_embedding'].shape)\n",
    "#   print(x['real_index'])\n",
    "#   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Ta6yhrOr0SjZ"
   },
   "outputs": [],
   "source": [
    "class SimpleDenseNetwork(nn.Module):\n",
    "    def __init__(self, n_classes, embedding_dimension):\n",
    "        super(SimpleDenseNetwork, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=self.embedding_dimension,\n",
    "                out_features=512,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=128, out_features=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BHPJSCmn_4Y"
   },
   "source": [
    "#Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "bXFra0LEuGTb"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_workers = 1\n",
    "EPOCHS = 10\n",
    "# embedding_dimension = 2048 + 34\n",
    "embedding_dimension = (\n",
    "    FACE_EMBEDDING_SIZE + TEXT_EMBEDDING_SIZE + POSE_EMBEDDING_SIZE\n",
    ")  # + SCENE_EMBEDDING_SIZE\n",
    "\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.001\n",
    "data_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bbxK8X8ftyIM",
    "outputId": "7f5d7b0a-37b8-49ca-8344-bd79582da841"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([56, 1280])\n",
      "torch.Size([56, 34])\n",
      "torch.Size([56, 768])\n",
      "torch.Size([56])\n",
      "torch.Size([56])\n"
     ]
    }
   ],
   "source": [
    "def get_dataset_and_dataloder(split, batch_size, data_size=None):\n",
    "    dataset = MSCTDDataSet(split, data_size=data_size, load=True)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    return dataset, dataloader\n",
    "    # dataloader = MSCTDDataLoader(dataloader)\n",
    "\n",
    "\n",
    "train_dataset, train_dataloader = get_dataset_and_dataloder(\"train\", BATCH_SIZE)\n",
    "val_dataset, val_dataloader = get_dataset_and_dataloder(\"val\", BATCH_SIZE)\n",
    "test_dataset, test_dataloader = get_dataset_and_dataloder(\"test\", BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8_-tAeMhMUG"
   },
   "source": [
    "#Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "B2B5IZCzhUAv"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "precision_macro = evaluate.load(\"precision\")\n",
    "precision_micro = evaluate.load(\"precision\")\n",
    "precision_weighted = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "f1_macro = evaluate.load(\"f1\")\n",
    "f1_micro = evaluate.load(\"f1\")\n",
    "f1_weighted = evaluate.load(\"f1\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "recall_macro = evaluate.load(\"recall\")\n",
    "recall_micro = evaluate.load(\"recall\")\n",
    "recall_weighted = evaluate.load(\"recall\")\n",
    "\n",
    "\n",
    "def validate(model, dataloader, loss_fn):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data_pair_index, batch in enumerate(dataloader):\n",
    "        # print(\"--------------\", data_pair_index, \"-------------\")\n",
    "        errors = (batch[\"pose_embedding\"] == -123).all(dim=1)\n",
    "        assert torch.all(~errors).item()\n",
    "\n",
    "        text_embedding = batch[\"text_embedding\"]\n",
    "        face_embedding = batch[\"face_embedding\"]\n",
    "        pose_embedding = batch[\"pose_embedding\"]\n",
    "        labels = batch[\"sentiment\"]\n",
    "        inputs = torch.cat((face_embedding, text_embedding, pose_embedding), 1)\n",
    "        logits = model(inputs)\n",
    "        probs = logits.argmax(dim=1)\n",
    "\n",
    "        accuracy.add_batch(predictions=probs, references=labels)\n",
    "        precision.add_batch(predictions=probs, references=labels)\n",
    "        precision_macro.add_batch(predictions=probs, references=labels)\n",
    "        precision_micro.add_batch(predictions=probs, references=labels)\n",
    "        precision_weighted.add_batch(predictions=probs, references=labels)\n",
    "        f1.add_batch(predictions=probs, references=labels)\n",
    "        f1_macro.add_batch(predictions=probs, references=labels)\n",
    "        f1_micro.add_batch(predictions=probs, references=labels)\n",
    "        f1_weighted.add_batch(predictions=probs, references=labels)\n",
    "        recall.add_batch(predictions=probs, references=labels)\n",
    "        recall_macro.add_batch(predictions=probs, references=labels)\n",
    "        recall_micro.add_batch(predictions=probs, references=labels)\n",
    "        recall_weighted.add_batch(predictions=probs, references=labels)\n",
    "\n",
    "        loss = loss_fn(logits, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(accuracy.compute())\n",
    "    print(precision.compute(average=None))\n",
    "    print(precision_macro.compute(average=\"macro\"))\n",
    "    print(precision_micro.compute(average=\"micro\"))\n",
    "    print(precision_weighted.compute(average=\"weighted\"))\n",
    "    print(f1.compute(average=None))\n",
    "    print(f1_macro.compute(average=\"macro\"))\n",
    "    print(f1_micro.compute(average=\"micro\"))\n",
    "    print(f1_weighted.compute(average=\"weighted\"))\n",
    "    print(recall.compute(average=None))\n",
    "    print(recall_macro.compute(average=\"macro\"))\n",
    "    print(recall_micro.compute(average=\"micro\"))\n",
    "    print(recall_weighted.compute(average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "RtzGPVTplryF"
   },
   "outputs": [],
   "source": [
    "def train_epoch(epoch_index, model, dataloader, loss_fn, optimizer):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for data_pair_index, batch in enumerate(dataloader):\n",
    "        # print(\"--------------\", data_pair_index, \"-------------\")\n",
    "        errors = (batch[\"pose_embedding\"] == -123).all(dim=1)\n",
    "        assert torch.all(~errors).item()\n",
    "        text_embedding = batch[\"text_embedding\"]\n",
    "        face_embedding = batch[\"face_embedding\"]\n",
    "        pose_embedding = batch[\"pose_embedding\"]\n",
    "        labels = batch[\"sentiment\"]\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print(text_embedding.shape)\n",
    "        # print(pose_embedding.shape)\n",
    "        # print(face_embedding.shape)\n",
    "        inputs = torch.cat((face_embedding, text_embedding, pose_embedding), 1)\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(\"Epoch loss: \", running_loss)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, train_dataloader, val_dataloader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"--------------epoch: \", epoch, \"-------------\")\n",
    "        model.train()\n",
    "        train_epoch(epoch, model, train_dataloader, loss_fn, optimizer)\n",
    "        model.eval()\n",
    "        validate(model, val_dataloader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "oWLiQFmvtvyd"
   },
   "outputs": [],
   "source": [
    "model = SimpleDenseNetwork(n_classes=3, embedding_dimension=embedding_dimension).to(\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExPXSnTztaC5"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InvQze-IzLf0"
   },
   "outputs": [],
   "source": [
    "train_model(model, EPOCHS, val_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v90Uk-4akcFR"
   },
   "outputs": [],
   "source": [
    "validate(model, val_dataloader, nn.CrossEntropyLoss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9oiWGTHeparu"
   },
   "outputs": [],
   "source": [
    "torch.save(model, SAVE_DIR / \"model.pt\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "zqSJWzTcejwm",
    "UQgq9LD6nnis"
   ],
   "name": "Full.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.8.11 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "de7383211b4ffff70067782259e5c1c9d8bdc7fbe1ab4077ac9d43b5a6892f49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
