{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjAnNBpam-Jg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca3b1420-efa8-406d-c134-dc15809e3a75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.7.3)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.7/dist-packages (0.18.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.12.1+cu113)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.1-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 25.2 MB/s \n",
            "\u001b[?25hCollecting mtcnn\n",
            "  Downloading mtcnn-0.1.1-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting mediapipe\n",
            "  Downloading mediapipe-0.8.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 32.9 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.6.0.66)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.7-py3-none-any.whl (509 kB)\n",
            "\u001b[K     |████████████████████████████████| 509 kB 62.6 MB/s \n",
            "\u001b[?25hCollecting evaluate\n",
            "  Downloading evaluate-0.2.2-py3-none-any.whl (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 9.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.6.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (1.3.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (3.2.2)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2.9.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (7.1.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image) (2021.11.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (3.0.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (4.1.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.15.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 13.6 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 62.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from mtcnn) (2.8.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (22.1.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.7/dist-packages (from mediapipe) (4.6.0.66)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from mediapipe) (1.2.0)\n",
            "Requirement already satisfied: protobuf<4,>=3.11 in /usr/local/lib/python3.7/dist-packages (from mediapipe) (3.17.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Collecting datasets>=2.0.0\n",
            "  Downloading datasets-2.4.0-py3-none-any.whl (365 kB)\n",
            "\u001b[K     |████████████████████████████████| 365 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from evaluate) (2022.7.1)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 59.8 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 55.5 MB/s \n",
            "\u001b[?25hCollecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from evaluate) (0.3.5.1)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 60.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.13.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, huggingface-hub, tokenizers, datasets, transformers, timm, mtcnn, mediapipe, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.4.0 evaluate-0.2.2 huggingface-hub-0.8.1 mediapipe-0.8.10.1 mtcnn-0.1.1 multiprocess-0.70.13 responses-0.18.0 timm-0.6.7 tokenizers-0.12.1 transformers-4.21.1 urllib3-1.25.11 xxhash-3.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scipy scikit-image torch tqdm transformers mtcnn mediapipe opencv-python torchvision numpy pandas timm evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5KkoGi4r12v",
        "outputId": "2249fae4-92b8-457b-ad41-e03d3d04d7de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "project_path = \"/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B75co_pTBvdO",
        "outputId": "8e9eb64f-66f4-4537-f1b8-03c074649b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/NLP/MultiModalEmotionRecognition/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2e9lCQoCIwo",
        "outputId": "c055cfc2-5ea8-4fc0-e4d7-c00691c58d62"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dev.zip\t\t image_index_val.txt  sentiment_val.txt  train_ende.zip\n",
            "english_val.txt  images\t\t      test.zip\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5r_Wnev6CBnm"
      },
      "outputs": [],
      "source": [
        "!unzip dev.zip\n",
        "!unzip test.zip\n",
        "!unzip train_ende.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-rhhJ0rETml"
      },
      "outputs": [],
      "source": [
        "%mv dev/ images/val\n",
        "%mv test/ images/test\n",
        "%mv train/ images/train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJ_jlOxeD5bk",
        "outputId": "b7038f5e-0d35-4bea-c69e-e9ad415c0f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20240\n"
          ]
        }
      ],
      "source": [
        "%ls images/train -1 | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxkroo-MvXsO",
        "outputId": "0f3c10e9-24bb-4c32-a946-5eef63c595f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQgq9LD6nnis"
      },
      "source": [
        "#Face Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBk_w05clql7"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import euclidean\n",
        "import math\n",
        "from skimage.transform import rotate\n",
        "from mtcnn import MTCNN\n",
        "import mediapipe\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "import urllib\n",
        "\n",
        "\n",
        "def get_model_path(model_name):\n",
        "    model_file = model_name + \".pt\"\n",
        "    cache_dir = os.path.join(os.path.expanduser(\"~\"), \".hsemotions\")\n",
        "    # cache_dir = \"emotion_models\"\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "    fpath = os.path.join(cache_dir, model_file)\n",
        "    if not os.path.isfile(fpath):\n",
        "        print(f\"{model_file} not exists\")\n",
        "        url = (\n",
        "            \"https://github.com/HSE-asavchenko/face-emotion-recognition/blob/main/models/affectnet_emotions/\"\n",
        "            + model_file\n",
        "            + \"?raw=true\"\n",
        "        )\n",
        "        print(\"Downloading\", model_name, \"from\", url)\n",
        "        urllib.request.urlretrieve(url, fpath)\n",
        "\n",
        "    return fpath\n",
        "\n",
        "\n",
        "class FaceAlignment:\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_rotation_on_images(input_images, angles):\n",
        "        rotated_images = [\n",
        "            rotate(image, angle) for image, angle in zip(input_images, angles)\n",
        "        ]\n",
        "        return rotated_images\n",
        "\n",
        "    @staticmethod\n",
        "    def compute_alignment_rotation_(eyes_coordinates):\n",
        "        angles = []\n",
        "        directions = []\n",
        "        for left_eye_coordinate, right_eye_coordinate in eyes_coordinates:\n",
        "\n",
        "            left_eye_x, left_eye_y = left_eye_coordinate\n",
        "            right_eye_x, right_eye_y = right_eye_coordinate\n",
        "\n",
        "            triangle_vertex = (\n",
        "                (right_eye_x, left_eye_y)\n",
        "                if left_eye_y > right_eye_y\n",
        "                else (left_eye_x, right_eye_y)\n",
        "            )\n",
        "            direction = (\n",
        "                -1 if left_eye_y > right_eye_y else 1\n",
        "            )  # rotate clockwise else counter-clockwise\n",
        "\n",
        "            # compute length of triangle edges\n",
        "            a = euclidean(left_eye_coordinate, triangle_vertex)\n",
        "            b = euclidean(right_eye_coordinate, triangle_vertex)\n",
        "            c = euclidean(right_eye_coordinate, left_eye_coordinate)\n",
        "\n",
        "            # cosine rule\n",
        "            if (\n",
        "                b != 0 and c != 0\n",
        "            ):  # this multiplication causes division by zero in cos_a calculation\n",
        "                cos_a = (b**2 + c**2 - a**2) / (2 * b * c)\n",
        "                angle = np.arccos(cos_a)  # angle in radian\n",
        "                angle = (angle * 180) / math.pi  # radian to degree\n",
        "            else:\n",
        "                angle = 0\n",
        "\n",
        "            angle = angle - 90 if direction == -1 else angle\n",
        "\n",
        "            angles.append(angle)\n",
        "            directions.append(direction)\n",
        "\n",
        "        return angles, directions\n",
        "\n",
        "\n",
        "class FaceDetection:\n",
        "\n",
        "    # first call extract_face\n",
        "    def __init__(self, model_name, minimum_confidence):\n",
        "\n",
        "        self.detected_faces_information = None\n",
        "        self.model_name = model_name\n",
        "        self.minimum_confidence = minimum_confidence\n",
        "\n",
        "        if model_name == \"MTCNN\":\n",
        "            detector_model = MTCNN()\n",
        "            self.detect_faces_function = (\n",
        "                lambda input_image: detector_model.detect_faces(input_image)\n",
        "            )\n",
        "\n",
        "    def extract_faces(self, input_image, return_detections_information=True):\n",
        "        self.detect_faces__(input_image)\n",
        "        faces = self.get_faces__(\n",
        "            input_image,\n",
        "        )\n",
        "        if return_detections_information:\n",
        "            return faces, self.detected_faces_information\n",
        "\n",
        "        else:\n",
        "            return faces\n",
        "\n",
        "    def detect_faces__(self, input_image):\n",
        "        detections = self.detect_faces_function(input_image)\n",
        "        self.detected_faces_information = list(\n",
        "            filter(\n",
        "                lambda element: element[\"confidence\"] > self.minimum_confidence,\n",
        "                detections,\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def get_detected_faces_information(self):\n",
        "        return self.detected_faces_information\n",
        "\n",
        "    def get_keypoints(\n",
        "        self,\n",
        "    ):\n",
        "        return list(\n",
        "            map(lambda element: element[\"keypoints\"], self.detected_faces_information)\n",
        "        )\n",
        "\n",
        "    def get_faces__(\n",
        "        self,\n",
        "        input_image,\n",
        "    ):\n",
        "        boxes = [\n",
        "            detection_information[\"box\"]\n",
        "            for detection_information in self.detected_faces_information\n",
        "        ]\n",
        "        y1y2x1x2 = [(int(y), int(y + h), int(x), int(x + w)) for x, y, w, h in boxes]\n",
        "        faces = [input_image[y1:y2, x1:x2] for y1, y2, x1, x2 in y1y2x1x2]\n",
        "        return faces\n",
        "\n",
        "    def get_eyes_coordinates(\n",
        "        self,\n",
        "    ):\n",
        "        eyes_coordinates = [\n",
        "            (info[\"keypoints\"][\"left_eye\"], info[\"keypoints\"][\"right_eye\"])\n",
        "            for info in self.detected_faces_information\n",
        "        ]\n",
        "        return eyes_coordinates\n",
        "\n",
        "\n",
        "class FaceEmotionRecognizer:\n",
        "    # supported values of model_name: enet_b0_8_best_vgaf, enet_b0_8_best_afew, enet_b2_8, enet_b0_8_va_mtl, enet_b2_7\n",
        "    def __init__(self, device, model_name=\"enet_b0_8_best_vgaf\"):\n",
        "        self.device = device\n",
        "        self.is_mtl = \"_mtl\" in model_name\n",
        "        if \"_7\" in model_name:\n",
        "            self.idx_to_class = {\n",
        "                0: \"Anger\",\n",
        "                1: \"Disgust\",\n",
        "                2: \"Fear\",\n",
        "                3: \"Happiness\",\n",
        "                4: \"Neutral\",\n",
        "                5: \"Sadness\",\n",
        "                6: \"Surprise\",\n",
        "            }\n",
        "        else:\n",
        "            self.idx_to_class = {\n",
        "                0: \"Anger\",\n",
        "                1: \"Contempt\",\n",
        "                2: \"Disgust\",\n",
        "                3: \"Fear\",\n",
        "                4: \"Happiness\",\n",
        "                5: \"Neutral\",\n",
        "                6: \"Sadness\",\n",
        "                7: \"Surprise\",\n",
        "            }\n",
        "\n",
        "        self.img_size = 224 if \"_b0_\" in model_name else 260\n",
        "        self.test_transforms = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize((self.img_size, self.img_size)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(\n",
        "                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n",
        "                ),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        path = get_model_path(model_name)\n",
        "\n",
        "        model = torch.load(path)\n",
        "        model = model.to(device)\n",
        "\n",
        "        if isinstance(model.classifier, torch.nn.Sequential):\n",
        "            self.classifier_weights = model.classifier[0].weight.data\n",
        "            self.classifier_bias = model.classifier[0].bias.data\n",
        "        else:\n",
        "            self.classifier_weights = model.classifier.weight.data\n",
        "            self.classifier_bias = model.classifier.bias.data\n",
        "\n",
        "        model.classifier = torch.nn.Identity()\n",
        "        self.model = model.eval()\n",
        "        print(path, self.test_transforms)\n",
        "\n",
        "    def compute_probability(self, features):\n",
        "        return torch.matmul(features, self.classifier_weights.T) + self.classifier_bias\n",
        "\n",
        "    def extract_representations_from_faces(self, input_faces):\n",
        "        faces = [self.test_transforms(Image.fromarray(face)) for face in input_faces]\n",
        "        features = self.model(torch.stack(faces, dim=0).to(self.device))\n",
        "        return features\n",
        "\n",
        "    def predict_emotions_from_representations(\n",
        "        self, representations, logits=True, return_features=True\n",
        "    ):\n",
        "        scores = self.compute_probability(representations)\n",
        "        if self.is_mtl:\n",
        "            predictions_indices = torch.argmax(scores[:, :-2], dim=1)\n",
        "\n",
        "        else:\n",
        "            predictions_indices = torch.argmax(scores, dim=1)\n",
        "\n",
        "        if self.is_mtl:\n",
        "            x = scores[:, :-2]\n",
        "\n",
        "        else:\n",
        "            x = scores\n",
        "        pred = torch.argmax(x[0])\n",
        "\n",
        "        if not logits:\n",
        "            e_x = torch.exp(x - torch.max(x, dim=1)[:, None])\n",
        "            e_x = e_x / e_x.sum(dim=1)[:, None]\n",
        "            if self.is_mtl:\n",
        "                scores[:, :-2] = e_x\n",
        "            else:\n",
        "                scores = e_x\n",
        "\n",
        "        return [\n",
        "            self.idx_to_class[pred.item()] for pred in (predictions_indices)\n",
        "        ], scores\n",
        "\n",
        "\n",
        "class FaceNormalizer:\n",
        "    def __init__(self):\n",
        "        self.mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "        face_mesh = self.mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "\n",
        "        mp_face_mesh = mediapipe.solutions.face_mesh\n",
        "        self.face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True)\n",
        "        self.routes_idx = self.initialize__()\n",
        "\n",
        "    def initialize__(self):\n",
        "        df = pd.DataFrame(\n",
        "            list(self.mp_face_mesh.FACEMESH_FACE_OVAL), columns=[\"p1\", \"p2\"]\n",
        "        )\n",
        "        routes_idx = []\n",
        "\n",
        "        p1 = df.iloc[0][\"p1\"]\n",
        "        p2 = df.iloc[0][\"p2\"]\n",
        "\n",
        "        for i in range(0, df.shape[0]):\n",
        "            obj = df[df[\"p1\"] == p2]\n",
        "            p1 = obj[\"p1\"].values[0]\n",
        "            p2 = obj[\"p2\"].values[0]\n",
        "\n",
        "            route_idx = []\n",
        "            route_idx.append(p1)\n",
        "            route_idx.append(p2)\n",
        "            routes_idx.append(route_idx)\n",
        "\n",
        "        return routes_idx\n",
        "\n",
        "    def get_landmarks__(self, input_image: np.ndarray):\n",
        "        if input_image.dtype == np.float:\n",
        "            input_image = (input_image * 255).astype(np.uint8)\n",
        "\n",
        "        results = self.face_mesh.process(input_image)\n",
        "        landmarks = results.multi_face_landmarks[0]\n",
        "\n",
        "        routes = []\n",
        "        # for source_idx, target_idx in mp_face_mesh.FACEMESH_FACE_OVAL:\n",
        "        for source_idx, target_idx in self.routes_idx:\n",
        "            source = landmarks.landmark[source_idx]\n",
        "            target = landmarks.landmark[target_idx]\n",
        "\n",
        "            relative_source = (\n",
        "                int(input_image.shape[1] * source.x),\n",
        "                int(input_image.shape[0] * source.y),\n",
        "            )\n",
        "            relative_target = (\n",
        "                int(input_image.shape[1] * target.x),\n",
        "                int(input_image.shape[0] * target.y),\n",
        "            )\n",
        "\n",
        "            # cv2.line(img, relative_source, relative_target, (255, 255, 255), thickness = 2)\n",
        "\n",
        "            routes.append(relative_source)\n",
        "            routes.append(relative_target)\n",
        "\n",
        "        return routes\n",
        "\n",
        "    @staticmethod\n",
        "    def normalize_with_landmark_points__(input_image, landmarks):\n",
        "        mask = np.zeros((input_image.shape[0], input_image.shape[1]))\n",
        "        mask = cv2.fillConvexPoly(mask, np.array(landmarks), 1)\n",
        "        mask = mask.astype(bool)\n",
        "\n",
        "        out = np.zeros_like(input_image)\n",
        "        out[mask] = input_image[mask]\n",
        "        return out\n",
        "\n",
        "    def normalize_faces_image(self, input_images):\n",
        "        normalized_faces_images = [\n",
        "            self.normalize_with_landmark_points__(\n",
        "                input_image, self.get_landmarks__(input_image)\n",
        "            )\n",
        "            for input_image in input_images\n",
        "        ]\n",
        "        return normalized_faces_images\n",
        "\n",
        "\n",
        "class EmotionRepresentationExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "    ):\n",
        "        self.face_detection_model: FaceDetection = None\n",
        "        self.face_alignment_model: FaceAlignment = None\n",
        "        self.face_normalizer_model: FaceNormalizer = None\n",
        "        self.face_emotion_recognition_model: FaceEmotionRecognizer = None\n",
        "\n",
        "        self.faces = None\n",
        "        self.normalized_rotated_faces = None\n",
        "        self.rotated_faces = None\n",
        "        self.rotation_angles = None\n",
        "        self.rotation_directions = None\n",
        "\n",
        "    def set_face_detection_model(self, face_detection_model):\n",
        "        self.face_detection_model = face_detection_model\n",
        "        return self\n",
        "\n",
        "    def set_face_alignment_model(self, face_alignment_model):\n",
        "        self.face_alignment_model = face_alignment_model\n",
        "        return self\n",
        "\n",
        "    def set_face_normalizer_model(self, face_normalizer_model):\n",
        "        self.face_normalizer_model = face_normalizer_model\n",
        "        return self\n",
        "\n",
        "    def set_face_emotion_recognition_model(self, face_emotion_recognition_model):\n",
        "        self.face_emotion_recognition_model = face_emotion_recognition_model\n",
        "        return self\n",
        "\n",
        "    def extract_representation(self, input_image):\n",
        "        faces, detected_faces_information = self.face_detection_model.extract_faces(\n",
        "            input_image, return_detections_information=True\n",
        "        )\n",
        "        (\n",
        "            rotation_angles,\n",
        "            rotation_directions,\n",
        "        ) = self.face_alignment_model.compute_alignment_rotation_(\n",
        "            self.face_detection_model.get_eyes_coordinates()\n",
        "        )\n",
        "        rotated_faces = self.face_alignment_model.apply_rotation_on_images(\n",
        "            faces, rotation_angles\n",
        "        )\n",
        "        normalized_rotated_faces = self.face_normalizer_model.normalize_faces_image(\n",
        "            rotated_faces\n",
        "        )\n",
        "\n",
        "        normalized_rotated_faces_255 = [\n",
        "            (image * 255).astype(np.uint8) for image in normalized_rotated_faces\n",
        "        ]\n",
        "\n",
        "        representations = (\n",
        "            self.face_emotion_recognition_model.extract_representations_from_faces(\n",
        "                normalized_rotated_faces_255\n",
        "            )\n",
        "        )\n",
        "        (\n",
        "            predictions,\n",
        "            scores,\n",
        "        ) = self.face_emotion_recognition_model.predict_emotions_from_representations(\n",
        "            representations\n",
        "        )\n",
        "\n",
        "        self.faces = faces\n",
        "        self.rotation_angles, self.rotation_directions = (\n",
        "            rotation_angles,\n",
        "            rotation_directions,\n",
        "        )\n",
        "        self.rotated_faces = rotated_faces\n",
        "        self.normalized_rotated_faces = normalized_rotated_faces_255\n",
        "\n",
        "        return predictions, scores, representations\n",
        "\n",
        "    def get_rotations_information(self):\n",
        "        return self.rotation_angles, self.rotation_directions\n",
        "\n",
        "    def get_faces(self):\n",
        "        return self.faces\n",
        "\n",
        "    def get_rotated_faces(self):\n",
        "        return self.rotated_faces\n",
        "\n",
        "    def get_normalized_rotated_faces(self):\n",
        "        return self.normalized_rotated_faces\n",
        "\n",
        "    def clear(self):\n",
        "        self.faces = None\n",
        "        self.normalized_rotated_faces = None\n",
        "        self.rotated_faces = None\n",
        "        self.rotation_angles = None\n",
        "        self.rotation_directions = None\n",
        "\n",
        "    def store_embeddings(self, file, embeddings):\n",
        "        with open(file, \"wb\") as file_out:\n",
        "            pickle.dump(\n",
        "                {\"embeddings\": embeddings}, file_out, protocol=pickle.HIGHEST_PROTOCOL\n",
        "            )\n",
        "\n",
        "    def load_embeddings(self, file):\n",
        "        with open(file, \"rb\") as file_in:\n",
        "            stored_data = pickle.load(file_in)\n",
        "            stored_embeddings = stored_data[\"embeddings\"]\n",
        "\n",
        "        return stored_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1E_norgnuy0"
      },
      "source": [
        "#Text Embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txucy8AKlr6Y"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "from transformers import RobertaForSequenceClassification\n",
        "import torch\n",
        "import pickle\n",
        "\n",
        "\n",
        "class TextEmbeddingExtractor:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_name=\"pysentimiento/robertuito-sentiment-analysis\",\n",
        "        batch_size=250,\n",
        "        show_progress_bar=True,\n",
        "        to_tensor=True,\n",
        "        max_length=128,\n",
        "    ):\n",
        "        self.model_name = model_name\n",
        "\n",
        "        self.device = device\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.show_progress_bar = show_progress_bar\n",
        "        self.to_tensor = to_tensor\n",
        "\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        # self.model = AutoModel.from_pretrained(self.model_name).to(self.device)\n",
        "\n",
        "        self.model = RobertaForSequenceClassification.from_pretrained(\n",
        "            self.model_name, num_labels=3, output_hidden_states=True\n",
        "        ).to(self.device)\n",
        "\n",
        "        # C1\n",
        "        self.generator = pipeline(\n",
        "            task=\"sentiment-analysis\",\n",
        "            model=self.model,\n",
        "            tokenizer=self.tokenizer,\n",
        "        )\n",
        "\n",
        "    def extract_embedding(\n",
        "        self,\n",
        "        input_batch_sentences,\n",
        "    ):\n",
        "        encoded_input = self.tokenizer(\n",
        "            input_batch_sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        ).to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model_output = self.model(**encoded_input)\n",
        "            hidden_states = model_output[\"hidden_states\"]\n",
        "            last_layer_hidden_states = hidden_states[\n",
        "                12\n",
        "            ]  # 12 = len(hidden_states) , dim = (batch_size, seq_len, 768)\n",
        "            cls_hidden_state = last_layer_hidden_states[:, 0, :]\n",
        "\n",
        "        return cls_hidden_state\n",
        "\n",
        "    def get_labels(self, input_batch_sentences):\n",
        "        return self.generator(input_batch_sentences)\n",
        "\n",
        "    @staticmethod\n",
        "    def store_embeddings(file, embeddings):\n",
        "        with open(file, \"wb\") as file_out:\n",
        "            pickle.dump(\n",
        "                {\"embeddings\": embeddings}, file_out, protocol=pickle.HIGHEST_PROTOCOL\n",
        "            )\n",
        "\n",
        "    @staticmethod\n",
        "    def load_embeddings(file):\n",
        "        with open(file, \"rb\") as file_in:\n",
        "            stored_data = pickle.load(file_in)\n",
        "            stored_embeddings = stored_data[\"embeddings\"]\n",
        "\n",
        "        return stored_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YWFhqHGgn4b2"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WAIa5q1lr3m"
      },
      "outputs": [],
      "source": [
        "import os, cv2, torch, ast\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class MSCTDDataSet(Dataset):\n",
        "    \"\"\"MSCTD dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data_size, base_path=\"data/\", dataset_type=\"train\"):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            base_path (str): path to data folder\n",
        "            dataset_type (str): dev, train, test\n",
        "        \"\"\"\n",
        "        base_path = Path(base_path)\n",
        "        self.text_path = base_path / f\"english_{dataset_type}.txt\"\n",
        "        self.image_index_path = base_path / f\"image_index_{dataset_type}.txt\"\n",
        "        self.sentiment_path = base_path / f\"sentiment_{dataset_type}.txt\"\n",
        "        self.image_dir = base_path / \"images\" / dataset_type\n",
        "        self.data_size = data_size\n",
        "        self.data_info = self.read_info()\n",
        "        self.image_pad = 10\n",
        "        self.face_embedding_extractor = self.get_face_embedding_extractor()\n",
        "\n",
        "    def get_face_embedding_extractor(self):\n",
        "        fd = FaceDetection(\"MTCNN\", minimum_confidence=0.95)\n",
        "        fa = FaceAlignment()\n",
        "        fn = FaceNormalizer()\n",
        "        model_name = \"enet_b0_8_best_afew\"\n",
        "        fer = FaceEmotionRecognizer(\"cuda:0\", model_name)\n",
        "        fre = (\n",
        "            EmotionRepresentationExtractor()\n",
        "            .set_face_detection_model(fd)\n",
        "            .set_face_alignment_model(fa)\n",
        "            .set_face_normalizer_model(fn)\n",
        "            .set_face_emotion_recognition_model(fer)\n",
        "        )\n",
        "        return fre\n",
        "\n",
        "    def read_info(self):\n",
        "        with open(self.text_path) as f:\n",
        "            texts = [t.strip() for t in f.readlines()]\n",
        "        with open(self.image_index_path) as f:\n",
        "            images = [ast.literal_eval(t.strip()) for t in f.readlines()]\n",
        "\n",
        "        with open(self.sentiment_path) as f:\n",
        "            sentiments = [int(t.strip()) for t in f.readlines()]\n",
        "        texts = texts[: self.data_size]\n",
        "        images = images[: self.data_size]\n",
        "        sentiments = sentiments[: self.data_size]\n",
        "        df = pd.DataFrame(\n",
        "            [texts, images, sentiments], index=[\"text\", \"image\", \"sentiment\"]\n",
        "        ).transpose()\n",
        "        return df\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_info.shape[0]\n",
        "\n",
        "    def get_face_features(self, idx):\n",
        "        img_name = self.image_dir / f\"{idx}.jpg\"\n",
        "        image = cv2.imread(str(img_name))[:, :, ::-1]\n",
        "        (\n",
        "            predictions,\n",
        "            scores,\n",
        "            representations,\n",
        "        ) = self.face_embedding_extractor.extract_representation(image)\n",
        "        return representations[0]\n",
        "\n",
        "    def get_sentiment(self, sentiment):\n",
        "        return sentiment\n",
        "\n",
        "    def get_text(self, text):\n",
        "        return text\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        data = self.data_info.iloc[idx]\n",
        "        try:\n",
        "            face = self.get_face_features(idx)\n",
        "        except (TypeError, RuntimeError):\n",
        "            face = -100 * torch.ones(1280).to(device)\n",
        "        sentiment = self.get_sentiment(data[\"sentiment\"])\n",
        "        text = self.get_text(data[\"text\"])\n",
        "        sample = {\"face\": face, \"text\": text, \"sentiment\": sentiment}\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7phGAYdn7LL"
      },
      "source": [
        "#Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCCY1P7Jlr01"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "class MSCTDDataLoader:\n",
        "    def __init__(self, dl, device, tokenizer=None, text_len=512):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        self.text_embedding_extractor = TextEmbeddingExtractor()\n",
        "\n",
        "    def __iter__(self):\n",
        "        for b in self.dl:\n",
        "            b[\"text_embedding\"] = self.text_embedding_extractor.extract_embedding(\n",
        "                b[\"text\"]\n",
        "            )\n",
        "            # if self.tokenizer:\n",
        "            #     b[\"text\"] = self.tokenizer(\n",
        "            #         b[\"text\"],\n",
        "            #         padding=\"max_length\",\n",
        "            #         max_length=self.text_len,  # including [CLS] end [SEP]\n",
        "            #         truncation=True,\n",
        "            #         return_tensors=\"pt\",\n",
        "            #         # return_offsets_mapping=True,\n",
        "            #     )\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "\n",
        "def to_device(data, device):\n",
        "    if isinstance(data, (list, tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    if isinstance(data, dict):\n",
        "        return {k: to_device(v, device) for k, v in data.items()}\n",
        "    if isinstance(data, str):\n",
        "        return data\n",
        "    return data.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta6yhrOr0SjZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class SimpleDenseNetwork(nn.Module):\n",
        "    def __init__(self, n_classes, embedding_dimension):\n",
        "        super(SimpleDenseNetwork, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "        self.embedding_dimension = embedding_dimension\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(\n",
        "                in_features=self.embedding_dimension,\n",
        "                out_features=512,\n",
        "            ),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=512, out_features=128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(in_features=128, out_features=3),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Softmax(),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_batch):\n",
        "        x = input_batch\n",
        "        x = self.fc(x)\n",
        "        output_batch = x\n",
        "\n",
        "        return output_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BHPJSCmn_4Y"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bXFra0LEuGTb"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "num_workers = 1\n",
        "EPOCHS = 1\n",
        "embedding_dimension = 2048\n",
        "learning_rate = 0.1\n",
        "momentum = 0.9\n",
        "data_size = 1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtzGPVTplryF"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "from datetime import datetime\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "def train_epoch(epoch_index, model, dataloader, loss_fn, optimizer):\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "    for data_pair_index, batch in enumerate(dataloader):\n",
        "        print(\"--------------\", data_pair_index, \"-------------\")\n",
        "        text_embedding = batch[\"text_embedding\"]\n",
        "        face_embedding = batch[\"face\"]\n",
        "        # pose_embeding = batch[\"pose_embeding\"]\n",
        "        labels = batch[\"sentiment\"]\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(torch.cat((text_embedding, face_embedding), 1))\n",
        "\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Gather data and report\n",
        "        running_loss += loss.item()\n",
        "        if data_pair_index % 1000 == 999:\n",
        "            last_loss = running_loss / 1000  # loss per batch\n",
        "            print(\"  batch {} loss: {}\".format(data_pair_index + 1, last_loss))\n",
        "            tb_x = epoch_index * len(dataloader) + data_pair_index + 1\n",
        "            print(\"Loss/train\", last_loss, tb_x)\n",
        "            running_loss = 0.0\n",
        "\n",
        "    return last_loss\n",
        "\n",
        "\n",
        "def train_model(model, epochs, train_dataloader, val_dataloader):\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "    for epoch in trange(epochs):\n",
        "        model.train()\n",
        "        train_epoch(epoch, model, train_dataloader, loss_fn, optimizer)\n",
        "        model.eval()\n",
        "        validate(model, val_dataloader, loss_fn)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWLiQFmvtvyd"
      },
      "outputs": [],
      "source": [
        "# model = SimpleDenseNetwork(n_classes=3, embedding_dimension=embedding_dimension).to(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbxK8X8ftyIM",
        "outputId": "b43d3518-6d2b-4ab3-fbca-811186481b85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.hsemotions/enet_b0_8_best_afew.pt Compose(\n",
            "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n",
            "/root/.hsemotions/enet_b0_8_best_afew.pt Compose(\n",
            "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n",
            "    ToTensor()\n",
            "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "val_dataset = MSCTDDataSet(data_size, project_path + \"data/\", \"val\")\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "val_dataloader = MSCTDDataLoader(val_dataloader, device)\n",
        "\n",
        "test_dataset = MSCTDDataSet(5000, project_path + \"data/\", \"test\")\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "test_dataloader = MSCTDDataLoader(test_dataloader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "InvQze-IzLf0",
        "outputId": "db339e96-0ccb-470b-e31a-355a10a86f77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:276: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- 0 -------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- 1 -------------\n",
            "-------------- 2 -------------\n",
            "-------------- 3 -------------\n",
            "-------------- 4 -------------\n"
          ]
        }
      ],
      "source": [
        "model = train_model(model, EPOCHS, val_dataloader, test_dataloader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8_-tAeMhMUG"
      },
      "source": [
        "#Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2B5IZCzhUAv"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "precision = evaluate.load(\"precision\")\n",
        "\n",
        "\n",
        "def validate(model, dataloader, loss_fn):\n",
        "    running_loss = 0.0\n",
        "    last_loss = 0.0\n",
        "\n",
        "    for data_pair_index, batch in enumerate(dataloader):\n",
        "        print(\"--------------\", data_pair_index, \"-------------\")\n",
        "        text_embedding = batch[\"text_embedding\"]\n",
        "        face_embedding = batch[\"face\"]\n",
        "        # pose_embeding = batch[\"pose_embeding\"]\n",
        "        labels = batch[\"sentiment\"]\n",
        "\n",
        "        logits = model(torch.cat((text_embedding, face_embedding), 1))\n",
        "        # print(outputs)\n",
        "        accuracy.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
        "        precision.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        running_loss += loss.item()\n",
        "        # print(running_loss)\n",
        "        # print('true answer',labels)\n",
        "        # print('prediction',logits.argmax(dim=1))\n",
        "        # if data_pair_index==2:\n",
        "        #   break\n",
        "    print(accuracy.compute())\n",
        "    print(precision.compute(average=None))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v90Uk-4akcFR",
        "outputId": "ff5c85d4-a107-43a2-ea25-c7df53de43fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------- 0 -------------\n",
            "-------------- 1 -------------\n",
            "-------------- 2 -------------\n",
            "-------------- 3 -------------\n",
            "-------------- 4 -------------\n",
            "-------------- 5 -------------\n",
            "-------------- 6 -------------\n",
            "-------------- 7 -------------\n",
            "-------------- 8 -------------\n",
            "-------------- 9 -------------\n",
            "-------------- 10 -------------\n",
            "-------------- 11 -------------\n",
            "-------------- 12 -------------\n",
            "-------------- 13 -------------\n",
            "-------------- 14 -------------\n",
            "-------------- 15 -------------\n",
            "-------------- 16 -------------\n",
            "-------------- 17 -------------\n",
            "-------------- 18 -------------\n",
            "-------------- 19 -------------\n",
            "-------------- 20 -------------\n",
            "-------------- 21 -------------\n",
            "-------------- 22 -------------\n",
            "-------------- 23 -------------\n",
            "-------------- 24 -------------\n",
            "-------------- 25 -------------\n",
            "-------------- 26 -------------\n",
            "-------------- 27 -------------\n",
            "-------------- 28 -------------\n",
            "-------------- 29 -------------\n",
            "-------------- 30 -------------\n",
            "-------------- 31 -------------\n",
            "-------------- 32 -------------\n",
            "-------------- 33 -------------\n",
            "-------------- 34 -------------\n",
            "-------------- 35 -------------\n",
            "-------------- 36 -------------\n",
            "-------------- 37 -------------\n",
            "-------------- 38 -------------\n",
            "-------------- 39 -------------\n",
            "-------------- 40 -------------\n",
            "-------------- 41 -------------\n",
            "-------------- 42 -------------\n",
            "-------------- 43 -------------\n",
            "-------------- 44 -------------\n",
            "-------------- 45 -------------\n",
            "-------------- 46 -------------\n",
            "-------------- 47 -------------\n",
            "-------------- 48 -------------\n",
            "-------------- 49 -------------\n",
            "-------------- 50 -------------\n",
            "-------------- 51 -------------\n",
            "-------------- 52 -------------\n",
            "-------------- 53 -------------\n",
            "-------------- 54 -------------\n",
            "-------------- 55 -------------\n",
            "-------------- 56 -------------\n",
            "-------------- 57 -------------\n",
            "-------------- 58 -------------\n",
            "-------------- 59 -------------\n",
            "-------------- 60 -------------\n",
            "-------------- 61 -------------\n",
            "-------------- 62 -------------\n",
            "-------------- 63 -------------\n",
            "-------------- 64 -------------\n",
            "-------------- 65 -------------\n",
            "-------------- 66 -------------\n",
            "-------------- 67 -------------\n",
            "-------------- 68 -------------\n",
            "-------------- 69 -------------\n",
            "-------------- 70 -------------\n",
            "-------------- 71 -------------\n",
            "-------------- 72 -------------\n",
            "-------------- 73 -------------\n",
            "-------------- 74 -------------\n",
            "-------------- 75 -------------\n",
            "-------------- 76 -------------\n",
            "-------------- 77 -------------\n",
            "-------------- 78 -------------\n",
            "-------------- 79 -------------\n",
            "-------------- 80 -------------\n",
            "-------------- 81 -------------\n",
            "-------------- 82 -------------\n",
            "-------------- 83 -------------\n",
            "-------------- 84 -------------\n",
            "-------------- 85 -------------\n",
            "-------------- 86 -------------\n",
            "-------------- 87 -------------\n",
            "-------------- 88 -------------\n",
            "-------------- 89 -------------\n",
            "-------------- 90 -------------\n",
            "-------------- 91 -------------\n",
            "-------------- 92 -------------\n",
            "-------------- 93 -------------\n",
            "-------------- 94 -------------\n",
            "-------------- 95 -------------\n",
            "-------------- 96 -------------\n",
            "-------------- 97 -------------\n",
            "-------------- 98 -------------\n",
            "-------------- 99 -------------\n",
            "-------------- 100 -------------\n",
            "-------------- 101 -------------\n",
            "-------------- 102 -------------\n",
            "-------------- 103 -------------\n",
            "-------------- 104 -------------\n",
            "-------------- 105 -------------\n",
            "-------------- 106 -------------\n",
            "-------------- 107 -------------\n",
            "-------------- 108 -------------\n",
            "-------------- 109 -------------\n",
            "-------------- 110 -------------\n",
            "-------------- 111 -------------\n",
            "-------------- 112 -------------\n",
            "-------------- 113 -------------\n",
            "-------------- 114 -------------\n",
            "-------------- 115 -------------\n",
            "-------------- 116 -------------\n",
            "-------------- 117 -------------\n",
            "-------------- 118 -------------\n",
            "-------------- 119 -------------\n",
            "-------------- 120 -------------\n",
            "-------------- 121 -------------\n",
            "-------------- 122 -------------\n",
            "-------------- 123 -------------\n",
            "-------------- 124 -------------\n",
            "-------------- 125 -------------\n",
            "-------------- 126 -------------\n",
            "-------------- 127 -------------\n",
            "-------------- 128 -------------\n",
            "-------------- 129 -------------\n",
            "-------------- 130 -------------\n",
            "-------------- 131 -------------\n",
            "-------------- 132 -------------\n",
            "-------------- 133 -------------\n",
            "-------------- 134 -------------\n",
            "-------------- 135 -------------\n",
            "-------------- 136 -------------\n",
            "-------------- 137 -------------\n",
            "-------------- 138 -------------\n",
            "-------------- 139 -------------\n",
            "-------------- 140 -------------\n",
            "-------------- 141 -------------\n",
            "-------------- 142 -------------\n",
            "-------------- 143 -------------\n",
            "-------------- 144 -------------\n",
            "-------------- 145 -------------\n",
            "-------------- 146 -------------\n",
            "-------------- 147 -------------\n",
            "-------------- 148 -------------\n",
            "-------------- 149 -------------\n",
            "-------------- 150 -------------\n",
            "-------------- 151 -------------\n",
            "-------------- 152 -------------\n",
            "-------------- 153 -------------\n",
            "-------------- 154 -------------\n",
            "-------------- 155 -------------\n",
            "-------------- 156 -------------\n",
            "{'accuracy': 0.2568}\n",
            "{'precision': array([0.2568, 0.    , 0.    ])}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "validate(model, test_dataloader, nn.CrossEntropyLoss())"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I8_-tAeMhMUG"
      ],
      "name": "Full.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}