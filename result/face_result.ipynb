{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "from pathlib import Path\n",
    "\n",
    "# drive.mount(\"/content/drive\")\n",
    "# project_path = Path(\"/content/drive/MyDrive/NLP/MultiModalEmotionRecognition\")\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "\n",
    "project_path = Path(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(str(project_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "3WAIa5q1lr3m"
   },
   "outputs": [],
   "source": [
    "import os, cv2, torch, ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class MSCTDDataSet(Dataset):\n",
    "    \"\"\"MSCTD dataset.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_path=\"data/\",\n",
    "        split=\"train\",\n",
    "        data_size=None,\n",
    "        load=False,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_path (str or path): path to data folder\n",
    "            split (str): dev, train, test\n",
    "        \"\"\"\n",
    "        if isinstance(base_path, str):\n",
    "            base_path = Path(base_path)\n",
    "        self.base_path = base_path\n",
    "        self.load_path = base_path / \"saved_features\"\n",
    "        self.split = split\n",
    "        self.sentiment_file_path = base_path / f\"sentiment_{split}.txt\"\n",
    "        self.image_dir = base_path / \"images\" / split\n",
    "\n",
    "        self.data_size = data_size\n",
    "        self.load = load\n",
    "        self.device = device\n",
    "\n",
    "        self.sentiments = None\n",
    "        self.indexes = None\n",
    "        self.face_embeddings = None\n",
    "        self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        if self.load:\n",
    "            try:\n",
    "                indexes = torch.load(self.load_path / f\"real_indexes_{self.split}.pt\")\n",
    "                sentiments = torch.load(self.load_path / f\"sentiments_{self.split}.pt\")\n",
    "                face_embeddings = torch.load(\n",
    "                    self.load_path / f\"face_embeddings_{self.split}.pt\"\n",
    "                )\n",
    "\n",
    "                assert (\n",
    "                    face_embeddings.shape[0] == indexes.shape[0]\n",
    "                ), \"ERROR: face and index list are not the same size in loading\"\n",
    "                assert (\n",
    "                    indexes.shape[0] == sentiments.shape[0]\n",
    "                ), \"ERROR: index and sentiment list are not the same size in loading\"\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\n",
    "                    \"Warning: passed load=True but not embedding file was located. Not loading\"\n",
    "                )\n",
    "                if str(e).startswith(\"ERROR\"):\n",
    "                    raise e\n",
    "\n",
    "        if self.data_size:\n",
    "            indexes = indexes[: self.data_size]\n",
    "            sentiments = sentiments[: self.data_size]\n",
    "            if not face_embeddings is None:\n",
    "                face_embeddings = face_embeddings[: self.data_size, :]\n",
    "\n",
    "        self.sentiments = sentiments\n",
    "        self.indexes = indexes\n",
    "        self.face_embeddings = face_embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.load:\n",
    "            return self.face_embeddings.shape[0]\n",
    "\n",
    "    def get_face_embeddings(self, index):\n",
    "        return self.face_embeddings[index]\n",
    "\n",
    "    def get_sentiment(self, index):\n",
    "        return self.sentiments[index]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if torch.is_tensor(index):\n",
    "            index = index.tolist()\n",
    "        try:\n",
    "            face_embedding = self.get_face_embeddings(index)\n",
    "        except Exception as e:\n",
    "            print(f\"error for split:{self.split} index: {index}\")\n",
    "            print(e)\n",
    "\n",
    "        sentiment = self.get_sentiment(index)\n",
    "        sample = {\n",
    "            \"index\": self.indexes[index],\n",
    "            \"face_embedding\": face_embedding,\n",
    "            \"sentiment\": sentiment,\n",
    "        }\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HCCY1P7Jlr01"
   },
   "outputs": [],
   "source": [
    "class MSCTDDataLoader:\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "\n",
    "    def __iter__(self):\n",
    "        for b in self.dl:\n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dl)\n",
    "\n",
    "\n",
    "def to_device(data, device):\n",
    "    if isinstance(data, (list, tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    if isinstance(data, dict):\n",
    "        return {k: to_device(v, device) for k, v in data.items()}\n",
    "    if isinstance(data, str):\n",
    "        return data\n",
    "    return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Ta6yhrOr0SjZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class SimpleDenseNetwork(nn.Module):\n",
    "    def __init__(self, n_classes, embedding_dimension):\n",
    "        super(SimpleDenseNetwork, self).__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        self.embedding_dimension = embedding_dimension\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                in_features=self.embedding_dimension,\n",
    "                out_features=512,\n",
    "            ),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=512, out_features=128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(in_features=128, out_features=3),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        x = input_batch\n",
    "        x = self.fc(x)\n",
    "        output_batch = x\n",
    "\n",
    "        return output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "bXFra0LEuGTb"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "num_workers = 1\n",
    "EPOCHS = 10\n",
    "embedding_dimension = 1280\n",
    "\n",
    "learning_rate = 0.001\n",
    "momentum = 0.001\n",
    "data_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "bbxK8X8ftyIM"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "val_dataset = MSCTDDataSet(project_path / \"data\", \"val\", data_size=data_size, load=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "val_dataloader = MSCTDDataLoader(val_dataloader, device)\n",
    "\n",
    "train_dataset = MSCTDDataSet(project_path / \"data\", \"train\", load=True)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "train_dataloader = MSCTDDataLoader(train_dataloader, device)\n",
    "\n",
    "# test_dataset = MSCTDDataSet(project_path / \"data\", \"test\", load=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "# test_dataloader = MSCTDDataLoader(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "B2B5IZCzhUAv"
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "\n",
    "def validate(model, dataloader, loss_fn):\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "\n",
    "    for data_pair_index, batch in enumerate(dataloader):\n",
    "        face_embedding = batch[\"face_embedding\"]\n",
    "        labels = batch[\"sentiment\"]\n",
    "        logits = model(face_embedding)\n",
    "        accuracy.add_batch(predictions=logits.argmax(dim=1), references=labels)\n",
    "        loss = loss_fn(logits, labels)\n",
    "        running_loss += loss.item()\n",
    "    print(accuracy.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RtzGPVTplryF"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def train_epoch(epoch_index, model, dataloader, loss_fn, optimizer):\n",
    "    running_loss = 0.0\n",
    "    # last_loss = 0.0\n",
    "\n",
    "    for data_pair_index, batch in enumerate(dataloader):\n",
    "        face_embedding = batch[\"face_embedding\"]\n",
    "        labels = batch[\"sentiment\"]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(face_embedding)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(\"Epoch loss: \", running_loss)\n",
    "\n",
    "\n",
    "def train_model(model, epochs, train_dataloader, val_dataloader):\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        print(\"--------------epoch: \", epoch, \"-------------\")\n",
    "        model.train()\n",
    "        train_epoch(epoch, model, train_dataloader, loss_fn, optimizer)\n",
    "        model.eval()\n",
    "        validate(model, val_dataloader, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "oWLiQFmvtvyd"
   },
   "outputs": [],
   "source": [
    "model = SimpleDenseNetwork(n_classes=3, embedding_dimension=embedding_dimension).to(\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch:  0 -------------\n",
      "Epoch loss:  434.9349400997162\n",
      "{'accuracy': 0.3662267546490702}\n",
      "--------------epoch:  1 -------------\n",
      "Epoch loss:  433.03176885843277\n",
      "{'accuracy': 0.3662267546490702}\n",
      "--------------epoch:  2 -------------\n",
      "Epoch loss:  431.8867508172989\n",
      "{'accuracy': 0.36682663467306537}\n",
      "--------------epoch:  3 -------------\n",
      "Epoch loss:  429.58835250139236\n",
      "{'accuracy': 0.368626274745051}\n",
      "--------------epoch:  4 -------------\n",
      "Epoch loss:  426.0436946749687\n",
      "{'accuracy': 0.3728254349130174}\n",
      "--------------epoch:  5 -------------\n",
      "Epoch loss:  423.6378684043884\n",
      "{'accuracy': 0.3707258548290342}\n",
      "--------------epoch:  6 -------------\n",
      "Epoch loss:  418.87018913030624\n",
      "{'accuracy': 0.3698260347930414}\n",
      "--------------epoch:  7 -------------\n",
      "Epoch loss:  413.3151898384094\n",
      "{'accuracy': 0.371625674865027}\n",
      "--------------epoch:  8 -------------\n",
      "Epoch loss:  405.90360221266747\n",
      "{'accuracy': 0.371625674865027}\n",
      "--------------epoch:  9 -------------\n",
      "Epoch loss:  396.8242737650871\n",
      "{'accuracy': 0.3743251349730054}\n"
     ]
    }
   ],
   "source": [
    "train_model(model, EPOCHS, train_dataloader, val_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "17880c9d61fb44f60131b7c571e7dba3ecf129f794c4b04264e3510409962454"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
