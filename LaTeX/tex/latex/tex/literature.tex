At the beginning of this project, we decided to find and read related articles about this area. Here are their brief explanation:

\paragraph{Is there an advantage for recognizing multi-modal emotional stimuli?~\cite{paulmann2011there}} This article indicated that emotion recognition is significantly better in response to multi-modal versus uni-modal stimuli. However, this article's results were based on human trials and it did not include any machine learning methods.

\paragraph{Context-dependent sentiment analysis in user-generated videos~\cite{poria2017context}} This article proposed a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process.

\paragraph{Multilogue-net: A context aware RNN for multi-modal emotion detection and sentiment analysis in conversation~\cite{shenoy2020multilogue}} This article proposed a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance, evaluated using CMU-MOSEI~\cite{zadeh2018multimodal} dataset.

\paragraph{Multimodal Sentiment Analysis on Video Streams using Lightweight Deep Neural Networks~\cite{yakaew2021multimodal}} This article proposed a deep neural learning approach based on multiple modalities in which extracted features of an audiovisual data stream are fused in real time for sentiment classification.

\paragraph{Multi-task Learning for Multi-modal Emotion Recognition and Sentiment Analysis~\cite{akhtar2019multi}} This article proposed a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance, by extracting an embedding from each multi-modal input (text, acoustic, and visual frames) from a GRU network and also from a CIM, which applies pair-wise attention on the previous embeddings. The embeddings from all these processes will be concatenated. Afterward, a classifier on both emotions and sentiments will be trained. This framework has been evaluated on CMU-MOSEI~\cite{zadeh2018multimodal} dataset.

%TODO: other articles


% related in 1 parag.

%Regarding extracting sentiment from a video, emotion and sentiment recognition is significantly accurate in response to multi-modal versus uni-modal stimuli~\cite{paulmann2011there}. A multi-modal model for sentiment analysis can be implemented using a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process~\cite{poria2017context}. As well as this, a context-level inter-modal attention framework for simultaneously predicting the sentiment and expressed emotions of an utterance has been implemented and evaluated on CMU-MOSEI~\cite{zadeh2018multimodal} dataset~\cite{shenoy2020multilogue}. A deep neural learning approach has been proposed based on multiple modalities in which extracted features of an audiovisual data stream are fused in real time for sentiment classification~\cite{yakaew2021multimodal}.



%Give a short, self-contained summary of necessary background
%information. 
%
%For our example if your project is a new \paxos implementation, the background
%would be a brief introduction to the consensus problem, the \paxos algorithm,
%the system model used by the \paxos algorithm
%and common optimizations and relevant variations of that algorithm.
%
%The goal of the background section is to make the paper 
%self-contained for an audience as large as possible. As in every
%section you start with a very brief overview of the section.
%For our example that would be: In this section, we introduce the consensus problem,
%the \paxos algorithm and common optimizations of that algorithm.
%
%If you copy a definition from a text book or some other source, you must cite that source. 
%See Example~\ref{def:consensus} below. 
