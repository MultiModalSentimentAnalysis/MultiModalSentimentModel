These multi-modal methods can also be implemented in other languages using transcripts and subtitles from those languages.
\\
There have been major successful methods in this field and some of them have been introduced in section~\ref{sec:literature} (Related Work) articles. Adopting their pre-train models and fine-tuning them on our datasets would lead to enhancing the results.
\\
There are different approaches for fixing transformer-based models. Using Masked Language Models we can mask the embedding of a few scenes and use the model to get their embeddings. We can also try other transformer-based networks with various loss functions to capture the main problem in the training. 



%Here are some other useful resources: (make these into references instead of links?)
%\begin{itemize}
%\item The co-chairs of the Ninth SOSP prepared some recommendations and valuable lessons in a paper titled \textit{How (and How Not) to Write a Good Systems Paper} \url{https://www.usenix.org/legacy/publications/library/proceedings/dsl97/good_paper.html}
%\item \url{http://gramoli.redbellyblockchain.io/web/doc/talks/researchmethod.pdf}
%\item \url{https://www.doc.ic.ac.uk/~prp/doc/talks/11-prp-paper_writing.pdf}
%\item This template is inspired by a template from Markus P\"uschel~\cite{puschel}.
%
%\end{itemize}