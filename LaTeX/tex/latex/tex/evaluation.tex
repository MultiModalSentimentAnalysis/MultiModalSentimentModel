The final results of our trained multi-modal sentiment recognition model in comparison with a pre-trained model on the text and a sentiment recognition model based on face, trained on train dataset, on the test dataset can be seen in Table \ref{table:results}. We used \textit{macro} mode of each criteria.

\begin{center}
	\begin{table}[h!]
		\begin{tabular}{|c|c c c c|} 
			\hline
			Model/Criteria & F1 & Accuracy & Precision & Recall \\
			\hline\hline
			Text & 0.408 & 0.424 & \textbf{0.608} & 0.466 \\
			\hline
			Face & 0.299 & 0.454 & 0.379 & 0.363 \\
			\hline
			Text, Face, Pose & 0.557 & 0.579 & 0.558 & \textbf{0.556} \\ \hline
			Text, Face, Pose, Scene & \textbf{0.559} & \textbf{0.581} & 0.564 & \textbf{0.556} \\
			\hline
		\end{tabular}
		\caption{Final results of multi-modal and single-modal models}
		\label{table:results}
	\end{table}
\end{center}

As it can be seen, the multi-modal model had better outcomes rather than the uni-modal ones. Only when models are being compared with precision, sentiment recognition based on text performs better. Moreover, recognizing sentiment solely using face has never been superior in comparison to others.\\
In addition, including scenes can also lead to improving our model, however, it cannot enhance it extremely and it contributes mildly.
\\
Understanding emotion in a scene is usually dependent on previous scenes, therefore we decided to examine a transformer-based model in order to use attention to find related scenes.
In this model, each embedding of a frame, consisting of face, pose, and text embeddings was an input for a sequence of transformer encoders. Then, the outcome was the input for a fully-connected layer, which resulted in our logits for our labels. We used different loss functions such as Cross-Entropy and negative likelihood loss to compute our loss in training. Although, when we examined this method we observed that the results are not as good as we expected which might be the result of a complex network or the need for other training methods.


%Here you evaluate your work using experiments. You start
%again with a very short summary of the section. The typical
%structure follows.
%
%\subsection{Experimental setup}
%Specify the context and setup of your experiments. This includes e.g.~what hardware (VMs) you are running on, what operating system these machines are running, how they are connected, ...
%
%Also explain how you generate load for your system and what parameters you used here. The general idea is to include enough information for others to reproduce your experiments. To that end, you should provide a detailed set of instructions for repeating your experiments. These instructions should not be included in the report, but should be provided as part of the source code repository on GitHub, typically as the \texttt{README.md} file, or as Shell scripts or Ansible scripts.
%
%If your experiments give strange or unexpected results, analyze, profile and debug your code. \textbf{Do not simply re-run experiments until they give the expected results.}
%
%Finally, running experiments is very time consuming and you may need to go through multiple rounds of experiment, debugging and optimization. \textbf{Do not delay running experiments until the end of your project period.} 
%
%\subsection{Results}
%The results of your experiments. Compare different variants of your design (e.g.~with and without optimizations) or compare performance to other designs or systems. Plots should show the average over multiple runs (at least 10 as a rule of thumb), including error bars, percentiles or min/max values.
%
%Discuss the plot and extract the overall performance. Do not repeat all numbers in the text, but mention relevant differences in numbers, e.g.~our optimization improves throughput by 26\%.
%Discuss how the results validate or contradict your assumptions.
%
%Perform experiments to evaluate your system under operating normal conditions, when experiencing failures or attacks, or with different workloads.
%
%Figure~\ref{fig:graph} shows a graph generated with \texttt{pgfplots} from 
%experiment data.
%\begin{figure}
%\begin{tikzpicture}
%\begin{axis}[
%    xlabel=Throughput ($\times 1000$ Ops/sec),
%    ylabel=Latency (ms),
%    legend entries={baseline, optimized},
%]
%%The mockup experiment data is stored in a csv file, and imported here.
%\addplot table [x=throughput, y=latency, col sep=comma] {data/data-unoptimized.csv};
%\addplot table [x=throughput, y=latency, col sep=comma] {data/data-optimized.csv};
%\end{axis}
%\end{tikzpicture}
%\caption{A graph showing latency and throughput of a baseline and optimized implementation. The axes show latency in milliseconds, and throughput in thousand operations per second. Data is made up.}
%\label{fig:graph}
%\end{figure}
